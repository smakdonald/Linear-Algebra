<?xml version='1.0' encoding='utf-8'?>

<pretext xml:lang="en-US" xmlns:xi="http://www.w3.org/2001/XInclude">

  <docinfo>

    <!-- the other option is "long" which will produce an -->
    <!-- entire front matter section with more headings   -->
    <author-biographies length="short" />

    <!--
    <brandlogo url="http://abstract.pugetsound.edu" source="images/cover_aata_2014.png" />
    -->

    <!--
    <covers front="images/original-front-cover-aata.pdf"
            back="images/plain-back-cover-aata.pdf"/>
    -->

    <!-- Prefix to enhance Sage notebook contents -->
    <!--
    <initialism>AATA</initialism>
    -->

    <!-- Math-related macros from "macros.tex" in previous LaTeX version -->
    <!-- \notdivide replaces \notmid to avoid conflict with tikz - TWJ 5/6/2010 -->
    <!-- Added operator \gf  - TWJ 2/26/2013 -->
    <!-- Added operator \inn - TWJ 4/6/2013  -->
    <!-- Added operator \aut - TWJ 4/6/2013  -->
    <!-- Added operator \Hom - TWJ 8/19/2010 -->
    <macros>
        <!-- Operators     -->
        \def\ann{\operatorname{ann}}
        \newcommand{\Ass}{\operatorname{Ass}}
        \def\Aut{\operatorname{Aut}}
        \def\can{{\mathrm {can}}}
        \def\char{\operatorname{char}}
        \def\cp{\operatorname{CharPoly}}
        \def\codim{\operatorname{codim}} 
        \def\coker{\operatorname{coker}}
        \DeclareMathOperator*{\colim}{colim} 
        \def\cont{\operatorname{cont}} 
        \def\diam{\operatorname{diam}} 
        \def\dm{\operatorname{dim}} 
        \DeclareMathOperator{\edim}{embdim} 
        \def\End{\operatorname{End}} 
        \def\eval{\operatorname{eval}} 
        \def\Ext{\operatorname{Ext}} 
        \def\Frac{\operatorname{Frac}}
        \def\Fun{\operatorname{Fun}}
        \def\Gal{\operatorname{Gal}}
        \def\gcd{\operatorname{gcd}}
        \newcommand{\GL}{\operatorname{GL}} 
        \newcommand{\ht}{\operatorname{height}} 
        \def\Hom{\operatorname{Hom}} 
        \def\id{\operatorname{id}} 
        \def\im{\operatorname{im}} 
        \def\Inn{\operatorname{Inn}}
        \def\ker{\operatorname{ker}}
        \def\lcm{\operatorname{lcm}} 
        \def\Mat{\operatorname{Mat}}
        \newcommand{\Min}{\operatorname{Min}}
        \def\mp{\operatorname{MinPoly}}
        \def\mSpec{\operatorname{mSpec}}
        \def\MSpec{\operatorname{MSpec}}
        \def\null{\operatorname{Nul}}
        \DeclareMathOperator{\ns}{nullspace}
        \newcommand{\opp}{\operatorname{opp}}
        \def\Orb{\operatorname{Orb}} 
        \def\Out{\operatorname{Out}}
        \def\Perm{\operatorname{Perm}}
        \def\ptstab{\operatorname{PtStab}} 
        \def\rad{\operatorname{rad}}
        \DeclareMathOperator{\range}{range}
        \def\rank{\operatorname{rank}}
        \def\res{\operatorname{res}}
        \def\setstab{\operatorname{SetStab}}
        \def\sign{{\operatorname{sign}}}
        \newcommand{\SL}{\operatorname{SL}}
        \def\Span{\operatorname{Span}}
        \def\Spec{\operatorname{Spec}}
        \def\Stab{\operatorname{Stab}} 
        \DeclareMathOperator{\Supp}{Supp}
        \def\Syl{\operatorname{Syl}}
        \def\Tor{\operatorname{Tor}}
        \def\trace{\operatorname{trace}}
        \def\uSpec{\operatorname{\underline{Spec}}}
        <!-- Categories     -->
        \newcommand{\Ob}{\mathrm{Ob}}
        \newcommand{\Set}{\mathbf{Set}}
        \newcommand{\Grp}{\mathbf{Grp}}
        \newcommand{\Ab}{\mathbf{Ab}}
        \newcommand{\Sgrp}{\mathbf{Sgrp}}
        \newcommand{\Ring}{\mathbf{Ring}} 
        \newcommand{\Fld}{\mathbf{Fld}}
        \newcommand{\cRing}{\mathbf{cRing}}
        \newcommand{\Mod}[1]{#1-\mathbf{Mod}} 
        \newcommand{\Cx}[1]{#1-\mathbf{Comp}} 
        \newcommand{\vs}[1]{#1-\mathbf{vect}}
        \newcommand{\Vs}[1]{#1-\mathbf{Vect}}
        \newcommand{\vsp}[1]{#1-\mathbf{vect}^+} 
        \newcommand{\Top}{\mathbf{Top}} 
        \newcommand{\Setp}{\mathbf{Set}_*} 
        \newcommand{\Alg}[1]{#1-\mathbf{Alg}} 
        \newcommand{\cAlg}[1]{#1-\mathbf{cAlg}} 
        \newcommand{\PO}{\mathbf{PO}}
        \newcommand{\Cont}{\mathrm{Cont}}
        \newcommand{\MaT}[1]{\mathbf{Mat}_{#1}}
        \newcommand{\Rep}[2]{\mathbf{Rep}_{#1}(#2)}
        <!-- Greek     -->
        \def\l{\lambda}
        \def\lx{\lambda_x}
        \newcommand{\a}{\alpha}
        \def\b{\beta}
        \def\d{\delta}
        \def\e{\varepsilon}
        \def\g{\gamma}
        \def\t{\theta}
        \def\s{\sigma}
        \def\z{\zeta}
        \def\vp{\varphi}
        <!-- Letters     -->
        <!-- MathBB     -->
        \newcommand{\A}{\mathbb{A}}
        \newcommand{\B}{\mathbb{B}}
        \newcommand{\C}{\mathbb{C}}
        \newcommand{\D}{\mathbb{D}}
        \newcommand{\E}{\mathbb{E}}
        \newcommand{\F}{\mathbb{F}}
        \newcommand{\G}{\mathbb{G}}
        \newcommand{\H}{\mathbb{H}}
        \newcommand{\I}{\mathbb{I}}
        \newcommand{\J}{\mathbb{J}}
        \newcommand{\K}{\mathbb{K}}
        \newcommand{\L}{\mathbb{L}} 
        \newcommand{\M}{\mathbb{M}}
        \newcommand{\N}{\mathbb{N}}
        \newcommand{\O}{\mathbb{O}}
        \newcommand{\P}{\mathbb{P}}
        \newcommand{\Q}{\mathbb{Q}} 
        \newcommand{\R}{\mathbb{R}} 
        \newcommand{\S}{\mathbb{S}}
        \newcommand{\T}{\mathbb{T}}
        \newcommand{\U}{\mathbb{U}}
        \newcommand{\V}{\mathbb{V}}
        \newcommand{\W}{\mathbb{W}}
        \newcommand{\X}{\mathbb{X}}
        \newcommand{\Y}{\mathbb{Y}}
        \newcommand{\Z}{\mathbb{Z}} 
        \newcommand{\ON}{\mathbb{ON}}
        <!-- MathCal     -->
        \def\cA{\mathcal A} 
        \def\cB{\mathcal B} 
        \def\cC{\mathcal C} 
        \def\cD{\mathcal D} 
        \def\cE{\mathcal E} 
        \def\cF{\mathcal F} 
        \def\cG{\mathcal G} 
        \def\cH{\mathcal H} 
        \def\cI{\mathcal I} 
        \def\cJ{\mathcal J} 
        \def\cK{\mathcal K} 
        \def\cL{\mathcal L}
        \def\cM{\mathcal M} 
        \def\cN{\mathcal N} 
        \def\cO{\mathcal O} 
        \def\cP{\mathcal P} 
        \def\cQ{\mathcal Q} 
        \def\cR{\mathcal R} 
        \def\cS{\mathcal S} 
        \def\cT{\mathcal T} 
        \def\cU{\mathcal U} 
        \def\cV{\mathcal V} 
        \def\cW{\mathcal W} 
        \def\cX{\mathcal X} 
        \def\cY{\mathcal Y} 
        \def\cZ{\mathcal Z} 
        <!-- MathFrak     -->
        \newcommand{\fa}{{\mathfrak a}} 
        \newcommand{\fb}{{\mathfrak b}} 
        \newcommand{\fc}{{\mathfrak c}} 
        \newcommand{\fd}{{\mathfrak d}} 
        \newcommand{\fe}{{\mathfrak e}}
        \newcommand{\fm}{{\mathfrak m}} 
        \newcommand{\fp}{{\mathfrak p}} 
        \newcommand{\fq}{{\mathfrak q}} 
        \newcommand{\fK}{{\mathfrak K}} 
        \newcommand{\fR}{{\mathfrak R}} 
        <!-- MathScr     -->
        \def\sA{\mathscr A} 
        \def\sB{\mathscr B} 
        \def\sC{\mathscr C} 
        \def\sD{\mathscr D} 
        \def\sE{\mathscr E} 
        \def\sF{\mathscr F} 
        \def\sG{\mathscr G} 
        \def\sH{\mathscr H} 
        \def\sI{\mathscr I} 
        \def\sJ{\mathscr J} 
        \def\sK{\mathscr K} 
        \def\sL{\mathscr L}
        \def\sM{\mathscr M}
        \def\sN{\mathscr N}
        \def\sO{\mathscr O}
        \def\sP{\mathscr P}
        \def\sQ{\mathscr Q}
        \def\sR{\mathscr R}
        \def\sS{\mathscr S}
        \def\sT{\mathscr T}
        \def\sU{\mathscr U}
        \def\sV{\mathscr V}
        \def\sW{\mathscr W}
        \def\sX{\mathscr X}
        \def\sY{\mathscr Y}
        \def\sZ{\mathscr Z}
        <!-- Tildes     -->
        \def\tS{\tilde{S}}
        <!-- Algebra     -->
        \def\sdp{\rtimes}
        \newcommand{\tensor}{\otimes} 
        \newcommand{\igen}[1]{\langle #1 \rangle} 
        \def\nsg{\unlhd} 
        \def\kval{{k-\mathrm{valued}}} 
        \def\kalg{{k-\mathrm{alg}}}
        \newcommand\GG[2]{\Gal(#1/#2)}
        <!-- Matrices     -->
        \newcommand{\MF}[3]{\Mat_{#1\times #2}(#3)}
        \newcommand{\vectwo}[2]{\begin{bmatrix} #1 \\ #2 \end{bmatrix}} 
        \newcommand{\vecthree}[3]{\begin{bmatrix} #1 \\ #2 \\ #3\end{bmatrix}} 
        \def\ob{{\mathfrak{ob}} }
        <!-- Misc     -->
        \def\qed{\square}
        \def\sse{\subseteq}
        \def\ss{\subset} 
        \def\ssne{\subsetneq}
        \def\sm{\setminus}
        \def\inv{^{-1}} 
        \newcommand{\es}{\emptyset} 
        \newcommand{\Zm}[1]{\Z/({#1})} 
        \def\ov#1{\overline{#1}} 
        \def\xdots{x_1, \dots, x_n} 
        \def\adots{a_1, \dots, a_n} 
        \def\bdots{b_1, \dots, b_n} 
        \def\udots{u_1, \dots, u_n} 
        \newcommand{\leg}[2]{\left(\frac{{#1}}{{#2}}\right)} 
        \def\th{^{th}} 
        \def\htpy{\simeq_{\mathrm{htpc}}} 
        <!-- Math Text     -->
        \def\textand{ \, \text{and} \, } 
        \def\textor{ \, \text{or} \, } 
        \def\textfor{ \, \text{for} \, } 
        \def\textfa{ \, \text{for all} \, } 
        \def\textst{ \, \text{such that} \, } 
        \def\textin{ \, \text{in} \, } 
        \def\fg{ \, \text{finitely generated} \, }
        \newcommand{\op}{\mathrm{op}}
        <!-- Arrows     -->
        \newcommand{\xra}[1]{\xrightarrow{#1}} 
        \newcommand{\xora}[1]{\xtwoheadrightarrow{#1}} 
        \newcommand{\xira}[1]{\xhookrightarrow{#1}} 
        \newcommand{\xla}[1]{\xleftarrow{#1}} 
        \def\lra{\longrightarrow}
        \def\into{\hookrightarrow}
        \def\onto{\twoheadrightarrow}
        <!-- Vectors     -->
        \newcommand{\vv}[1]{\mathbf{#1}}
        \newcommand{\lm}[2]{{#1}\,\l + {#2}\,\mu} 
        \renewcommand{\v}{\vv{v}}
        \renewcommand{\u}{\vv{u}}
        \newcommand{\w}{\vv{w}}
        \newcommand{\x}{\vv{x}}
        \renewcommand{\k}{\vv{k}}
        \newcommand{\0}{\vv{0}}
        \newcommand{\1}{\vv{1}}
        \newcommand{\vecs}[2]{#1_1,#1_2,\dots,#1_{#2}}
        \newcommand{\us}[1][n]{\vecs{\u}{#1}}
        \newcommand{\vs}[1][n]{\vecs{\v}{#1}}
        \newcommand{\ws}[1][n]{\vecs{\w}{#1}}
        \newcommand{\vps}[1][n']{\vecs{\v'}{#1}}
        \newcommand{\ls}[1][n]{\vecs{\l}{#1}}
        \newcommand{\mus}[1][n]{\vecs{\mu}{#1}} 
        \newcommand{\lps}[1][n]{\vecs{\l'}{#1}}
        \def\td{\tilde{\delta}}
        \def\oo{\overline{\omega}}
        \def\ctJ{\tilde{\mathcal J}}
        \def\tPhi{\tilde{\Phi}}
        \def\te{\tilde{e}}
        \def\M{\operatorname{M}}
        \newcommand{\homotopic}{\simeq}
        \newcommand{\homeq}{\cong}
        \newcommand{\iso}{\approx}
        \newcommand{\dual}{\vee} 
        \DeclarePairedDelimiter{\abs}{|}{|}
        \newcommand{\bv}{{\bar{v}}}
        \newcommand{\bu}{{\bar{u}}}
        \newcommand{\bw}{{\bar{w}}}
        \newcommand{\by}{{\bar{y}}}
        \newcommand{\ba}{{\bar{a}}}
        \newcommand{\bb}{{\bar{b}}}
        \newcommand{\bx}{{\bar{x}}}
        \DeclarePairedDelimiterX\setof[2]{\{}{\}}{#1\,|\,#2}
        \newcommand{\vx}{\underline{x}}
        \renewcommand{mod}[1]{\text{(mod }{#1})}

        <!--Jamie/Alexandra-->
        \DeclarePairedDelimiterX\setof[2]{\{}{\}}{#1\,:\,#2}
        \DeclareDocumentCommand \Slv { O{\l} O{i} O{n} } { \sum_{{#2}=1}^{{#3}} {#1}_{{#2}} \v_{{#2}} }
        \DeclarePairedDelimiter{\parens}{(}{)}
        \DeclarePairedDelimiter{\abs}{|}{|}
        \NewDocumentCommand\cmat{r[]}{\begin{bmatrix} #1 \end{bmatrix}}
    </macros>

    <!-- this is the default, but supresses a warning -->
    <cross-references text="type-global" />

    <!-- tikz package and libraries for images -->
    <latex-image-preamble>
    \usepackage{tikz}
    \usetikzlibrary{backgrounds}
    \usetikzlibrary{arrows,matrix}
    \usetikzlibrary{snakes}
    </latex-image-preamble>

    <!--
    <rename @element="identity" xml:lang="fr-FR">Exercise</rename>
    -->

  </docinfo>
  
  <book xml:id="linear-algebra">
    <title>Linear Algebra</title>
    <subtitle>Vectors, Matrices, Spaces, and Transformations </subtitle>

    <frontmatter xml:id="frontmatter">
      <titlepage>

        <author>
          <personname>Sam Macdonald</personname>
          <institution>University of Nebraska -- Lincoln</institution>
        </author>

        <!--
        <author>
          <personname>Jamie Radcliffe</personname>
          <institution>University of Nebraska - Lincoln</institution>
        </author>

        <author>
          <personname>Alexandra Seceleanu</personname>
          <institution>University of Nebraska - Lincoln</institution>
        </author>
        -->

        <date>
          <today />
        </date>
      </titlepage>

      <colophon>

        <website>
          <name>
            <c>smakdonald.github</c>
          </name>
          <address>https://smakdonald.github.io/index.html</address>
        </website>

        <copyright>
          <year>2020<ndash />2023</year>
          <holder>Sam Macdonald</holder>
          <shortlicense> 
            This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit <url href="http://creativecommons.org/licenses/by-sa/4.0/" visual="creativecommons.org/licenses/by-sa/4.0"> CreativeCommons.org</url>
          </shortlicense>
        </copyright>
      </colophon>

    </frontmatter>

    <part xml:id="part-linear">
      <title>Linear Algebra</title>

      <!--
      **************
      Linear Algebra
      **************
      -->
      
      <chapter xml:id="ch-matrices">
        <title>Matrices and Linear Systems</title>
      
        <section xml:id="sec-linear-systems-matrix-equations">
          <title>Linear Systems and Matrix Equations </title>
      
          <introduction>
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
          </introduction>

          <subsection xml:id="subsec-matrix-basics">
            <title>Matrix Basics</title>

            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>

          <subsection xml:id="subsec-encoding-linear-systems">
            <title>Encoding Linear Systems</title>

            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-matrix-arithmetic">
          <title>Matrix Arithmetic </title>
      
          <introduction>
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
          </introduction>

          <subsection xml:id="subsec-addition-scalar-mult">
            <title>Addition and Scalar Multiplication</title>
            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>

          <subsection xml:id="subsec-multiplication">
            <title>Matrix Multiplication</title>
            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>

          <subsection xml:id="subsec-invertibility">
            <title>Invertibility of Square Matrices</title>
            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-solving-linear-systems">
          <title>Solving Linear Systems </title>
      
          <introduction>
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
          </introduction>

          <subsection xml:id="subsec-gaussian-elimination">
            <title>Gaussian Elimination</title>
            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>

          <subsection xml:id="subsec-homogeneous-systems">
            <title>Solving Homogeneous Linear Systems</title>
            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>

          <subsection xml:id="subsec-inhomogeneous-systems">
            <title>Solving Inhomogeneous Linear Systems</title>
            <p>
              Coming soon to an OER near you!
            </p>

          </subsection>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>
      
      </chapter> 

      <chapter xml:id="ch-intro-determinants">
        <title>Introduction to Determinants</title>
      
        <section xml:id="sec-computing-dets">
          <title>Computing Determinants </title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-det-properties">
          <title>Properties of Determinants</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

      </chapter> 

      <chapter xml:id="ch-vector-spaces">
        <title>Vector Spaces</title>
      
        <section xml:id="sec-vector-space-basics">
          <title>Vector Space Basics </title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-subspaces">
          <title>Subspaces, Sums, and Direct Products</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

      </chapter> 

      <chapter xml:id="ch-bases">
        <title>Span and Bases</title>
      
        <section xml:id="sec-span">
          <title>Span</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="linear-independence">
          <title>Linear Independence</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="basis-basics">
          <title>Basis Basics</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="vs-dim">
          <title>Dimension of Vector Spaces</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

      </chapter> 

      <chapter xml:id="ch-linear-maps">
        <title>Linear Maps</title>
      
        <section xml:id="sec-linear-map-basics">
          <title>Linear Map Basics</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="nullspace-and-range">
          <title>Nullspace and Range</title>
          
          <introduction>
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
          </introduction>

          <subsection xml:id="subsec-nullspace">
            <title>Nullspace</title>
            <p>
              Coming soon to an OER near you!
            </p>
            
          </subsection>

          <subsection xml:id="subsec-range">
            <title>Range</title>
            <p>
              Coming soon to an OER near you!
            </p>
            
          </subsection>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="rank-nullity">
          <title>The Rank-Nullity Theorem</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

      </chapter> 

      <chapter xml:id="ch-eigen">
        <title>Eigenvectors, Eigenvalues, Diagonalization</title>
      
        <section xml:id="sec-eigen">
          <title>Eigenvectors and Eigenvalues</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-charpoly">
          <title>The Characteristic Polynomial</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-diagonal-matrices">
          <title>Diagonal Matrices</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

      </chapter> 

      <chapter xml:id="ch-orthogonality">
        <title>Orthogonality</title>
      
        <section xml:id="sec-inner-products">
          <title>Inner Products</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-orthogonal-sets-projections">
          <title>Orthogonal Sets and Projections</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

        <section xml:id="sec-gram-shcmidt-least-squares">
          <title>Gram Schmidt and Least Squares</title>
          
            <blockquote>
              <p>
                <q></q>
              </p>
              <attribution></attribution>
            </blockquote>
      
          <exercises>
    
            <exercisegroup>
              <title>Computations</title>
              <introduction>
                <p>
                </p>
              </introduction>
    
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>

                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
    
    
            </exercisegroup>
    
            <exercisegroup>
              <title>Formal Proofs</title>
              <introduction>
                <p>
                </p>
              </introduction>
            
              <exercise>
                <statement>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </statement>
    
                <solution>
                  <p>
                    Coming soon to an OER near you!
                  </p>
                </solution>
              </exercise>
            </exercisegroup>
          </exercises>
            
        </section>

      </chapter> 

    </part>

    <part xml:id="part-theory">
      <title>Theory of Linear Transformations</title>

      <!--
      ********************************
      Theory of Linear Transformations
      ********************************
      -->

      <chapter xml:id="ch-introduction">
        <title>Introduction</title>
        
        <section xml:id="sec-algebra">
          <title>Algebra</title>
      
          <p>
            One of the first examples of vectors people encountered were <em>displacement vectors</em>, or as we'll call them, <em>steps</em>. A step in the plane is the process of moving a certain distance in a certain direction. For instance <m>(2,-1)</m> is the action of moving <m>2</m> units right and <m>1</m> unit down. So, using a bit of trigonometry, we see that the step moves you a distance <m>\sqrt{2^2+(-1)^2}</m> in the direction <m>\theta = \arctan(-1/2)</m> in polar coordinates. (See <xref ref="fig:firststep" />)
          </p>
    
          <p>
            One important thing to notice is that the step <m>(2,-1)</m> might occur when moving from the point <m>(1,1)</m> to the point <m>(3,0)</m>, or when moving from the point <m>(7,10)</m> to the point <m>(9,9)</m>, or many other possibilities.
            <fn>An apology is necessary here for using the same notation for steps and for points in the plane. It is (as mathematicians have discovered from long experience) both a blessing and a curse to do this.</fn> 
            In other words, steps don't have a fixed position, but do have a length and a direction.
          </p>
    
          <p>
            The effect of doing the step <m>(3,7)</m> and then the step <m>(2,-3)</m> is identical with the effect of doing the step <m>(5,4)</m>. Moreover, this operation feels a lot like <em>adding</em> the steps together. Similarly, adding <m>(2,-3)</m> to itself gives us a step that we very naturally think of as twice <m>(2,-3)</m>, or <m>2 \,(2,-3) = (4,-6)</m>. It seems natural to think of this operation as that of multiplying the step <m>(2,-3)</m> by the number <m>2</m>. (See <xref ref="fig:vecalg" />)
          </p>
    
          <p>
            These definitions are clearly natural geometrically—it is a very pleasant fact that when we write down the algebraic version of these definitions they are also very natural.
          </p>
    
          <definition xml:id="def-step">
            <statement>
              <p>
                We consider the set of all steps in the plane, thought of as ordered pairs of real numbers: 
                <me>
                  \R^2 = \setof{(x, y)}{x, y\in \R}.
                </me> 
                Given steps <m>(x, y)</m> and <m>(x',y')</m> in <m>\R^2</m> and a number <m>\l\in \R</m> we define 
                <me>
                  \begin{aligned}
                    (x, y)+(x',y') &amp;= (x+x',y+y') \\
                    \l\,(x, y) &amp;= (\l\,x, \l\, y).
                  \end{aligned}
                </me>
              </p>
            </statement>
          </definition>
    
          <p>
            This is the first appearance of algebra in linear algebra. We define algebraic operations—addition of steps and multiplication of steps by numbers—and think about the way that these operations behave and interact. The following definition is a central one.
          </p>

          <example>
            <p>
              If <m>\v=(x, y)</m> and <m>\w=(x',y')</m> are steps in <m>\R^2</m> and <m>\l,\mu \in \R</m> then we say that <me>\l\v + \mu\w = (\l x+\mu x', \l y+\mu y')</me> is a <em>linear combination</em> of <m>\v</m> and <m>\w</m>.
            </p>
          </example>
    
          <definition xml:id="def-vector-ish">
            <statement>
              <p>
                More generally if we have <m>n</m> steps <m>\v_i=(x_i, y_i)</m> and <m>n</m> numbers <m>\l_i</m> then we get a linear combination 
                <me>
                  \Slv = \l_1\v_1 + \l_2\v_2 + \dots + \l_n\v_{n} = \parens[\Big]{ \sum_{i=1}^n \l_i x_i , \sum_{i=1}^n \l_i y_i }.
                </me>
              </p>
            </statement>
          </definition>
    
          <p>
            From here on out we will switch to using the more conventional name of <em>vector</em> for these steps, but you should be warned that we will end up using <q>vector</q> to refer to many different things. If we can take linear combinations of some objects, we'll call them vectors.
          </p>
      
        </section>

        <section xml:id="sec-geometry">
          <title>Geometry</title>

          <introduction>
            <p>
              Let's think about one of the most basic geometric objects there is: a line in the plane. There are two natural ways to specify a line. The description that probably first comes to mind is that of the solution set of a linear equation. The line <m>x+2y = 6</m> has slope <m>-1/2</m> and goes through <m>(6,0)</m>; the line <m>0x+y = 7</m> is horizontal and goes through <m>(0,7)</m>; and the line <m>x+0y = 3</m> is vertical, going through <m>(3,0)</m>. (See <xref ref="fig:lines" />.)
            </p>
      
            <p>
              Because of the existence of the third example, it is more consistent to think of the general description of a line as the solution set of 
              <me>
                ax + by = c,
              </me> 
              where <m>a, b</m>, and <m>c</m> are constants (with not both <m>a</m> and <m>b</m> equalling <m>0</m>). Though this description is very familiar, it's not very <em>geometric</em>.
            </p>

            <question>
              <statement>
                <p>
                Why is the set of solutions of the equation <m>x+2y = 6</m> a line?
              </p>
              </statement>

              <solution>
                <p>
                  If <m>x,y</m> satisfy <m>x+2y = 6</m> we can write <m>x=-2y+6</m> and so 
                  <me>
                    (x,y)=(-2y+6,y)=(6,0)+y(-2,1)
                  </me> 
                  tells us the point <m>(x,y)</m> can be reached by taking the step <m>(6,0)</m> after which we move in the direction of the step <m>(2,-1)</m> either forward of backward <m>|y|</m> times the length of <m>(2,-1)</m>.
                </p>
              </solution>
                
            </question>

          </introduction>
        
          <subsection xml:id="subsec-dot-products">
            <title>Dot Products</title>
    
            <p>
              As we mentioned at the start, linear algebra connects algebra and geometry, so we'd like to make a connection between these two descriptions. Let's start by thinking about the vertical line through <m>(3,0)</m>. This is all the points whose <m>x</m> coordinate is <m>3</m>, or, in other words, all the points whose distance in the direction of the <m>x</m>-axis is <m>3</m>. Similarly the horizontal line through <m>(0,7)</m> consists of all the points in the plane whose vertical distance from the origin is <m>7</m>. Let's try to generalize this notion of <q>distance along an axis</q>.
            </p>
    
            <p>
              Looking at <xref ref="fig:add" /> we see that if we pick an axis the measure of <q>distance along that axis</q> is <em>additive</em>: the answer for the sum of two vectors is the sum of the answers for each individually. In other words, letting <m>L</m> be the axis, we have 
              <me>
                (\text{distance of $\u$ along $L$})+(\text{distance of $\v$ along $L$}) = (\text{distance of $\u+\v$ along $L$}).
              </me> 
              It's more obvious that if we scale a vector by a factor of <m>7</m> say, then the distance along the axis scales by the same factor. So, if <m>c</m> is any real number, we have 
              <me>
                c\,(\text{distance of $\u$ along $L$})=(\text{distance of $c\,\u$ along $L$}).
              </me> 
              Again, using a bit of trigonometry, we have that the length of <m>\u</m> in the direction of <m>L</m> is just the length of <m>\u</m> times <m>\cos(\theta)</m>, where <m>\theta</m> is the (non-obtuse) angle between <m>\u</m> and <m>L</m>.
            </p>
    
            <p>
              Now we have a second question. How do we specify the direction of the axis <m>L</m>? Having spent so much time thinking about vectors, let's do it according to a vector. Let's pick a vector <m>\w</m> that's in the direction of this axis. Many vectors specify this same direction: if <m>\w</m> works then so does <m>170\w</m>, or <m>0.1\w</m>.
            </p>
    
            <p>
              If we make things a little more symmetric, as mathematicians are prone to do, everything becomes much nicer, and a calculation that was, all along, geometry, geometry, geometry, suddenly becomes algebra. Let's remind ourselves how. If we write <m>\abs{\u}</m> for the length of <m>\u</m>, recall that the distance of <m>\u</m> along the axis is <m>\abs{\u}\cos(\theta)</m>.
            </p>
    
            <definition xml:id="def-dot-product">
              <statement>
                <p>
                  We define the <term>dot product</term> of steps <m>\u</m> and <m>\w</m> to be <me>\u \cdot \w = \abs{\u} \abs{\w} \cos(\theta),</me>
                </p>
              </statement>
            </definition>
            <p>
            </p>

            <proposition xml:id="prop-dot-product-properties">
              <statement>
                <p><ol>
                  <li>
                          <p>
                  the dot product is symmetric, i.e., <m>\u\cdot \w = \w \cdot \u</m>.
                </p>
                  </li>
        
                  <li>
                          <p>
                  <m>\u</m> and <m>\v</m> are perpendicular if and only if <m>\u\cdot\v=0</m>
                </p>
                  </li>
        
                  <li>
                          <p>
                  <m>(\u+\v)\cdot \w=\u\cdot \w+\v\cdot \w</m>
                </p>
                  </li>
        
                </ol></p>
              </statement>

              <proof>
                <p>
                  (1) and (2) can be seen from the definition. The third property follows is because we are simply scaling the lengths of <m>\u</m> and <m>\v</m> along the axis by a factor of <m>\abs{\w}</m>. But since it's additive in <m>\u</m> and symmetric, the dot product must also be additive in <m>\w</m> as claimed in (4).
                </p><!--</div attr= class="proof">-->
        
                <p>
                  Why does this help? Because we can split our calculation into small calculations that are trivial to do. Let's pick two vectors of length <m>1</m>, one along the <m>x</m>-axis and one along the <m>y</m>-axis. The traditional names for these vectors are <m>\e_1</m> and <m>\e_2</m>. Now suppose that <m>\u=(a, b)=a\e_1+b\e_2</m> and <m>\w=(c, d) = c\e_1+d\e_2</m>. Then, using additivity, we have <me>\u\cdot \w = (a\e_1+b\e_2) \cdot (c\e_1+d\e_2) = ac \, \e_1\cdot\e_1+ ad \, \e_1\cdot\e_2 +bc\,  \e_1\cdot\e_2 + bd \,\e_2\cdot \e_2.</me> Every term in this final expression is easy to work out geometrically. All lengths are <m>1</m>, all angles are either <m>0</m> or <m>90^\circ</m>. We have, for instance <m>\e_1 \cdot \e_1 = \abs{\e_1}\abs{\e_1} \cos(0) = 1\times1\times1=1</m>. Similarly, <me>\e_1\cdot\e_2 = 1 \qquad \text{and} \qquad \e_1\cdot\e_2 = \e_2\cdot \e_1 = 0.</me> Altogether we get <m>(a, b)\cdot(c, d) = ac+bd</m>.
                </p>
        
                <p>
                  To summarize, we have the following definition, that neatly encapsulates both geometry and algebra.
                </p>
              </proof>
            </proposition>

            <definition xml:id="def-dot-product-2">
              <statement>
                <p>
                  If <m>\u = (x, y)</m> and <m>\w=(x', y')</m> are vectors in <m>\R^2</m> then we define their <em>dot product</em> to be <me>\u \cdot \w = x x' + y y' = \abs{\u} \, \abs{\w} \cos(\theta),</me> where <m>\theta</m> is the angle between <m>\u</m> and <m>\v</m> and <m>\abs{\u} , \abs{\w}</m> denote the length of <m>\u</m> and of <m>\w</m>.
                </p>
              </statement>
            </definition>
    
          </subsection>

        </section>

        <section xml:id="sec-computation">
          <title>Computation</title>

          <p>
            One of the primary focuses of your first linear algebra course was, almost certainly, the solution of systems of linear equations, using techniques such as row reduction and back substitution. We won't recap these algorithms, though there are exercises designed to refresh your memory about the basic issues. Instead we want to present here a broader perspective on what was going on when you were solving such problems.
          </p>
    
          <p>
            Let's consider a representative system of linear equations.
          </p>
    
          <p>
            <me>
              \systeme{x+2y-3z = -7, x+y-2z=-5, 2x+7y-2z = 8 }
            </me>
          </p>
    
          <p>
            The standard approach to solving such a system, called Gaussian elimination, involves adding or subtracting the equations from one another, possible multiplying some by constants. For instance, we might start the solution process for this system by subtracting the first equation from the second (with the intention of eliminating <m>x</m> from the second equation): <me>\veceqn{x+y-2z=-5} \ - \ \veceqn{x+2y-3z = -7} \ =\  \veceqn{-y + z = 2}</me> We eliminate <m>x</m> from the third equation also, by computing <me>\veceqn{2x+7y-2z = 8} \ -\ 2\,\veceqn{x+2y-3z = -7} = \veceqn{3y+4z = 22},</me> and we have made progress on solving our system by deducing that any solution to the original system must also satisfy <me>\systeme{-y+z = 2, 3y+4z = 22\rlap{.}}</me> We continue the process by computing <me>\veceqn{3y+4z=22} \ +\ 3\,\veceqn{-y+z = 2} = \veceqn{7z=28},</me> and thereby deduce that <m>z=4</m>. We could demonstrate this fact quickly by just writing down the linear combination of our equations <me>-\frac57\,\veceqn{x+2y-3z = -7}\ +\ \frac37\, \veceqn{x+y-2z=-5} \ +\ \frac17 \, \veceqn{2x+7y-2z = 8} \ =\ \veceqn{z=4}.</me> We certainly don't advocate this as a sensible way to write down the manual solution of systems of linear equations. However, it is clear that what we are doing in Gaussian elimination is taking linear combinations of things—in this case the things are themselves linear equations. This is the first of many examples where we want to form linear combinations of things that aren't just vectors in two or three dimensions. This leads us to the notion of a <em>vector space</em>.
          </p>
        </section>

        <section xml:id="sec-vector-spaces">
          <title>Vector Spaces</title>
    
          <p>
            One of the two core elements of linear algebra is the notion of a <em>vector space</em>. We'll discuss it informally here and postpone a formal presentation. A vector space is a collection of objects which one can add and multiply by scalars - therefore one can also take linear combinations. The elements of such a collection will be called <em>vectors</em>.
          </p>
          
          <example>
            <p>
                The set <m>\R^2=\setof{(x, y)}{x, y\in \R}</m> is the set of all steps in the plane. We have seen how to take linear combinations of steps.
            </p>
          </example>

          <example>
            <p>
              Similarly we can talk about vectors with more then two entries: <m>\R^3</m> for instance, but also <m>\R^4</m>, <m>\R^5</m>, …, <em>etc</em>. Formally we have <me>\R^n = \setof{(x_1, x_2, \dots, x_{n})}{x_1, x_2, \dots, x_{n} \in \R},</me> and we define <me>\begin{aligned}
        (x_1, x_2, \dots, x_{n}) + (y_1, y_2, \dots, y_{n}) &amp;= (x_1+y_1,x_2+y_2,\dots,x_n+y_n) \\
        \l\, (x_1, x_2, \dots, x_{n}) &amp;= (\l\, x_1, \l\, x_2, \dots, \l\, x_{n}),
    \end{aligned}</me> where <m>\l</m> is a real number.
          </p>
          </example>

          <example>
            <p>
              We can go even further and think about sequences of infinite length, such as those that appear in calculus and analysis. <me>\R^\N = \setof{(a_n)_{n\ge 0}}{\text{$\forall n\in \N$ we have $a_n\in \R$}} = \setof{a}{\text{$a$ is a function $\N\to \R$}}.</me> We can add and scale such things in the natural way: <me>\begin{aligned}
            (a_n)_{n\ge 0} + (b_n)_{n\ge 0} &amp;= (a_n+b_n)_{n\ge 0} \\
            \l\, (a_n)_{n\ge 0} &amp;= (\l a_n)_{n\ge 0}.
        
    \end{aligned}</me>
          </p>
          </example>
    
          <example>
            <p>
              A <em>polynomial with real coefficients</em> is an expression of the form <m>p(X)=a_dX^d+ a_{d-1}X^{d-1}+ \cdots +a_1X+a_0</m>, where <m>a_d, \ldots, a_0\in\R</m>. The set of all polynomials with real coefficients is denoted <me>\R[X]=\{a_dX^d+ a_{d-1}X^{d-1}+ \cdots +a_1X+a_0 \mid a_0, \ldots, a_d\in \R\}.</me> a vector space. We already know how to add polynomials and multiply them by real numbers. If <m>p</m> and <m>q</m> are polynomials with real coefficients and <m>\l\in \R</m> then we have new polynomials <m>p+q</m> and <m>\l\, p</m> with <me>(p+q)(X) = p(X) + q(X) \quad\text{and}\quad (\l p)(X) = \l\, p(X).</me>
          </p>

          <p>
            We also know how to multiply two polynomials together. Eventually we'll talk about gadgets which are vector spaces and for which we can also multiply vectors together. Such things will be called <em>algebras</em>. For the moment though we're only concerned with scaling vectors—multiplying them by numbers.
          </p>
          </example>
       
          <example>
            <p>
              If we define <m>M_{m\times n}(\R)</m> to be the set of all <m>m\times n</m> matrices with real entries then this forms a vector space. <me>M_{m\times n}(\R) = \setof*{\cmat[a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
                                              a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
                                              \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                              a_{m1} &amp; a_{m2} &amp; \cdots &amp; \a_{m n}]}{\text{$\forall i, j$ we have $ a_{i j}\in \R$}}.</me> We add matrices of the same dimensions by <me>\cmat[a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
                  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
                  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                  a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{m n}] +
            \cmat[b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1n} \\
                  b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2n} \\
                  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                  b_{m1} &amp; b_{m2} &amp; \cdots &amp; b_{m n}] =
            \cmat[  a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; \cdots &amp; a_{1n} + b_{1n}\\
                    a_{21} + b_{12} &amp; a_{22} + b_{22} &amp; \cdots &amp; a_{2n} + b_{2n} \\
                      \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                    a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \cdots &amp; a_{m n} + b_{m n} ].</me> Similarly, we scale them by <me>\l\; \cmat[a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
                  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
                  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                  a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{m n}] = 
                          \cmat[\l\,a_{11} &amp; \l\,a_{12} &amp; \cdots &amp; \l\,a_{1n} \\
                              \l\,a_{21} &amp; \l\,a_{22} &amp; \cdots &amp; \l\,a_{2n} \\
                              \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                              \l\,a_{m1} &amp; \l\,a_{m2} &amp; \cdots &amp; \l\,a_{m n}].</me>
          </p>
          </example>
      
          <example>
            <p>
              Going even further than <xref ref="ex:poly" />, we can consider vector spaces where the things we take linear combinations of (the <em>vectors</em>) are other functions. For example <me>C(\R)=\set{\text{$f:\R\to\R$ such that $f$ is continuous}}.</me> We know that linear combinations of continuous functions are continuous, so this is a vector space.
          </p>
          </example>
      
        </section>

        <section xml:id="sec-linear-maps">
          <title>Linear Maps</title>
    
          <p>
            There's one more crucial ingredient to linear algebra that we need to introduce at this point. Geometry isn't just about points and lines and planes—it's also about operations on such things, such as rotations and reflections. The core of linear algebra at this level is the study of vector spaces and <em>linear maps</em>. These are functions between vector spaces that respect addition and scaling of vectors. Let's see some examples.
          </p>

          <example>
            <p>
              Let's take our first example of adding vectors and apply a rotation to each one. In other words let's define a map <m>R:\R^2\to \R^2</m> that rotates any step by (in this case) <m>110^\circ</m>. The diagram demonstrating that <m>(3,7)+(2,-3)=(5,4)</m> rotates together to give a demonstration that <me>R(3,7) + R(2,-3) = R(5,4) = R\parens[\big]{(3,7)+(2,-3)}.</me>
            </p>
          </example>

          <example>
            <p>
              In (brief) summary: the rotation of a sum is the sum of the rotations. It should be clear that the rotation of a scaling of <m>\v</m> is simply the scaling of the rotation.
            </p>
          </example>

          <example>
            <p>
              Now let's apply a reflection through the red line in the diagram (which happens to be the line <m>5x+y=0</m>). We define <m>T:\R^2\to \R^2</m> that reflects any step in this line. The diagram demonstrating that <m>(3,7)+(2,-3)=(5,4)</m> reflects together to give a demonstration that <me>T(3,7) + T(2,-3) = T(5,4) = T\parens[\big]{(3,7)+(2,-3)}.</me>
            </p>
      
            <p>
              The reflection of a sum is the sum of the reflections. Scaling also works.
            </p>
          </example>

          <example>
            <p>
              Given a line <m>L</m>, the map <m>P:\R^2\to \R^2</m> given by <m>P(u)=</m> the projection of <m>u</m> onto the direction of <m>L</m> is a linear map. According to this map is additive, that is, <m>P(u+v)=P(u)+P(v)</m> and it is not hard to see that <m>P(\lambda u)=\lambda P(u)</m> also holds.
            </p>
          </example>
      
          <p>
            We extract the essence of these three examples into the following definition
          </p>

          <definition xml:id="def-linear-map">
            <statement>
              <p>
                Given two vectors spaces <m>V</m> and <m>W</m> (over <m>\R</m>) we say that a function <m>T:V\to W</m> is a <em>linear map</em> if for all <m>\v,\v'\in V</m> and <m>\l\in \R</m> we have <me>\begin{aligned}
                T(\v+\v') &amp;= T(\v) + T(\v') \\
                T(\l\,\v) &amp;= \l\, T(\v). \qedhere
            
        \end{aligned}</me>
              </p>
            </statement>
          </definition>

          <example>
            <p>
              Consider the vector space <m>\R[X]</m> of polynomials in a variable <m>X</m> with real coefficients (see ). Consider the map <m>D:\R[X]\to \R[X]</m> defined by <m>D(p)=p'</m>. In other words <m>D</m> maps every polynomial to its derivative. This is a linear map. That simply means that if <m>\l\in \R</m> and <m>p, q\in \R[X]</m> then <me>(p+q)' = p'+q' \qquad\text{and}\qquad (\l p)' = \l p'.</me> In other words the derivative of the sum of polynomials is the sum of the individual derivatives, and the derivative of a constant multiple of a polynomial is that same multiple of the derivative. These standard facts (that apply not only to polynomials but to other differentiable functions) are precisely the condition for <m>D</m> to be a linear map.
            </p>
          </example>

          <example>
            <p>
              Consider the matrix <m>A\in M_{3\times 3}(\R)</m> given by <me>A = \cmat[1&amp;2&amp;-3\\ 1&amp;1&amp;-2 \\ 2&amp;7&amp;-2].</me> Define a function <m>T: \R^3 \to \R^3</m> by <me>\label{eqn:T}
              T((x, y, z)) = A \,  \mat[x\\ y\\ z] = \cmat[1&amp;2&amp;-3\\ 1&amp;1&amp;-2 \\ 2&amp;7&amp;-2] \, \mat[x\\ y\\ z]= \cmat[x+2y-3z\\ x+y-2z\\ 2x+7y-2z].</me> Standard facts about matrix multiplication tell us that this is a linear map: <me>\begin{aligned}
              T((x, y, z)+(x', y', z')) &amp;= A \parens[\Bigg]{ \mat[x\\ y\\ z] + \mat[x'\\ y'\\ z']} = A \mat[x\\ y\\ z] + A \mat[x'\\ y'\\ z'] = 
                  T((x, y, z)) + T((x', y', z')) \\
              T( \l (x, y, z)) &amp;= A \parens[\Bigg]{ \l \mat[x\\ y\\ z]} = \l\, A \mat[x\\ y\\ z] = \l \, T((x, y, z)) .
          
      \end{aligned}</me> Not only that, but this linear map can be viewed as being constructed using linear combinations. We have <me>T((x, y, z)) = \cmat[1&amp;2&amp;-3\\ 1&amp;1&amp;-2 \\ 2&amp;7&amp;-2] \, \mat[x\\ y\\ z]= \cmat[x+2y-3z\\ x+y-2z\\ 2x+7y-2z] 
                  = x\cmat[1\\1\\2] + y\cmat[2\\1\\7] + z\cmat[-3\\-2\\-2].</me> The result of computing <m>T((x, y, z))</m> is the linear combination, using coefficients <m>x, y</m>, and <m>z</m>, of the <em>columns</em> of <m>A</m>.
            </p>
          </example>
  
        </section>

        <section xml:id="sec-tying-together">
          <title>Tying Things Together</title>
    
          <introduction>
            <p>
              The previous sections discussed issues that had connections and commonalities between them. In this final section we'll think about <xref ref="sec:computation" /> from the higher vantage point permitted by our other discussions.
            </p>
      
            <p>
              We considered the following system of linear equations <me>\systeme{x+2y-3z = -7, x+y-2z=-5, 2x+7y-2z = 8 }</me> give us two very important perspectives on this linear system.
            </p>
          </introduction>

          <subsection xml:id="subsec-perspective-1">
            <title>Perspective 1: Intersection of Three Planes</title>
            <p>
              <term></term>
            </p>
      
            <p>
              In <xref ref="sec-geometry" /> we alluded to the fact that the solutions to a linear equation in <m>\R^3</m> form a plane. In we'll see why this is so. Once we know that each equation defines a plane in <m>\R^3</m>, we see the set of solutions to the system is the set of all points lying on all three planes, that is, the intersection of the three planes. We anticipate that unless there is something special about these planes (such as two of them being parallel) the three planes should have exactly one common point.
            </p>
  
            <proposition>
              <statement>
                <p>
                  The set of solutions to the equation <m>ax+by+cz=d</m>, where <m>a,b,c</m> are not all zero, form a plane in 3-space.
              </p>
              </statement>
  
              <proof>
                <p>
                  Note that the equation <m>ax+by+cz=d</m> can be written using dot product as <me>(a,b,c)\cdot (x,y,z)=d.</me> Let's consider a particular solution <m>(x_0,y_0,z_0)</m> to the above equation. For example, if <m>a\neq 0</m> I could pick <m>(x_0,y_0,z_0)=(d/a,0,0)</m> and similarly if <m>b\neq0</m> and <m>c\neq 0</m>. Then we have <me>(a,b,c)\cdot (x_0,y_0,z_0)=d</me> and subtracting this from the above equation gives <me>(a,b,c)\cdot (x-x_0,y-y_0,z-z_0)=0.</me> This means <m>(x-x_0,y-y_0,z-z_0)</m> is perpendicular to <m>(a,b,c)</m> (or 0). Thinking of <m>(x-x_0,y-y_0,z-z_0)</m> as a step with head at <m>(x,y,z)</m> and tail at <m>(x_0,y_0,z_0)</m> we see it is perpendicular to <m>(a,b,c)</m>. There is a unique plane that passes through the point <m>(x_0,y_0,z_0)</m> and is perpendicular to <m>(a,b,c)</m> and we have just seen that <m>(x,y,z)</m> is a solution if and only if it lies in this plane.
                </p>
              </proof>
            </proposition>
          </subsection>

          <subsection xml:id="subsec-perspective-2">
            <title>Perspective 2: Linear Combinations of Vectors <m>=</m> Range of a Transformation</title>
            
            <p>
              <term></term>
            </p>
      
            <p>
              We can rewrite the system as <me>x\cmat[1\\1\\2] + y\cmat[2\\1\\7] + z\cmat[-3\\-2\\-2] = \cmat[-7\\-5\\8].</me> Thus the system is asking which coefficients <m>x, y, z</m> we can use in a linear combination of three vectors on the left hand side to obtain the vector on the right hand side.
            </p>
      
            <p>
              This in turn, as we saw in , can be rephrase in the language of linear transformations. Using the function <m>T</m> defined in <xref ref="eqn:T" /> we can write the system as <me>T((x, y, z)) = \cmat[-7\\-5\\8],</me> This can be rephrased as understanding whether the vector on the right hand side is a value of <m>T</m> (we will soon phrase this as whether the vector on the right hand side is in the range of <m>T</m>) and if so, which inputs give this value. In other words we are looking for the <em>preimage</em> of the vector on the right hand side with respect to <m>T</m>.
            </p>
      
            <p>
              To summarize, if we consider the coefficient matrix of the system <me>A = \cmat[1&amp;2&amp;-3\\ 1&amp;1&amp;-2 \\ 2&amp;7&amp;-2].</me> the perspective of planes consider the dot products of <m>(x,y,z)</m> with the <em>rows</em> of <m>A</m>, while the perspective of linear combinations considers linear combinations of the <em>columns</em> of <m>A</m> with undetermined coefficients. We will see throughout that being able to see such things from alternative viewpoints allows us a much more interesting view of the world of linear algebra around us.
            </p>

          </subsection>

        </section>

      </chapter>

      <chapter xml:id="ch-beginnings">
        <title>Beginnings</title>

        <introduction>
          <p>
            As described in the introduction, linear algebra is about the intertwined notions of vector spaces and linear maps. Here we give the careful definitions that were omitted in the introduction.
          </p>
        </introduction>
        
        <section xml:id="sec-fields">
          <title>Fields</title>
    
          <p>
            We start by discussing where our <em>scalars</em> can come from. Scalars are the numbers that we use to scale vectors. So far we have used only real numbers for our scalars. But there are other possibilities.
          </p>

          <definition xml:id="def-field">
            <statement>
              <p>
                A <em>field</em> is a collection of objects (that we think of as numbers) that satisfy the same algebraic properties as <m>\R</m> with respect to multiplication and addition.
              </p>

              <p>
                Here then is an explicit list of what we require in order to say that a collection of numbers, equipped with definitions of <m>+</m> and <m>\times</m>, is a field:
              </p>

              <p>
                A <em>field</em> is a set <m>\F</m> (that we think of as numbers) together with definitions of <m>+</m> and <m>\times</m> (in other words, together with functions <m>+,\times:\F\times \F \to \F</m>) such all the following properties hold:
              </p>

              <p><ol>
                <li>
                        <p>
                Addition is commutative: for all <m>x, y\in \F</m> we have <m>x+y=y+x</m>
              </p>
                </li>
      
                <li>
                        <p>
                Addition is associative: for all <m>x, y, z\in \F</m> we have <m>x+(y+z) = (x+y)+z</m>
              </p>
                </li>
      
                <li>
                        <p>
                There is an additive identity: there is a special number <m>0\in \F</m> such that for all <m>x\in \F</m> we have <m>0+x = x</m>
              </p>
                </li>
      
                <li>
                        <p>
                There are additive inverses: for all <m>x\in \F</m> there is an element that we denote <m>-x</m> such that <m>x+(-x)=0</m>.
              </p>
                </li>
      
              </ol></p>

              <p><ol>
                <li>
                          <p>
                Multiplication is commutative: for all <m>x, y\in \F</m> we have <m>x y=y x</m>
              </p>
                </li>
    
                <li>
                          <p>
                Multiplication is associative: for all <m>x, y\in \F</m> we have <m>x(y z)=(x y)z</m>
              </p>
                </li>
    
                <li>
                          <p>
                There is a multiplicative identity: there is a special number <m>1\in \F</m> such that for all <m>x\in \F</m> we have <m>1x = x</m>
              </p>
                </li>
    
                <li>
                          <p>
                There are multiplicative inverse for non-zero numbers: for all <m>x\in \F\wo\set{0}</m> there is an element that we denote <m>x\inv</m> such that <m>xx\inv=1</m>
              </p>
                </li>
    
              </ol></p>
    
              <p>
                If you have seen such things before (and do not worry if you haven't) we're just stating that <m>(\F,+)</m> and <m>(\F\wo\set{0},\times)</m> are Abelian groups.
              </p>

              <p><ol>
                <li>
                            <p>
                Multiplication distributes over addition: for all <m>x, y, z\in \F</m> we have <m>x(y+z) = x y+x z</m>.
              </p>
                </li>
  
                <li>
                            <p>
                <m>1\not = 0</m>
              </p>
                </li>
  
              </ol></p>
            </statement>
          </definition>

          <remark>
            <p>
              The fact that the operations of addition and multiplication are functions <m>+,\times:\F\times \F \to \F</m> implicity gives the following additional properties:
            </p>

            <p><ol>
              <li>
                          <p>
              Closure under addition: for all <m>x, y\in \F</m> we have <m>x+y\in \F</m>.
            </p>
              </li>

              <li>
                          <p>
              Closure under addition: for all <m>x, y\in \F</m> we have <m>x\times y\in \F</m>.
            </p>
              </li>

            </ol></p>

            <p>
              The most important examples of fields are <m>\R</m> and <m>\C</m> but there are others that we will seriously consider.
            </p>
          </remark>

          <example>
            <p>
              The <em>complex numbers</em> are the numbers of the form <me>\C=\{a+bi \mid a,b\in \R\}</me> with addition and multiplication defined by <me>\begin{aligned}
(a+bi)+(c+di) &amp;= (a+b)+(c+d)i\\
(a+bi)\times(c+di) &amp;= (ac-bd)+(ad+bc)i
\end{aligned}</me> Note that by our definition of multiplication <me>i^2=i\times i=(0+1i)\times(0+1i)=-1+0i=-1.</me>
            </p><!--</div attr= class="example">-->

            <p>
              We can think of <m>\R</m> as a subset of <m>\C</m> by identifying <m>r\in \R</m> with <m>r=r+0i\in\C</m>. This makes <m>\R</m> as subfield of <m>\C</m>. A <em>subfield</em> is a subset of a field which is itself a field with respect to the operations on the larger set.
            </p>
          </example>

          <example>
            <p>
              The rational numbers <m>\Q</m> are a field, indeed they are a subfield of <m>\R</m> (simply meaning that <m>\Q\of \R</m> and the field operations for <m>\Q</m> are the same whether you think of elements of <m>\Q</m> as rational numbers or real numbers). This fact means that conditions A1-A2, M1-M2, and C1 are trivially satisfied, since they're true for real numbers, not just rationals. Since the special real numbers <m>0,1</m> are in fact in <m>\Q</m> we also get A3, M3, and C2 for free. This only leaves A4 and M4; given <m>x\in \Q</m> we know that there are real numbers <m>-x</m> and <m>x\inv</m> such that <m>x+(-x)=0</m> and <m>xx\inv=1</m>, but what we need to know is that there are rational numbers with these properties
              <fn>Of course if <m>x=0</m> we don't need to find a multiplicative inverse for <m>x</m>.</fn>. 
              Fortunately it is that case that if <m>x</m> is rational then so are <m>-x</m> and <m>x\inv</m>. Thus <m>\Q</m> is a field.
            </p>

          </example>

          <example>
            <p>
              The <em>field with two elements</em>, <m>\F_2</m>, has as its only elements the two residue classes modulo <m>2</m>, with addition and multiplication defined modulo <m>2</m>. To be specific, we have the following addition and multiplication tables. <me>\begin{array}{c|cc}
          + &amp; 0 &amp; 1 \\
          \hline
          0 &amp; 0 &amp; 1 \\
          1 &amp; 1 &amp; 0 \\         
      \end{array} \qquad
      \begin{array}{c|cc}
          \times &amp; 0 &amp; 1 \\
          \hline
          0 &amp; 0 &amp; 0 \\
          1 &amp; 0 &amp; 1 \\
      \end{array}</me> There are in fact many interesting examples of vector spaces where we use <m>\F_2</m> for our numbers. For instance many schemes of data transmission use the ideas of linear algebra to build error correction into digital communication.
          </p>
          </example>

          <definition xml:id="def-field-abbreviations">
            <statement>
              <p>
                We make some standard abbreviations when computing in fields. We write <me>\begin{aligned}
          x-y &amp;\defeq x+(-y) \\
          x/y &amp;\defeq x y\inv \\
          x^0 &amp;\defeq 1 \\
          x^n &amp;\defeq \underbrace{x \times x \times \dots \times x}_{\text{$n$ factors}}\quad \text{for integers $n\ge 1$} \\
          x^{-m} &amp;\defeq \underbrace{x\inv \times x\inv \times \dots \times x\inv}_{\text{$m$ factors}} \quad \text{for integers $m\ge 1$} 
      
  \end{aligned}</me> We also define, for <m>n\in \N</m>, <me>n \defeq \underbrace{1+1+\dots+1}_{\text{$n$ summands}} \in \F.</me>
              </p>
            </statement>
          </definition>

          <remark>
            <p>
              Above we have defined elements <m>1,2,3,\dots</m> in any field <m>\F</m>. In <m>\Z_7</m> we have <m>10=3=-4</m>, and in <m>\Z_{29}</m> we have <m>59=30=1</m>.
            </p>

            <p>
              You might think it would be confusing that <m>287</m> is simultaneously the name of an integer and a field element, particularly in fields <m>\F_{7}</m> or <m>\F_{41}</m> where <m>287=0</m>, but context resolves all such confusions, usually without anyone noticing.
            </p>

            <p>
              The following rules of arithmetic hold in any field.
            </p>
          </remark>

          <lemma xml:id="lem-field-arithmetic">
            <statement>
              <p>
                If <m>\F</m> is a field an <m>x, y\in \F\wo\set{0}</m> then for For all <m>n, m\in \Z</m> we have <me>\begin{aligned}
          (x y)^n &amp;= x^n y^n &amp; (x/y)^n &amp;= x^n y^{-n} \\
          x^n x^m &amp;= x^{n+m} &amp; \left(x^n\right)^m &amp;= x^{nm}.
      
  \end{aligned}</me>
              </p>
            </statement>
          </lemma>

        </section>

        <section xml:id="sec-vector-spaces-again">
          <title>Vector Spaces</title>

          <definition xml:id="def-vector-space">
            <statement>
              <p>
                A <em>vector space <m>V</m> over a field <m>\F</m></em> is a collection of objects (that we think of as vectors) such that we can form linear combinations <m>x_1\v_1 + x_2\v_2 + \dots +x_n\v_n</m> where the vectors <m>\v_1,\v_2,\dots,\v_n</m> are from <m>V</m> and the scalars <m>x_1,x_2,\dots,x_n</m> are from the field <m>\F</m>.
              </p>

              <p>
                A <em>vector space <m>V</m> over a field <m>\F</m></em> is a set <m>V</m> together with operations of vector addition <m>+\colon V\times V \to V</m> and scalar multiplication <m>\F\times V\to V</m> (the latter of which we denote by simple concatenation; <m>x\v</m> is the scalar multiple of <m>\v</m> by <m>x</m>). These must satisfy the following conditions:
              </p>

              <p><ol>
                <li>
                        <p>
                Vector addition is commutative: for all <m>\v,\w\in V</m> we have <m>\v+\w=\w+\v</m>
              </p>
                </li>
      
                <li>
                        <p>
                Vector addition is associative: for all <m>\u,\v,\w\in V</m> we have <m>\u+(\v+\w) = (\u+\v)+\w</m>
              </p>
                </li>
      
                <li>
                        <p>
                There is a (vector) additive identity: there exists a special vector <m>\0\in V</m> such that for all <m>\v\in V</m> we have <m>\v+\0=\v</m>.
              </p>
                </li>
      
                <li>
                        <p>
                There are (vector) additive inverses: for all <m>\v\in V</m> there exists a vector <m>-\v</m> such that <m>\v+(-\v)=\0</m>
              </p>
                </li>
      
              </ol></p>

              <p><ol>
                <li>
                          <p>
                Scalar multiplication respects field addition: for all <m>x, y\in\F</m> and <m>\v\in V</m> we have <m>(x+y)\v=x\v+y\v</m>
              </p>
                </li>
    
                <li>
                          <p>
                Scalar multiplication respects field multiplication: for all <m>x ,y\in \F</m> and <m>\v\in V</m> we have <m>(x y)\v=x(y\v)</m>
              </p>
                </li>
    
                <li>
                          <p>
                Scalar multiplication respects the field identity: for all <m>\v\in V</m> we have <m>1\v=\v</m>
              </p>
                </li>
    
                <li>
                          <p>
                Scalar multiplication respects vector addition: for all <m>x\in \F</m> and <m>\v,\w\in W</m> we have <m>x(\v+\w) = x\v+x\w</m>.
              </p>
                </li>
    
              </ol></p>
            </statement>
          </definition>

          <definition xml:id="def-vector-space-abbreviation">
            <statement>
              <p>
                We make a standard abbreviation in the context of a vector space as follows: <me>\v-\w \defeq \v+(-\w)</me> for <m>\v,\w\in V</m>.
              </p>
            </statement>
          </definition>

          <lemma xml:id="lem-vector-space-properties">
            <statement>
              <p>
                If <m>V</m> is a vector space over the field <m>\F</m> then the definitions we have made work well with our intuitions in the following ways.
              </p>
    
              <p><ol>
                <li>
                          <p>
                All identities are unique. There is a unique element <m>0\in \F</m> satisfying A3. There is a unique <m>1\in \F</m> satisfying M3. There is a unique element <m>\0\in V</m> satisfying VA3.
              </p>
                </li>
    
                <li>
                          <p>
                All inverses are unique. For <m>x\in \F</m>, <m>y\in\F\wo\set{0}</m>, and <m>\v\in V</m> there are unique elements <m>-x, y\inv \in \F</m> and <m>-\v\in V</m> such that <me>x+(-x) = 0 \qquad y y\inv = 1 \qquad \v+(-\v) = \0.</me>
              </p>
                </li>
    
                <li>
                          <p>
                Inverting twice gets you back where you started: for all <m>x\in \F</m>, <m>y\in \F\wo\set{0}</m>, and <m>\v\in V</m> we have <me>-(-x) = x \qquad (y\inv)\inv = y \qquad -(-\v) = \v.</me>
              </p>
                </li>
    
                <li>
                          <p>
                For all <m>x\in \F</m> and <m>\v\in V</m> we have <m>(-x)\v = -(x\v)</m>. In particular <m>(-1)\v=-\v</m>.
              </p>
                </li>
    
                <li>
                          <p>
                For <m>x, y\in\F</m> and <m>\v\in V</m> we have
              </p>
    
              <p><ol>
                <li>
                          <p>
                <m>x y=0</m> if and only if at least one of <m>x, y</m> is <m>0</m>
              </p>
                </li>
    
                <li>
                          <p>
                <m>x\v=\0</m> if and only if <m>x=0</m> or <m>\v=\0</m>.
              </p>
                </li>
    
              </ol></p>
                </li>
    
              </ol></p>
            </statement>

            <proof>
              <p>
              …of these is traditionally considered good for the soul. Some hints: if <m>0,0'\in \F</m> both satisfy A3 then what is <m>0+0'</m>? If <m>\w</m> and <m>\w'</m> satisfy <m>\v+\w = \v+\w' = \0</m> what is <m>\w+\v+\w'</m>?
            </p>
            </proof>
          </lemma>

          <example>
            <p>
              <ol>
                <li>
                  <p>
                    The classic vector spaces (over <m>\R</m>) are <m>\R^2</m> and <m>\R^3</m>. With identical checks we see that also <m>\R^n</m>, <m>n\ge 1</m> and <m>\R^\N</m> are vector spaces.
                  </p>
                </li>

                <li>
                  <p>
                    Also vital are subspaces of <m>\R^n</m>, such as the plane <m>\setof{(x, y, z)}{x+y+z=0}\of \R^3</m> and the line <m>\setof{t(1,0,-1)}{t\in \R}\of \R^3</m>.
                  </p>
                </li>

                <li>
                  <p>
                    <term>Important remark</term>: note that both the line and the plane given here pass through the origin <m>(0,0,0)</m>. A line or plane that does not pass through the origin is not a subspace. Why not?
                  </p>
                </li>

                <li>
                  <p>
                    Similarly <m>\Q, \Q^2, \Q^3,\dots</m>, and <m>\Q^\N</m> are vector spaces over <m>\Q</m> and <m>\F^n</m> (for any field <m>\F</m> and any <m>n\ge 0</m>) is a vector space over <m>\F</m>.
                  </p>
                </li>

                <li>
                  <p>
                    The set of polynomial <m>\R[X]</m> is a vector space over <m>\R</m> and in fact an <m>\R</m>-algebra.
                    <fn> An algebra is a vector space for which we can also multiply two vectors (in addition to multiplying vectors with scalars.</fn>                 
                  </p>
                </li>

                <li>
                  <p>
                    <m>\R</m> is a vector space over <m>\Q</m>
                  </p>
                </li>

                <li>
                  <p>
                    <m>\C</m> is a vector space over <m>\R</m>
                  </p>
                </li>

                <li>
                  <p>
                    The set <m>M_{m\by n}(\F)</m> of <m>m\by n</m> matrices whose entries come from <m>\F</m> is a vector space of <m>\F</m>, with the usual notions of adding matrices and multiplying them by scalars.
                  </p>
                </li>

                <li>
                  <p>
                    The set <m>C(\R)=\setof{f:[0,1]\to\R}{\text{$f$ is continuous}}</m> is a vector space over <m>\R</m>. It is a subspace of <m>\R^{[0,1]}</m>.
                  </p>
                </li>
              </ol>
            </p>
          </example>
    
          <p>
            Many of the most important examples of vector spaces come from taking a restricted collection of vectors from some larger vector space.
          </p>

          <definition xml:id="def-subspace">
            <statement>
              <p>
                If <m>V</m> is a vector space over <m>\F</m> and <m>W\of V</m> then we say that <m>W</m> is a <em>(linear) subspace of <m>V</m></em> if <m>W</m> is non-empty and <m>W</m> is closed under taking linear combinations. That is to say, for all <m>\vs \in W</m>, <m>\ls \in \F</m> we have <m>\sum_{i=1}^n \l_i\v_i \in W</m>.
              </p>
            </statement>
          </definition>

          <lemma xml:id="lem-subspace-test">
            <statement>
              <p>
                If <m>V</m> is a vector space then <m>W</m> is a subspace of <m>V</m> if and only if <m>\0\in W</m> and for all <m>\v,\w\in W</m>, <m>\l,\mu\in \F</m> we have <m>\l\v+\mu\w\in W</m>.
              </p>
            </statement>

            <proof>
              <p>
                Suppose <m>W</m> is a subspace of <m>V</m>. Since <m>W</m> is nonempty there exists <m>\v\in W</m>. Setting <m>\v_1=v,\v_2=-v</m> and <m>\l_1=\l_2=1</m> we deduce that <m>\0=v+(-v)\in W</m>. Furthermore setting <m>n=2</m>, <m>\v_1=v, \v_2=\w</m> and <m>\l_1=\l, \l_2=\mu</m> in the definition of subspace gives <m>\l\v+\mu\w\in W</m>.
              </p>
    
              <p>
                Conversely, suppose we know <m>\0\in W</m> and for all <m>\v,\w\in W</m>, <m>\l,\mu\in \F</m> we have <m>\l\v+\mu\w\in W</m>. Then one can prove by induction on <m>n</m> that <m>\sum_{i=1}^n \l_i\v_i \in W</m>.
              </p>
    
              <p>
                The base case <m>n=1</m> follows by setting <m>\l=\l_1,\v=\v_1</m> and <m>\w=\0</m>. For the inductive step, assuming <m>\sum_{i=1}^n \l_i\v_i \in W</m> we can set <m>v=\sum_{i=1}^n \l_i\v_i</m>, <m>\l_1=1</m> and <m>\mu=\l_{n+1}, w=\v_{n+1}</m> to conclude <m>\sum_{i=1}^{n+1} \l_i\v_i \in W</m>.
              </p>
            </proof>
          </lemma>

          <lemma xml:id="lem-subspace-is-vector-space">
            <statement>
              <p>
                If <m>V</m> is a vector space over <m>\F</m> and <m>W\of V</m> is a subspace of <m>V</m> then <m>W</m> is a vector space in its own right (with the operations that come with <m>V</m>; we add and scale vectors in <m>W</m> just as we do in <m>V</m>.)
              </p>
            </statement>

            <proof>
              <p>
                Most of the vector space axioms are satisfied automatically: VA1, VA2, and SM1-4 work in <m>W</m> because they work for every vector in <m>V</m> and every scalar in <m>\F</m>. One key point is that because <m>W</m> is closed under taking linear combinations there really <em>is</em> a vector addition map <m>+\colon W\times W \to W</m> and a scalar multiplication map <m>\F\times W\to W</m>. The result of adding two vectors in <m>W</m> is again a vector in <m>W</m>, similarly for scaling.
              </p>

              <p>
                That leaves only VA3 and VA4. We need to show that <m>\0\in W</m> and that if <m>\w\in W</m> then also <m>-\w\in W</m>. Both of these work by choosing appropriate scalings. Since <m>W\neq \emptyset</m> there exists some <m>\w\in W</m>, so then <m>\0 = 0\w \in W</m> (because <m>W</m> is closed under linear combinations). Similarly, if <m>\w\in W</m> then <m>-\w=(-1)\w\in W</m>.
              </p>
            </proof>
          </lemma>

          <definition xml:id="def-span">
            <statement>
              <p>
                If <m>V</m> is a vector space over <m>\F</m> and <m>S\of V</m> then the <em>span of <m>S</m></em>, denoted <m>\linspan(S)</m>, is the set <me>\linspan(S) = \setof[\Big]{\sum_{i=0}^n \l_i\v_i}{\vs\in S, \ls\in \F, n\ge 0}.</me> I.e., it is the collection of all linear combinations of vectors in <m>S</m>.
              </p>
            </statement>
          </definition>

          <example>
            <p>
              If <m>S=\emptyset</m> then <m>\linspan(S) =\{\0\}</m> since the empty linear combination is by convention the <m>\0</m> vector.
            </p>
          </example>

          <example>
            <p>
              If <m>S=\{(1,0,-1)\}\subset \R^3</m> then <m>\linspan(S) =\{\l(1,0,-1) \mid \l\in\R\}</m>. The set of points <m>\linspan(S)</m> is a line through the origin of <m>\R^3</m>.
            </p>
          </example>

          <example>
            <p>
              <m>\R[X]= \linspan\left(\{1, X, X^2, X^3, \ldots, X^i, \ldots \mid i\geq 0\}\right).</m> The set of polynomials of degree at most <m>d</m> is <m>\R[X]_{\leq d}= \linspan\left(\{1, X, X^2, X^3, \ldots, X^d\}\right).</m>
          </p>
          </example>

          <lemma xml:id="lem-span-is-subspace">
            <statement>
              <p>
                If <m>V</m> is a vector space over <m>\F</m> and <m>S\of V</m> then <m>\linspan(S)</m> is a subspace of <m>V</m>.
              </p>
            </statement>

            <proof>
              <p>
                Clearly <m>\0\in \linspan(S)</m>; simply take <m>n=0</m> in the definition. Now if <m>\v,\v'\in \linspan(S)</m> and <m>\l,\mu\in \F</m> we have, for some <m>n, n'\ge 0</m> and <m>\vs</m>, <m>\vps\in S</m>, <m>\ls</m>, <m>\lps\in \F</m>, <me>\v = \sum_{i=1}^n \l_i \v_i \qquad \text{and} \qquad \v' = \sum_{i=1}^{n'} \l'_i \v'_i.</me> Thus we have <me>\l\v+\mu\v' = \l\sum_{i=1}^n \l_i \v_i + \mu \sum_{i=1}^{n'} \l'_i \v'_i \in \linspan(S).</me>
              </p>
            </proof>
          </lemma>

          <example>
            <p>
              Example <xref ref="ex:R[X]span" /> implies that the polynomials of degree at most <m>d</m> form a subspace of <m>\R[X]</m>.
            </p>
          </example>

          <remark>
            <p>
              By contrast, the set of polynomials of degree exactly <m>d</m> do not form a subspace of <m>\R[X]</m>. Why not?
            </p>
          </remark>

        </section>

        <section xml:id="sec-linear-maps-again">
          <title>Linear Maps</title>
    
          <p>
            We don't need a rough idea of what a linear map is—it's just what we defined it to be earlier.
          </p>

          <definition xml:id="def-linear-map-2">
            <statement>
              <p>
                If <m>V,W</m> are vector spaces over <m>\F</m> then we say that a function <m>T:V\to W</m> is <em>linear</em> if it preserves linear combinations. That is to say, for all <m>\u,\v\in V</m> and <m>\l,\mu\in \F</m> we have <me>T(\l\u+\mu\v) = \l T(\u) + \mu T(\v).</me>
              </p>
            </statement>
          </definition>

          <question>
            <p>
              Prove that Definition <xref ref="def-linear-map-2" /> is equivalent to requiring that for all <m>\u,\v\in V</m> and <m>\l \in \F</m> we have <m>T(\u+\v) = T(\u) + T(\v)</m> and <m>T(\l\v)=\l T(\v)</m>.
            </p>
          </question>

          <example>
            <p>
              <ol>
                <li>
                  <p>
                    <m>T:\R^3\to \R</m> given by <m>T((x, y, z)) = 3x+2y+z</m>.
                  </p>
                </li>

                <li>
                  <p>
                    If <m>A\in M_{m\by n}(\F)</m> then we can define a linear map <m>M_A:\F^n\to \F^m</m> by doing matrix multiplication. To be completely explicit we define, for <m>\v=(v_1,v_2,\dots,v_n)\in \F^n</m>, <me>M_A(\v) = A\v = \parens[\Big]{\sum_{j=1}^n A_{i j} v_j }_{i=1}^m.</me> It is a fact, that we will prove soon, that <em>every</em> linear map from <m>\F^n</m> to <m>\F^m</m> is of the form <m>M_A</m> for some matrix <m>A\in M_{m\by n}(\F)</m>.
                  </p>
                </li>

                <li>
                  <p>
                    Let <m>C[0,1]=\{f:[0,1]\to\R : f</m> is continuous<m>\}</m>. Define <m>I:C[0,1]\to \R</m> by <m>\displaystyle I(f) = \int_{0}^1 f(x) \, dx</m>. Then <m>I</m> is linear by standard facts about integration.
                  </p>
                </li>
              </ol>
            </p>
          </example>

          <definition xml:id="def-lvw">
            <statement>
              <p>
                If <m>V</m> and <m>W</m> are vector spaces over a field <m>\F</m>, we let <m>L(V,W)</m> denote the set of all linear maps from <m>V</m> to <m>W</m>, so <me>L(V,W) = \setof{T:V\to W}{\text{$T$ is linear}}.</me> We have different notation for the special case of <m>L(V,W)</m> where <m>W=\F</m> (which is of course a vector space over <m>\F</m>). We write <m>V\dual \defeq L(V,\F)</m>. These linear maps are especially important because they correspond to the left hand sides of linear equations.
              </p>
            </statement>
          </definition>

          <lemma xml:id="lem-lvw-vector-space">
            <statement>
              <p>
                If <m>V,W</m> are vector spaces over <m>\F</m> then <m>L(V,W)</m> is also a vector space over <m>\F</m> when we define, for <m>T,T'\in L(V,W)</m> and <m>\l\in \F</m>, <me>T+T' : \v \mapsto T(\v)+T'(\v) \qquad \l T: \v \mapsto \l T(\v).</me>
              </p>
            </statement>

            <proof>
              <p>
                The proof is straightforward definition checking. As a representative sample I'll prove that vector addition is associative. For <m>R,S,T \in L(V,W)</m> and <m>\v\in V</m> we have <me>\begin{aligned}
                \parens[\big]{R+(S+T)}(\v) &amp;= R(\v) + (S+T)(\v) \\
                    &amp;= R(\v) + (S(\v)+T(\v)) \\
                    &amp;= (R(\v)+S(\v))+T(\v) \\
                    &amp;= (R+S)(\v)+T(\v) \\
                    &amp;= \parens[\big]{(R+S)+T}(\v),
            
        \end{aligned}</me> where the central equality is associativity of vector addition in <m>W</m> and everything else is the definition of addition in <m>L(V,W)</m>.
              </p>
        
              <p>
                The additive identity in <m>L(V,W)</m> is the linear map <m>Z:V\to W</m> that sends every vector <m>v\in V</m> to <m>\0_W</m>, the zero vector in <m>W</m>, that is, <m>Z(v)=0_W</m> for all <m>v\in V</m>.
              </p><!--</div attr= class="proof">-->
        
              <p>
                There are subspaces naturally associated to a linear map. We define them, then show they are subspaces in an upcoming result
              </p>
            </proof>
          </lemma>

          <definition xml:id="def-range-nullspace">
            <statement>
              <p>
                If <m>T:V\to W</m> is a linear map we define the following subspaces associated to it. <me>\begin{aligned}
                T(V) = \range(T) &amp;\defeq \setof{T(\v)}{\v\in V} \\
                \ker(T) = \nullspace(T) &amp;\defeq \setof{\v \in V}{T(\v)=\0}.
            
        \end{aligned}</me>
              </p>
            </statement>
          </definition>

          <example>
            <p>
              Examples of kernels:
            </p>

            <p>
              <ol>
                <li>
                  <p>
                    for <m>T:\R^3\to \R</m> given by <m>T((x, y, z)) = ax+by+cz</m> where not all of <m>a,b,c</m> are zero yields that <m>\ker(T)</m> is a plane passing through the origin (note that <m>(0.0,0)</m> is a particular solution to <m>ax+by+cz=0</m>).
                  </p>

                  <p>
                    If <m>A\in M_{m\by n}(\F)</m> the kernel of the linear map <m>M_A:\F^n\to \F^m, M_A(\v) = A\v</m> is the set of solutions <m>\x</m> to the <em>homogeneous</em> system of equations <m>A\x=\0</m>.
                  </p>

                  <p>
                    If <m>D:\R[X]\to \R[X]</m> is given by <m>D(p)=\frac{d}{dX}(p)</m> then <m>\ker(D)=R[X]_0=\{r\mid r\in \R\}</m>, the set of constant polynomials
                  </p>
                </li>
              </ol>
            </p>
          </example>

          <example>
            <p>
              Examples of kernels:
            </p>

            <p>
              If <m>T:C[0,1]\to \R</m> is given by <m>T(f)=\int_{0}^1 f(x)dx</m> then the range of <m>T</m> is <m>\range(T) =\R</m>. To see this take any <m>r\in\R</m> and consider the constant function <m>f_r:[0,1]\to\R, f_r(x)=r</m>. Then <m>f_r\in C[0,1]</m> and <m>T(f_r)=\int_{0}^1 f_r(x) dx=\int_{0}^1 r dx= r</m>. In conclusion, every <m>r\in \R</m> is in <m>\range(T)</m>.
            </p>
          </example>

          <lemma xml:id="lem-range-ker-subspaces">
            <statement>
              <p>
                If <m>T\in L(V,W)</m> then <m>T(V)</m> is a subspace of <m>W</m> and <m>\ker(T)</m> is a subspace of <m>V</m>.
              </p>
            </statement>

            <proof>
              <p>
                Consider first vectors <m>T(\u),T(\v)\in T(V)</m>. For <m>\l,\mu\in \F</m> we have <me>\l T(\u) + \mu T(\v) = T(\l \u+\mu \v) \in T(V),</me> so <m>T(V)</m> is closed under taking linear combinations. For <m>\ker(T)</m> we have a criterion for testing membership rather than a way of generating vectors, so the proof looks a little different. Take <m>\u,\v\in \ker(T)</m> and <m>\l,\mu\in \F</m>. We would like to prove that <m>\l\u+\mu\v\in \ker(T)</m>, and to verify this we simply apply <m>T</m>: <me>T(\l\u+\mu\v) = \l T(\u) + \mu T(\v) = \l \0 + \mu \0 = \0+\0 = \0.</me> Thus <m>\ker(T)</m> is closed under taking linear combinations and we are done.
              </p>
            </proof>
          </lemma>

          <example>
            <p>
              The set of constant polynomials is a subspace of <m>\R[X]</m> as it is the kernel of the derivative map <m>D:\R[X]\to \R[X]</m> is given by <m>D(p)=\frac{d}{dX}(p)</m>.
            </p><!--</div attr= class="example">-->
      
            <p>
              We now see that we can use the notion of kernel and image to analyze properties of linear maps such as being injective or being surjective.
            </p>
          </example>

          <definition xml:id="def-surjective">
            <statement>
              <p>
                If <m>A</m> and <m>B</m> are sets and <m>f:A\to B</m> is a function, we say that <m>f</m> is <em>surjective</em> provided that for each <m>b\in B</m> there exists <m>a\in A</m> such that <m>f(a)=b</m>.
              </p>
            </statement>
          </definition>

          <example>
            <p>
              Surjective or not surjective?
            </p>
            
            <p>
              <ol>
                <li>
                  <p>
                    <m>R:\R^2\to \R^2</m> where <m>R</m> is a rotation as in <xref ref="ex:rotation" /> is surjective
                  </p>
                </li>

                <li>
                  <p>
                    <m>T:\R^2\to \R^2</m> where <m>T</m> is a rotation as in <xref ref="ex:reflection" /> is surjective
                  </p>
                </li>

                <li>
                  <p>
                    <m>P:\R^2\to \R^2</m> where <m>P</m> is a projection as in <xref ref="ex:projection" /> is NOT surjective. However if we change the target of the map to be the line <m>L</m> onto which we project then <m>P:\R^2\to L</m> is surjective. The moral here is that whether a map is surjective or not depends on its codomain.
                  </p>
                </li>

              </ol>
            </p>
          </example>

          <p>
            Here is the relation between surjective and range.
          </p>

          <proposition xml:id="prop-">
            <statement>
              <p>
                A linear map <m>T:V\to W</m> is surjective if and only if <m>\range(T)=W</m>.
              </p>
            </statement>

            <proof>
              <p>
                According to Definition <xref ref="def:kerrange" />, <m>\range(T)=\{T(\v)\mid v\in V\}</m>. Thus <m>\range(T)=W</m> if and only if for every <m>\w\in W</m> there exists <m>\v\in V</m> so that <m>\w=T(\v)</m> if and only if <m>T</m> is surjective.
              </p>
            </proof>
          </proposition>
    
          <p>
            Now we switch to injective.
          </p>

          <definition xml:id="def-injective">
            <statement>
              <p>
                If <m>A</m> and <m>B</m> are sets and <m>f:A\to B</m> is a function, we say that <m>f</m> is <em>injective</em> provided that whenever <m>f(x)=f(y)</m> then <m>x=y</m>.
              </p>
            </statement>
          </definition>

          <example>
            <p>
              Injective or not injective?
            </p>

            <p>
              <ol>
                <li>
                  <p>
                    <m>D:\R[X]\to \R[X]</m> where <m>D(p)=p'</m> is the derivative map is NOT injective. Indeed the constant polynomials <m>p(x)=5</m> and <m>q(x)=7</m> satisfy <m>D(p)=D(q)=0</m> but of course <m>p\neq q</m>.
                  </p>
                </li>

                <li>
                  <p>
                    the map <m>C:\R[X]\to \R^\N</m> which takes a polynomial <m>p(X)=a_0+a_1X+a_2X^2+\cdots a_dX^d</m> to its sequence of coefficients <m>C(p)=(a_0,a_1,a_2,\ldots, a_d, 0, 0,0, \ldots)</m> is injective.
                  </p>
                </li>
              </ol>
            </p>
          </example>

          <p>
            There is a close relation between injective and kernel.
          </p>

          <theorem xml:id="thm-injective-iff-trivial-kernel">
            <statement>
              <p>
                A linear map <m>T:V\to W</m> is injective if and only if <m>\ker(T)=\{\0_V\}</m>.
              </p>
            </statement>

            <proof>
              <p>
                Coming soon to an OER near you!
              </p>
            </proof>
          </theorem>


        </section>

      </chapter>

      <chapter xml:id="ch-vector-spaces-linear-maps">
        <title>Vector Spaces and Linear Maps: Revisited</title>
        
        <section xml:id="sec-bases">
          <title>Linear Independence, Spanning Sets, and Bases</title>
          
          <p>
            We now introduce three related notions that you are probably familiar with from a previous course; linear independence, spanning sets, and bases.
          </p>
    
          <p>
            Linear independence has to do with whether one can write the same vector <m>\w\in \linspan(S)</m> as a linear combination of vectors in <m>S</m> in two <em>different</em> ways <me>\begin{aligned}
    \sum_{i=1}^na_i\v_i =\w =\sum_{i=1}^nb_i\v_i.
    \end{aligned}</me> This leads to considering an equation of the form <m>\sum_{i=1}^n(a_i-b_i)\v_i=\0</m>.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>V</m> is a vector space over <m>\F</m> and <m>S\of V</m> then <m>S</m> is <em>linearly independent</em> if every linear combination of (distinct) vectors from <m>S</m> equaling <m>\0</m> has all its coefficients equal to <m>0</m>. In other words for all <m>\vs\in S</m>, <m>\ls\in \F</m> with <m>\v_i\not=\v_j</m> for <m>i\not= j</m> we have <me>\Slv = \0 \quad\implies\quad \l_1=\l_2=\dots=\l_n=0.</me>
          </p><!--</div attr= class="definition">-->
    
          <p>
            We note that the definition applies when <m>n=1</m> (and indeed 0). If <m>\0\in S</m> then <m>S</m> is not linear independent since we can take <m>\l_1=1</m> and <m>\v_1=\0\in S</m> and have <m>\l_1\v_1 =\0</m> while <m>\l_1\not =0</m>.
          </p>
    
    <!-- div attr= class="example"-->
    <!-- div attr= class="compactenum"-->
          <p>
            The set <m>\set{(1,0),(0,1)} \of\R^2</m> is linearly independent since <me>\l(1,0) + \mu(0,1) = (0,0) \implies (\l,\mu)=(0,0) \implies \l=0,\mu=0.</me> Similarly <m>\set{\e_1,\e_2,\e_3}\of \R^3</m> is linearly independent.
          </p>
    
          <p>
            In <m>\R^3</m> the vectors <m>(1,0, 0), (2,1,0)</m> and <m>(3,2,1)</m> are linearly independent. Indeed, <me>\l_1(1,0, 0)+\l_2(2,1,0)+\l_3(3,2,1)=\0</me> if and only if <me>\systeme{\l_1+2\l_2+\l_3= 0, \l_2+2\l_3 =0, \l_3=0 }</me> which yields <m>\l_1=\l_2=\l_3=0</m>.
          </p>
    
          <p>
            <m>\set{\sin,\cos}\of \R^\R</m> is linearly independent since if <m>\l \sin + \mu \cos = 0</m> (the right hand side here is the 0 function) then in particular <m>(\l \sin + \mu \cos)(0) = \mu = 0</m> and <m>(\l \sin + \mu \cos)(\pi/2) = \l = 0</m>.
          </p>
    
          <p>
            <m>\set{1,\sqrt 2} \of \R</m> is linearly independent when we think of <m>\R</m> as a vector space over <m>\Q</m>. Suppose <m>\l,\mu\in \Q</m> satisfy <m>\l 1 + \mu \sqrt 2=0</m>. If <m>\mu=0</m> we have <m>\l 1 = \l=0</m>. On the other hand if <m>\mu\not=0</m> then <m>\sqrt2 = -\l/\mu \in \Q</m> which we know is impossible.
          </p>
    
          <p>
            The set <m>S=\setof{\cos(x)^n}{n\in \N} \of \R^\R</m> is linearly independent. A linear combination from <m>S</m> equaling the <m>0</m> function would be a finite sequence <m>\l_0,\ls\in \R</m> such that <me>\l_0 + \l_1\cos(x) + \dots + \l_n \cos^n(x) = 0</me> for all <m>x\in \R</m>. In other words the polynomial <m>p = \sum_{i=0}^n \l_iX^i \in \R[X]</m> would satisfy <m>p(\cos(x))=0</m> for all <m>x\in \R</m>. Thus every value of <m>\cos(x)</m> would have to be a root of <m>p</m>. We know that <m>\cos</m> takes infinitely many values and the only polynomial with infinitely many roots is the <m>0</m> polynomial, so our original linear combination must have been trivial.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="example">-->
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>V</m> is a vector space over <m>\F</m> and <m>S\of V</m> then <m>S</m> is <em>linearly dependent</em> if <m>S</m> is not linearly independent.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
    <!-- div attr= class="compactenum"-->
          <p>
            A single vector <m>\{\v\}</m> is linearly dependent if and only if <m>\v=\0</m>.
          </p>
    
          <p>
            Two vectors form a linearly dependent set <m>\{\u,\v\}</m> if and only if one of the vectors is a scalar multiple of the other.
          </p>
    
          <p>
            If three vectors form a linearly dependent set <m>\{\u,\v, \w\}</m> it does not follow that one of them must be a scalar multiple of another. For example the set <m>(0,1), (1,0), (1,1)\}</m> is linearly dependent since <m>1\, (0,1)+ 1\,(1,0)+ (-1)\, (1,1)=(0,0)</m> but no vector from this set is a scalar multiple of another.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="example">-->
    
          <p>
            We now show how the property of being linearly independent passes from a set to its subsets.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
             Let <m>V</m> be a vector space over <m>\F</m>.
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            If <m>S\of T\of V</m> and <m>T</m> is linearly independent then <m>S</m> is linearly independent.
          </p>
    
          <p>
            If <m>S\of V</m> is linearly independent then for all <m>\v\in \linspan(S)</m> there is a unique way of writing <m>\v</m> as a linear combination from <m>S</m> in the following sense: for all <m>\v\in \linspan(S)</m> there is a unique finite subset <m>\set{\vs} \of S</m> and unique corresponding coefficients <m>\ls\in \F\setminus\{0\}</m> such that <m>\Slv = \v</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            The first part is immediate since any linear combination from <m>S</m> is also a linear combination from <m>T</m>, so any linear combination from <m>S</m> equaling <m>\0</m> is also a linear combination from <m>T</m> equaling <m>\0</m>, so is, by hypothesis, trivial. For the second part suppose that some vector <m>\0\neq \v\in \linspan(S)</m> can be represented in two ways: <me>\Slv = \v = \Smuv.</me> (We've thrown all the vectors used in either representation into the set <m>\set{\vs}</m>.) Subtracting the right hand side from the left we have <me>\sum_{i=1}^n (\l_i-\mu_i) \v_i = \0,</me> and thus, since <m>S</m> is linearly independent, <m>\l_i=\mu_i</m> for all <m>i</m>. Now, eliminating those <m>v_i</m> for which <m>\l_i=0</m> from the set <m>\set{\vs}</m> gives the claimed uniqueness.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="qu"-->
          <p>
            Let <m>V</m> be a vector space over <m>\F</m>. Prove that if <m>S\of T\of V</m> and <m>S</m> is linearly dependent then <m>T</m> is linearly dependent.
          </p><!--</div attr= class="qu">-->
    
    <!-- div attr= class="definition"-->
          <p>
            A subset <m>S</m> of a vector space <m>V</m> is called <em>spanning</em> (or a <em>spanning set</em>) if <m>\linspan(S)=V</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
    <!-- div attr= class="compactenum"-->
          <p>
            The set <m>\set{\e_1=(1,0,0),\e_2=(0,1,0),\e_3=(0,0,1)}\of \R^3</m> is a spanning set for <m>\R^3</m>.
          </p>
    
          <p>
            The set <m>S=\{X^i \mid i\in \Z, i\geq 0\}</m> is a spanning set for <m>\R[X]</m>.
          </p>
    
          <p>
            The set of <m>m\times n</m> matrices <m>E_{ij}</m> that have all entries equal to 0 except for the entry in row <m>i</m> and column <m>j</m> which is 1 is a spanning set for <m>M_{m\times n}(\F)</m>.
          </p>
    
          <p>
            If <m>V</m> is a vector space, then <m>V</m> is a (rather large) spanning set for <m>V</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="example">-->
    
    <!-- div attr= class="lemma"-->
          <p>
             Let <m>V</m> be a vector space.
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            If <m>S\of T\of V</m> then <m>\linspan(S)\of \linspan(T)</m>. In particular if <m>S</m> is spanning then so is <m>T</m>.
          </p>
    
          <p>
             For a subset <m>S\of V</m> and a vector <m>\v\in S</m> we have <m>\linspan(S\cup \{\v\}) = \linspan(S)</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            The first is very straightforward. It boils down to observing that if <m>S\of T</m> then linear combinations from <m>S</m> are in fact linear combinations from <m>T</m>.
          </p>
    
          <p>
            Now suppose <m>S</m> spans <m>V</m> then <me>\begin{aligned}
        V &amp;=\linspan{S} &amp;\text{ because }S\text{ spans } V\\
        &amp;\subseteq \linspan{T} \subseteq \linspan{V} &amp;\text{ because }S\of T\of V\\
        &amp;=V &amp;\text{ because }V\text{ spans } V.
        
    \end{aligned}</me> This shows that <m>\linspan(T)=V</m>.
          </p>
    
          <p>
            For the second claim, first note that one inclusion, <m>\linspan(S)\of \linspan(S\cup \{\v\})</m>, always holds, by part (a) since <m>S\of S\cup\{\v\}</m>. Now that <m>\w\in \linspan(S\cup \{\v\})</m>. By definition of span, there exist <m>v_1,\ldots, v_n\in S</m> and <m>\l_1,\ldots, \l_n,\mu \in \F</m> so that <me>\w=\sum_{i=1}^n \l_i\v_i+\mu\v.</me> Since <m>\v\in \linspan(S)</m> we also have <m>v'_1,\ldots, v'_m\in S</m> and <m>\gamma_1,\ldots, \gamma_m \in \F</m> so that <me>\v=\sum_{j=1}^m \gamma_j\v'_j.</me> Plugging in to the equation above gives <m>\w=\sum_{i=1}^n \l_i\v_i+\sum_{j=1}^m \gamma_j\v'_j</m>, that is, <m>\w\in \linspan(S)</m>. We have shown that <m>\linspan(S\cup \{\v\})\of \linspan(S)</m> which completes the proof that <m>\linspan(S\cup \{\v\})= \linspan(S)</m>.
          </p><!--</div attr= class="proof">-->
    
          <p>
            <xref ref="lem:LI" /> leaves open the possibility that when we enlarge a linearly independent set by adding any vector it could become linearly dependent. Likewise <xref ref="lem:SP" /> leaves open the possibility that when we take out any vector from a spanning set, the remaining set could no longer be spanning. We give names to these situations.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            A set <m>S</m> is <em>maximal linearly independent</em> if <m>S</m> is linearly independent and every strict superset <m>T\supsetneq S</m> is not linearly independent.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="definition"-->
          <p>
            A set <m>S</m> is <em>minimal spanning</em> if <m>S</m> is a spanning set and every strict subset <m>T\subsetneq S</m> is not spanning.
          </p><!--</div attr= class="definition">-->
    
          <p>
            There are two crucial facts that relate the notions of linear independence and spanning, that are recorded in the following lemma.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
             Let <m>I, S</m> be subsets of a vector space <m>V</m>.
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            If <m>I</m> is a maximal linearly independent set then it spans <m>V</m>.
          </p>
    
          <p>
            If <m>S</m> is a minimal spanning set for <m>V</m> then it is linearly independent.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Let <m>I</m> be a maximal linearly independent subset of <m>V</m> and let <m>\v\in V</m>. If <m>\v\in I</m> then certainly <m>\v\in \linspan(I)</m>. Otherwise <m>T=I\cup \set{\v}</m> is a strict superset of <m>I</m> and is therefore not linearly independent. So there is some non-trivial linear combination of vectors from <m>I\cup \set{\v}</m> that is <m>\0</m>. It cannot be that all the vectors involved in this linear combination come from <m>I</m>, else <m>I</m> would not be linearly independent. Thus for some <m>\l\not=0</m> and some <m>\vs\in I</m>, <m>\ls\in \F</m> we have <me>\l\v + \Slv = \0</me> and hence <me>\v = -\frac1\l \Slv \in \linspan(I).</me> For the second part suppose that <m>T</m> is a minimal spanning set and <m>\Slv = \0</m> is a non-trivial linear combination from <m>T</m> with, without loss of generality, <m>\l_1\not=0</m>. Then <me>\v_1 = -\frac1{\l_1} \sum_{i=2}^n \v_i \in \linspan(T\wo\set{\v_1})</me> and hence (by part <xref ref="part:SUT" />) of the spanning lemma applied to <m>S=T\wo\set{\v_1}</m>) <me>\linspan(T\wo\set{\v_1}) = \linspan(T) = V.</me> This contradicts the minimality of <m>T</m>, so no such linear combination can exist.
          </p><!--</div attr= class="proof">-->
    
          <p>
            The Interaction Lemma allows us to make the following definition.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            A <em>basis</em> is a linearly independent spanning set.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="remark"-->
          <p>
             The Interaction <xref ref="lem:interaction" /> suggests how to find a basis for a vector space <m>V</m>: start with a spanning set and remove vectors until one arrives at a minimal spanning set, which will be a basis. Alternatively start with a linearly independent set and add vectors until one arrives at a maximal linearly independent set. However this process involves choices (not all of which will work). So there can be more than one basis for <m>V</m>.
          </p><!--</div attr= class="remark">-->
    
    <!-- div attr= class="example"-->
          <p>
            For example, consider the following spanning set for <m>\R^3</m>: <me>S=\{(1,4,5), (2,2,0), (3,0,0), (5,2,0)\}</me> The following subsets are still spanning (in fact minimal spanning) sets for <m>\R^3</m> <me>\{(1,4,5), (2,2,0), (3,0,0)\},\quad  \{(1,4,5), (2,2,0), (5,2,0)\}, \quad \{(1,4,5), (3,0,0), (5,2,0)\}.</me> Thus all of the above sets are bases for <m>\R^3</m>. However the subset <m>S'=\{(2,2,0), (3,0,0), (5,2,0)\}</m> of <m>S</m> is not a spanning set for <m>\R^3</m> since any vector in <m>\linspan(S')</m> has the last coordinate equal to 0.
          </p><!--</div attr= class="example">-->
    
          <p>
            The following theorem comes at the heart of the study of linear independence in vector spaces. It is called a lemma because of its ubiquity. It generalizes <xref ref="rem:basis" />.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
            If <m>V</m> is a vector space, <m>S\of V</m> is a spanning set, and <m>I\of V</m> is a finite linearly independent set then there exists a spanning set <m>T</m> with <m>I \of T \of I\cup S</m> and <m>\abs{T}\le \abs{S}</m>.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We prove the result by induction on <m>\abs{I}</m>. If <m>\abs{I}=0</m> then <m>I=\emptyset</m> and we can take <m>T=S</m>. Otherwise <m>\abs{I}&gt;0</m>. Pick <m>\v\in I</m> and set <m>I' = I\wo\set{\v}</m>. By induction there exists a spanning set <m>T'</m> with <m>I' \of T' \of I'\cup S</m> with <m>\abs{T'}\le \abs{S}</m>. If <m>\v\in T'</m> then there is nothing further to do; we can take <m>T=T'</m>. Otherwise, our plan now is to insert <m>\v</m> into <m>T'</m> and kick out one element of <m>T'</m>, <em>but not an element of <m>I</m></em>, so that the result is still spanning. Since <m>T'</m> is spanning we have some <m>\vs\in T'</m>, <m>\ls\in \F</m> such that <me>\v = \Slv.</me> I claim that for some <m>\v_i\in T'\wo I'</m> we have <m>\l_i\not =0</m>, for otherwise <m>\v-\Slv=\0</m> would be a non-trivial linear combination of vectors from <m>I</m> equalling <m>\0</m>, a contradiction to the linearly independence of <m>I</m>. Now note that <me>\v_i = \frac1{\l_i}\parens[\Big]{\v - \sum_{j\not=i} \l_j\v_j} \in \linspan(T' \cup \set{\v} \wo \set{\v_i}),</me> so <me>\linspan(T'  \cup \set{\v} \wo \set{\v_i}) = \linspan(T'\cup\set{\v}) = V.</me> Thus we can take <m>T= T'  \cup \set{\v} \wo \set{\v_i}</m>. It is spanning, and it is easy to check that it satisfies the other conditions.
          </p><!--</div attr= class="proof">-->
    
          <p>
            We define the crucial notion of finite dimensionality and then draw some fundamental conclusions from the Steinitz exchange lemma in the theorem following.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>V</m> is a vector space with a finite spanning set then we say <m>V</m> is <em>finite dimensional</em>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="theorem"-->
          <p>
             Let <m>V</m> be a finite dimensional vector space.
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            All bases of <m>V</m> have the same size.
          </p>
    
          <p>
            Any spanning set <m>S</m> of <m>V</m> contains a basis.
          </p>
    
          <p>
            Any linearly independent set <m>I\of V</m> is contained in a basis.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            For a) suppose that <m>B_1</m> and <m>B_2</m> are bases of <m>V</m>. By symmetry it suffices to prove that <m>\abs{B_1}\le \abs{B_2}</m>. Since <m>B_1</m> is (in particular) linearly independent, and <m>B_2</m> is spanning, the Steinitz exchange lemma tells us that there exists a spanning set <m>T</m> with <m>B_1\of T</m> and <m>\abs{T}\le \abs{B_2}</m>. We thus have <m>\abs{B_1}\le \abs{T} \le \abs{B_2}</m>.
          </p>
    
          <p>
            For b), let <m>S\of V</m> be spanning. By induction on <m>\abs{S}</m> it is easy to prove that <m>S</m> contains a minimal spanning set. [If <m>\abs{S}=0</m> then <m>S</m> itself is clearly minimal; if <m>\abs{S}&gt;0</m> then either <m>S</m> is minimal or it contains a strict subset <m>S'\subsetneqq S</m> that is spanning. By induction <m>S'</m> contains a minimal spanning set, which is a minimal spanning set contained in <m>S</m>.] Minimal spanning sets are bases by the Interaction Lemma.
          </p>
    
          <p>
            Finally, let <m>I\of V</m> be linearly independent and let <m>S</m> be a finite spanning set for <m>V</m>. (Such an <m>S</m> exists by the definition of finite dimensionality.) By part b) there is a basis <m>B</m> contained in <m>S</m>. By the Steinitz exchange property there is a spanning set <m>T</m> with <m>I\of T</m> and <m>\abs{T}\le \abs{B}</m>. By b) we know that <m>T</m> contains a basis, but unless that basis is exactly <m>T</m> then we would have found a basis for <m>V</m> strictly smaller than <m>B</m>, contradicting a). Thus <m>T</m> is a basis containing <m>I</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="corollary"-->
          <p>
            Let <m>V</m> be a finite dimensional vector space with <m>\dim V=d</m>. Then
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            any spanning set <m>S</m> of <m>V</m> such that <m>S</m> has <m>d</m> elements is a basis of <m>V</m>
          </p>
    
          <p>
            any linearly independent set <m>S</m> of <m>V</m> such that <m>S</m> has <m>d</m> elements is a basis of <m>V</m>
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="corollary">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Suppose a spanning set <m>S</m> of <m>V</m> has <m>d</m> elements. By <xref ref="thm:dim" /> (2), <m>S</m> contains a basis <m>B</m> and by <xref ref="thm:dim" /> (1) <m>B</m> has <m>d=\dim V</m> elements. Since <m>S</m> has <m>d</m> elements the only subset of <m>S</m> that has <m>d</m> elements is <m>S</m> itself so <m>S=B</m>. Thus <m>S</m> is a basis since <m>B</m> was.
          </p>
    
          <p>
            Suppose a lienarly set <m>I</m> of <m>V</m> has <m>d</m> elements. By <xref ref="thm:dim" /> (3), <m>I</m> is contained a basis <m>B</m> and by <xref ref="thm:dim" /> (1) <m>B</m> has <m>d=\dim V</m> elements. Since <m>B</m> has <m>d</m> elements the only subset of <m>B</m> that has <m>d</m> elements is <m>B</m> itself so <m>I=B</m>. Thus <m>I</m> is a basis since <m>B</m> was.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="corollary"-->
          <p>
            Let <m>V</m> be a finite dimensional vector space and let <m>W</m> be a subspace of <m>V</m>. Then <m>W</m> is a finite dimensional vector space and <m>\dim W\leq \dim V</m>.
          </p><!--</div attr= class="corollary">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Let <m>B_W</m> be a basis of <m>W</m>. Then <m>B_W</m> is a linearly independent subset of <m>W</m> and hence also of <m>V</m> since <m>W\of V</m>. By <xref ref="thm:dim" /> (3) <m>V</m> has a basis <m>B_V</m> so that <m>B_W\of B_V</m>. Since <m>B_V</m> is finite, so is <m>B_W</m> and moreover <m>\dim W=\# B_W\leq B_V=\dim V</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="example"-->
          <p>
            A basis for <m>\F^n</m> is given by the vectors <m>\{\e_1,\ldots, \e_n\}</m>, ehere <m>\e_i=(0, \ldots, 0,1, 0,\ldots, 0)</m> and <m>\e_i</m> has its unique entry equal to 1 in position <m>i</m>. This is called the <term>standard basis</term>. Thus <m>\dim(\F^n)=n</m>.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="example"-->
          <p>
            A basis for <m>\C</m> as a vector space over <m>\R</m> is <m>\{1, i\}</m> so that <m>\dim_\R(\C)=2</m>. A basis for <m>\C</m> as a vector space over <m>\C</m> is <m>\{1\}</m> so that <m>\dim_\C(\C)=1</m>.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="example"-->
          <p>
             Let's find a basis for the following subspace of <m>\R^4</m>: <me>V=\left \{(x_1,x_2,x_3,x_4)\mid \begin{cases}\systeme{ x_1-2x_3=0,x_1+x_2+x_4=0}\end{cases}\right\}.</me> We know that <m>V</m> is a subspace since it is the kernel of the linear map <me>T:\R^4\to \R^2, T(x_1,x_2,x_3,x_4)=(x_1-2x_3,x_1+x_2+x_4)=\begin{bmatrix} 1 &amp; 0 &amp; -2 &amp; 0\\ 1 &amp; 1 &amp; 0 &amp; 1\end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix}.</me> By <xref ref="lem:kerrangesubspaces" /> the kernels (nullspaces) are subspaces.
          </p>
    
          <p>
            The system <me>\begin{cases}
    \systeme{ x_1-2x_3=0,x_1+x_2+x_4=0}
    \end{cases}</me> is equivalent to the following system in reduced row echelon form <me>\begin{cases}
    \systeme{ x_1-2x_3=0,x_2+2x_3+x_4=0}
    \end{cases}</me> in which the variables <m>x_1, x_2</m> are dependent (correspond to pivots) and the variables <m>x_3, x_4</m> are free. The general solution to this system is <me>\begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix}=\begin{bmatrix}2x_3 \\ -2x_3-x_4 \\ x_3 \\ x_4 \end{bmatrix}=x_3\begin{bmatrix}2 \\ -2 \\ 1 \\ 0 \end{bmatrix} +x_4\begin{bmatrix}0 \\ -1 \\ 0\\ 1 \end{bmatrix}.</me> By the work above the vectors <m>(2,-2,1,0), (0,-1,0,1)</m> span <m>V</m> and are linearly independent so they form a basis for <m>V</m>. In particular, <m>\dim(V)=2</m>.
          </p><!--</div attr= class="example">-->

        </section>

        <section xml:id="sec-lights-out">
          <title>Applications: Lights Out</title>
          
          <p>
            There is an electronic game called Lights Out released by Tiger Toys in 1995. The game board consists of twenty-five lights in a <m>5\by 5</m> grid. Pushing any of the lights causes that light to switch state (from on to off or <em>vice versa</em>) and also causes all the lights in the same row or column as the one being pushed to switch state. One is presented with a pattern of lights, and the objective is to press some of the lights so as to switch all the lights off.
          </p>
    
          <p>
            How does a mathematician play the game? The states of the board are in one-to-one correspondence with the vector space <m>V = M_{5\by 5}(\F_2)</m>. Define <m>M_{i j}</m> to be the element of <m>V</m> with <m>1</m>s in every entry of row <m>i</m> and also in every entry of column <m>j</m> and with all other entries <m>0</m>.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
            If the board is in position <m>M</m> then pushing the light at position <m>(i, j)</m> produces the position <m>M+M_{i j}</m>.
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Clear since adding <m>1</m> in <m>\F_2</m> turns a <m>1</m> into a <m>0</m> and a <m>0</m> into a <m>1</m>.
          </p><!--</div attr= class="proof">-->
    
          <p>
            Thus the problem of solving a Lights Out game with starting position <m>M</m> is exactly that of finding a set <m>S</m> if positions such that <me>M + \sum_{(i, j)\in S} M_{i j} = \0,</me> where <m>\0</m> denotes the <m>5\times 5</m> zero matrix. Since <m>a=-a</m> for all <m>a\in F_2</m>, <m>M=-M</m> for all <m>M</m> in <m>M_{5\by 5}(\F_2)</m>, the displayed equation above is the same as <me>M = \sum_{(i, j)\in S} M_{i j},</me> that is, representing <m>M</m> as a <m>\Z_2</m>-linear combination from <m>\setof{M_{i j}}{i, j\in \{1,\ldots, 5\}}</m>. This problem can be solved easily computationally since, as vector spaces, there is no essential difference between <m>M_{5\by 5}(\F_2)</m> and <m>\F_2^{25}</m> (we will soon say more rigorously that <m>M_{5\by 5}(\F_2)</m> and <m>\F_2^{25}</m> are isomorphic vector spaces). All the algorithmic techniques, such as Gaussian elimination, discussed in a first linear algebra course work equally well over any field.
          </p>

        </section>

        <section xml:id="sec-rep-linear-maps-matrices">
          <title>Representing Linear Maps by Matrices</title>
          
          <subsection xml:id="sec-consequences">
            <title>Linear maps are determined by a basis</title>
    
            <p>
              Thinking about bases gives us a lot over control over linear maps, as the following result shows.
            </p>
    
    <!-- div attr= class="theorem"-->
            <p>
               If <m>B</m> is a basis for the vector space <m>V</m> over <m>\F</m> and <m>W</m> is another vector space over <m>\F</m> then linear maps in <m>L(V,W)</m> are determined uniquely by their values on <m>B</m>. To be precise, given an arbitrary function
               <fn>We don't assume that <m>f</m> is linear since we don't even have a definition of linearity for such maps. <m>B</m> is not a vector space.</fn> 
          <m>f:B\to W</m> there exists a unique linear map <m>T:V\to W</m> such that <m>T(\b)=f(\b)</m> for all <m>\b\in B</m>.
            </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
            <p>
              The uniqueness is clear since if <m>\v\in V</m> is written as a linear combination from <m>B</m>, so <m>\v =\sum_1^n\l_i \b_i</m>, then we must have that <me>\label{eq:Tdef}
            T(\v) = \sum_1^n \l_i T(\b_i) = \sum_1^n \l_i f(\b_i).</me> Thus to prove existence we simply need to show that (<xref ref="eq:Tdef" />) defines a linear map. Consider <m>\v,\w\in V</m> and <m>\l,\mu\in \F</m>. We'll need to express <m>\v,\w</m> as linear combinations from <m>B</m>; let <m>\set{\vecs{\b}{n}}</m> be collection of all vectors from <m>B</m> that appear in either representation. Suppose <me>\v = \sum_1^n \l_i \b_i \qquad \w = \sum_1^n \mu_i \b_i.</me> [Notice that some of the <m>\l_i,\mu_i</m> might be <m>0</m>.] Now we have <me>\l\v + \mu\w = \sum_1^n (\l\l_i+\mu\mu_i) \b_i,</me> and this is the unique representation of <m>\l\v+\mu \w</m> using the basis <m>B</m>, except that some of the <m>\l\l_i+\mu\mu_i</m> might be <m>0</m>. Thus <me>T(\l\v+\mu \w) = \sum_1^n (\l\l_i+\mu\mu_i) f(\b_i) = \l\sum_1^n \l_i f(\b_i) + \mu\sum_1^n \mu_i f(\b_i) = \l T(\v)+\mu T(\w).</me>
            </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="example"-->
            <p>
               Recall that <m>\R^n</m> has standard basis <m>B=\{\e_1, \ldots, \e_n\}</m>. Suppose that I consider a function <m>f:B\to \R[X]</m> given by <m>e_i\mapsto X^{i-1}</m>. Then the unique linear function <m>T:\R^n \to \R[X]</m> that extends <m>f</m> in the sense of <xref ref="thm:basis_lin_map" /> is given by <me>T((c_0,\ldots, c_{n-1}))=T\left(c_0\e_1+c_1 \e_2+\cdots+c_{n-1}\e_n\right)=c_0+c_1X+\cdots+c_{n-1}X^{n-1}.</me> Note the beauty of the matter is <xref ref="thm:basis_lin_map" /> ensures <m>T</m> as defined above is a linear map. We do not need to check this provided we invoke the Theorem.
            </p><!--</div attr= class="example">-->
    
          </subsection>
    
          <subsection xml:id="linear-maps-are-represented-by-matrices">
            <title>Linear maps are represented by matrices</title>
    
            <p>
              It is also true that in some sense any linear map between finite dimensional vectors spaces is <q>really</q> a matrix.
            </p>
    
    <!-- div attr= class="definition"-->
            <p>
              Suppose <m>V,W</m> are finite dimensional vector spaces over <m>\F</m> and <m>T:V\to W</m> is linear. Given (ordered) bases <m>B = \set{\vs}, B'=\set{\ws[m]}</m> for <m>V</m> and <m>W</m> respectively then <em>the matrix of <m>T</m> with respect to <m>B,B'</m></em> is the matrix <m>[T]_B^{B'}=A = [a_{ij}]_{i=1}^m\vphantom{[]}_{j=1}^n</m> such that for all <m>i\in [n]</m> we have <me>T(\v_i) = \sum_1^m a_{ji} \w_j.</me> Note that this determines the <m>a_{ij}</m> since <m>T(\v_i)</m> has a unique expression as a linear combination from <m>B'</m>. Note that once we've picked the basis <m>B</m> every vector in <m>v</m> can be thought of as specified by its coefficients with respect to <m>B</m>. If <m>\v=\Slv</m> then we can think of <m>\v</m> as corresponding to the column vector <m>\col{\l_1\\\l_2\\\vdots\\\l_n}</m>. Similarly the image <m>T(\v)</m> can be thought of a a column vector <m>\col{\mu_1\\\mu_2\\\vdots\\\mu_m}</m> if <m>T(\v) = \sum_1^m \mu_j \w_j</m>. The matrix <m>A</m> is chosen so that <me>\col{\mu_1\\\mu_2\\\vdots\\\mu_m} = A \col{\l_1\\\l_2\\\vdots\\\l_n}.</me>
            </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
            <p>
              Let's consider the counterclockwise rotation of angle <m>\theta</m> in the plane. Denote this by <m>R_\theta:\R^2 \to \R^2</m>. Then <m>R_\theta(1,0)=(\cos \theta, \sin \theta)</m> and <m>R_\theta(0,1)=(-\sin \theta, \cos \theta)</m>, thus the matrix of <m>R_\theta</m> with respect to the standard basis is <me>[R_\theta]=
    \begin{bmatrix} 
    \cos \theta &amp; -\sin \theta\\
    \sin \theta &amp; \cos \theta
    \end{bmatrix}.</me>
            </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="example"-->
            <p>
              Let's consider the derivative map. Denote this by <m>V_{10}</m> the vector space of polynomials of degree <m>\leq 10</m> and let <m>D:V_{10}\to V_{10}</m> be the derivative map <m>D(p)=p'</m>. Then a basis for <m>R_{\leq 10}</m> is <m>B=\{1,X, X^2, \ldots, X^{10}\}</m> and the matrix of <m>D</m> with respect to this basis is <me>[D]_B^B=
    \begin{bmatrix} 
    0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 4 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 5 &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 6 &amp; 0 &amp; 0 &amp; 0&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 7 &amp; 0 &amp; 0&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 8 &amp; 0&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 9&amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0&amp; 10\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
    \end{bmatrix}.</me>
            </p><!--</div attr= class="example">-->
    
          </subsection>

        </section>

        <section xml:id="sec-isomorphisms">
          <title>Isomorphisms</title>
          
          <p>
            There is a sense in which the vector space <m>V_{d}</m> of polynomials of degree at most <m>d</m> and <m>\R^{d+1}</m> are similar: each polynomial <m>c_0+c_1X+\cdots+c_dX^d</m> is determined by its coefficient vector <m>(c_0,c_1, \ldots, c_d)\in   \R^{d+1}</m>. More formally, as we have seen in <xref ref="ex:iso" />, there is a linear transformation <m>T:\R^{d+1}\to V_d</m> <me>T((c_0,\ldots, c_{d}))=c_0+c_1X+\cdots+c_{d}X^{d}.</me> This linear map has the property that for every <m>p\in V_d</m> there exists a unique <m>\vv{c}\in\R^{d+1}</m> (the vector of coefficients of <m>p</m>) so that <m>T(\vv{c})=p</m>. This makes <m>T</m> a bijection.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="definition"-->
          <p>
            A function
            <fn>not necessarily linear </fn> 
            is called a <term>bijection</term> if it is both injective and surjective.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="remark"-->
          <p>
             It is a standard fact that a function <m>T:V\to W</m> is a bijection if and only if there exists a function <m>S:W\to V</m> termed the <em>inverse</em> of <m>T</m> which satisfies <m>S(T(\v))=\v</m> for all <m>\v\in V</m> and <m>T(S(\w))=\w</m> for all <m>\w\in W</m>.
          </p><!--</div attr= class="remark">-->
    
    <!-- div attr= class="definition"-->
          <p>
            Let <m>V</m>, <m>W</m> be vector spaces over the same field <m>\F</m> and let <m>T:V\to W</m> be a linear map. We say that <m>T</m> is an <term>isomorphism</term> if there exists a linear map <m>S:W\to V</m> so that <m>S(T(\v))=\v</m> for all <m>\v\in V</m> and <m>T( S(\w))=\w</m> for all <m>\w\in W</m>.
          </p>
    
          <p>
            We say that vector spaces <m>V</m>, <m>W</m> are <term>isomorphic</term> and write <m>V\cong W</m> if there exists an <term>isomorphism</term> <m>T:V\to W</m> or <m>T:W\to V</m>.
          </p><!--</div attr= class="definition">-->
    
          <p>
            It turns out (it's a small miracle) that a linear function being an isomorphism is the same as being a bijection.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
             Let <m>V</m>, <m>W</m> be vector spaces over the same field <m>\F</m> and let <m>T:V\to W</m> be a linear map. Then <m>T</m> is invertible if and only if <m>T</m> is a bijection.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            By Remark <xref ref="rem:inverse" /> <m>T:V\to W</m> is a bijection if and only if there exists a function <m>S:W\to V</m> which satisfies <m>S(T(\v))=\v</m> for all <m>\v\in V</m> and <m>T(S(\w))=\w</m> for all <m>\w\in W</m>. To conclude that <m>T</m> is an isomorphism we still have to show that the inverse of <m>T</m>, <m>S</m> is a linear map. Consider <m>\u,\w\in W</m> and <m>\l,\mu\in \F</m>. Then <me>T(S(\l\u+\mu\w))= \l\u+\mu\w=\l T(S(\u))+ \mu T( S(\w))=T(\l S(\u)+\mu S(\w)).</me> Since <m>T</m> is injective we conclude that <m>S(\l\u+\mu\w)=\l S(\u)+\mu S(\w)</m> so <m>S</m> is a linear map.
          </p><!--</div attr= class="proof">-->
    
          <p>
            We have seen a lot of example of vector spaces. It turns out that up to isomorphism finite-dimensional vectors spaces are quite simple. Moreover, the isomorphism class of a finite dimensional vector space is completely determined by its dimension.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>V</m> is a finite dimensional vector space over <m>\F</m>, of dimension <m>n</m> say, then <m>V\iso \F^n</m>.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Let <m>\set{\v_1, \v_2, \dots, \v_{n}}</m> be a basis for <m>V</m>. Define <m>T:\F^n \to V</m> by <me>T((\l_i)_1^n) = \sum_1^n \l_i\v_i.</me> This is clearly a linear map. The kernel of <m>T</m> is <m>\set{\0}</m>, since if <m>\sum_1^n \l_i\v_i = \0</m> then by the linear independence of the <m>\v_i</m> all of the scalars must be <m>0</m>. Thus <m>T</m> is injective by <xref ref="thm:injker" />. Similarly <m>\range(T)=V</m> since every vector in <m>V</m> can be expressed as a linear combination of the <m>\v_i</m>. Thus, by <xref ref="thm:iso" />, <m>T</m> is invertible, meaning exactly that <m>V</m> and <m>\F^n</m> are isomorphic.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="corollary"-->
          <p>
            If <m>V, W</m> are finite dimensional vector spaces over the same field <m>\F</m> then <m>V\cong W</m> if and only if <m>\dim(V)=\dim(W)</m>.
          </p><!--</div attr= class="corollary">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Homework.
          </p><!--</div attr= class="proof">-->
    
          <p>
            Earlier we have seen that linear maps between vector spaces are represented by matrices. More formally we have:
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
             Let <m>V</m>, <m>W</m> be vector spaces over the same field <m>\F</m> with <m>\dim V=n, \dim W=m</m>. Then there is a vector space isomorphism <me>L(V,W)\cong M_{m\times n}(\F).</me>
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            <em>Proof Idea.</em> Fix bases <m>B, B'</m> for <m>V</m> and <m>W</m> respectively. The map <m>\Theta:L(V,W)\to M_{m\times n}(\F)</m> that takes a linear map <m>T:V\to W</m> to the matrix of <m>T</m> with respect to <m>B,B'</m> is an isomorphism (check!).
          </p><!--</div attr= class="proof">-->
    
          <p>
            We can say more about how the isomorphism above interacts with composition of maps.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
            Let <m>T:U\to V</m> and <m>S:V\to W</m> be linear maps and let <m>B, B', B''</m> be bases for <m>U, V, W</m> respectively. Then
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            The function <m>S\circ T</m> given by <m>(S\circ T)(\u)=S(T(\u))</m> is a linear map.
          </p>
    
          <p>
            If <m>[S]_B^{B'}, [T]_{B'}^{B''}</m> are the matrices representing <m>S</m> and <m>T</m> with respect to the given bases then <me>[S\circ T]_{B}^{B''}=[S]_{B'}^{B''} \cdot [T]_{B}^{B'}</me>
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="remark"-->
          <p>
            If you know about rings: the set <m>L(V,W)</m> is actually a ring with respect to the operations of addition of functions and composition of functions <m>(S\circ T)(\v)=S(T(\v))</m>. The above lemma says that the map <m>T\mapsto [T]_B^{B'}</m> defined in the proof of <xref ref="thm:lintransfisomatrices" /> is also a <em>ring isomorphism</em>.
          </p><!--</div attr= class="remark">-->
    
    <!-- div attr= class="remark"-->
          <p>
            If you know about commutative diagrams: We may put together Remarks <xref ref="thm:isoF^n" /> and <xref ref="thm:lintransfisomatrices" /> in the following way: fix bases <m>B</m> for <m>V</m> and <m>B'</m> for <m>W</m> and let <m>\phi_V:\F^n\to V</m> and <m>\phi_W:\F^m\to W</m> be the maps constructed as in <xref ref="thm:isoF^n" /> based on <m>B</m> and <m>B'</m>. Then the following diagram commutes
          </p>
    
          <p>
            <me>\begin{tikzcd}
    \F^n \arrow{r}{\phi_V} \arrow{d}[swap]{\Theta(T)=[T]_B^{B'}}  &amp; V \arrow{d}{T} \\
    \F^m  \arrow{r}{\phi_W} &amp; W.
    \end{tikzcd}</me>
          </p><!--</div attr= class="remark">-->

        </section>

        <section xml:id="sec-rank-nullity">
          <title>Rank-Nullity</title>
          
          <p>
            As we mentioned in the introduction, we will consider all sorts of linear equations—anything of the form <me>T(\x) = \w</me> where <m>T:V\to W</m> is a linear map. The most fundamental result about such equations is the Rank-Nullity formula. It relates the dimension of the range of <m>T</m> (the set of <m>\w</m> for which there exists a solution to the equation), also called the rank of <m>T</m>, to the dimensions of <m>V</m> and <m>\ker(T)</m>.
          </p>
    
    <!-- div attr= class="example"-->
          <p>
            In <xref ref="ex:kerneldim" /> we analyzed the linear map <me>T:\R^4\to \R^2, T(x_1,x_2,x_3,x_4)=(x_1-2x_3,x_1+x_2+x_4)=\begin{bmatrix} 1 &amp; 0 &amp; -2 &amp; 0\\ 1 &amp; 1 &amp; 0 &amp; 1\end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix}.</me> and found <m>\dim \ker(T)=2</m>. Towards this end it was helpful to put the matrix of this linear transformation in RREF using <em>row operations</em>.
          </p>
    
          <p>
            Let us consider now the range of <m>T</m>, which is a subspace of <m>\R^2</m> by <xref ref="lem:kerrangesubspaces" />. The following is a crucial observation: <m>\range(T)=\linspan\{T(\e_1), T(\e_2), T(\e_3),  T(\e_4)\}</m>. Because <me>\{T(\e_1), T(\e_2), T(\e_3),  T(\e_4)\}\subset \range(T),</me> we have <m>\linspan\{T(\e_1), T(\e_2), T(\e_3),  T(\e_4)\}\subseteq \range(T)</m> by the Spanning Lemma <xref ref="lem:SP" />. Conversely, if <m>\w\in \range(T)</m> then <m>\w=T(\v)</m> for some <m>\v\in \R^4</m> which can be written as <m>\v=v_1\e_1+v_2\e_2+ v_3\e_3+v_4\e_4</m>. Then <me>\begin{aligned}
     \w &amp;=T(v_1\e_1+v_2\e_2+ v_3\e_3+v_4\e_4)=v_1T(\e_1)+v_2T(\e_2)+ v_3T(\e_3)+v_4T(\e_4)\\
     &amp;\in\linspan\{T(\e_1), T(\e_2), T(\e_3),  T(\e_4)\}.
     
    \end{aligned}</me>
          </p>
    
          <p>
            Rrecall that <m>T(\e_1), T(\e_2), T(\e_3),  T(\e_4)</m> are the columns of the matrix of the linear transformation <m>T</m> <me>A= \begin{bmatrix} 1 &amp; 0 &amp; -2 &amp; 0\\ 1 &amp; 1 &amp; 0 &amp; 1\end{bmatrix}.</me> We want to find a basis for the vector space spanned by the columns of <m>A</m>. We can do so by using column ofperations to put <m>A</m> in column echelon form <me>A  \xrightarrow{CEF} \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 0\end{bmatrix}.</me> We thus see that <m>\range(T)</m> has basis <m>\left\{ \begin{bmatrix} 1 \\0\end{bmatrix}, \begin{bmatrix} 0 \\1\end{bmatrix}\right\}</m>. Thus <m>\dim \range(T)=2</m>.
          </p>
    
          <p>
            Note that <m>\dim \ker(T)+\dim \range(T)=2+2=4=\dim \R^4</m>, where <m>\R^4</m> is the domain of <m>T</m>. This kind of relationship holds in general by the Rank-Nullity <xref ref="thm:ranknullity" />.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>T:V\to W</m> is a linear map and <m>V</m> is finite dimensional then <me>\dim(V) = \dim(\ker(T)) + \dim(\range(T)).</me>
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Let <m>\set{\us[k]}</m> be a basis for <m>\ker(T)\of V</m> and <m>\set{\ws[\ell]}</m> a basis for <m>\range(T)\of W</m>. By definition there exist vectors <m>\set{\vs[\ell]}</m> such that for each <m>i</m> we have <m>T(\v_i)=\w_i</m>. We claim that <m>B = \set{\us[k],\vs[\ell]}</m> is a basis for <m>V</m>, and hence that <m>V</m> has dimension <m>k+\ell = \dim(\ker(T))+\dim(\range(T))</m>. Let's show first that <m>B</m> is linearly independent. To that end suppose that <m>\ls[k]</m> and <m>\mus[\ell]</m> in <m>\F</m> are such that <me>\sum_1^k \l_i \u_i + \sum_1^\ell \mu_j \v_j = \0.</me> Applying <m>T</m> we get <m>\sum_1^\ell \mu_j \w_j = \0</m> (since each <m>\u_i</m> is in the kernel of <m>T</m>) so by the linear independence of the <m>\w_j</m> we have <m>\mu_1=\mu_2=\dots=\mu_\ell = 0</m>. This tells us in turn that <m>\sum_1^k \l_i \u_i = \0</m>, forcing <m>\l_1=\l_2=\dots=\l_k=0</m>.
          </p>
    
          <p>
            Now we show that <m>B</m> spans <m>V</m>. Given <m>\v\in V</m> we can represent <m>T(\v)</m> as a linear combination <m>\sum_1^\ell \mu_j \w_j</m> of the <m>\w_j</m>. Now <m>\v-\sum_1^\ell \mu_j\v_j</m> is in <m>\ker(T)</m> since <me>T\parens[\Big]{\v-\sum_1^\ell \mu_j\v_j} = T(\v) - \sum_1^\ell \mu_j \w_j = \0.</me> Having shown that <m>\v-\sum_1^\ell \mu_j\v_j</m> is in <m>\ker(T)</m> we get, for some scalars <m>\ls[k]</m>, that <me>\v-\sum_1^\ell \mu_j\v_j = \sum_1^k \l_i \u_i \qquad\text{so}\qquad \v = \sum_1^k \l_i \u_i + \sum_1^\ell \mu_j\v_j,</me> and <m>B</m> spans <m>V</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="corollary"-->
          <p>
             If <m>V</m> and <m>W</m> are finite dimensional vector spaces with <m>\dim V=\dim W</m> then the following are equivalent
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            <m>T</m> is injective
          </p>
    
          <p>
            <m>T</m> is surjective
          </p>
    
          <p>
            <m>T</m> is bijective
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="corollary">-->
    
    <!-- div attr= class="proof"-->
          <p>
            <m>(1)\Rightarrow (2)</m> Suppose <m>T</m> is injective. Then <m>\ker(T)=\{0_V\}</m> has <m>\dim \ker(T)=0</m>. By Rank-Nullity, it follows that <m>\dim \range(T)=\dim V=\dim W</m>. Since <m>\range(T)\subseteq W</m> and <m>\dim \range(T)=\dim W</m> we conclude <m>\range(T)=W</m>, in other words <m>T</m> is surjective.
          </p>
    
          <p>
            <m>(2)\Rightarrow (3)</m> Suppose <m>T</m> is surjective. Then <m>\range(T)=W</m> so <m>\dim \range(T)=\dim W</m>. By Rank-Nullity, it follows that <m>\dim \ker(T)=0</m> and so <m>\ker(T)=\{0_V\}</m>. Therefore <m>T</m> is injective and therefore also bijective.
          </p>
    
          <p>
            <m>(3)\Rightarrow (1)</m> follows by definition.
          </p><!--</div attr= class="proof">-->

        </section>

        <section xml:id="sec-poly-interpolation">
          <title>Application: Polynomial Interpolation</title>
          
          <p>
            Lagrangian polynomial interpolation refers to the following problem:
          </p>
    
          <blockquote>
                  <p>
            Let <m>x_1,\ldots,x_n\in \F</m> be distinct and let <m>y_1,\ldots,y_n\in \F</m>. Find a polynomial <m>f\in \F[x]</m> of degree at most <m>n-1</m> such that <m>f(x_i) = y_i</m> for all <m>i</m>.
          </p>
          </blockquote>
    
          <p>
            Polynomial interpolation questions can be rephrased in terms of solutions of linear systems of equations. Write down a general polynomial of degree at most <m>n-1</m> as <me>f = a_0  + a_1 x+  \cdots + a_{n-1} x^{n-1}.</me> Then we want <m>f</m> to satisfy the system of <m>n</m> equations <m>f(x_i) = y_i</m> (<m>i=1,\ldots,n</m>). Written out more explicitly, we need to find <m>a_{n-1},\ldots,a_0\in \F</m> such that <me>\begin{aligned}
    a_0 +  a_1 x_1 \cdots + a_{n-1}x_1^{n-1}   &amp;= y_1\\
    a_0 +  a_1 x_2\cdots + a_{n-1}x_2^{n-1}  &amp;= y_2\\
    &amp;\vdots\\
    a_0 +  a_1 x_n \cdots + a_{n-1} x_{n-1}^{n-1} &amp;= y_n
    \end{aligned}</me> Alternately, in matrix form, we want to solve the following equation for <m>(a_0, \ldots, a_{n-1})</m>: <me>\begin{pmatrix}
    1 &amp;  x_1 &amp;  \cdots &amp; x_1^{n-1} \\
    1 &amp;  x_2 &amp;  \cdots &amp; x_2^{n-1} \\
    \vdots &amp;  \ddots &amp; \vdots &amp; \vdots\\
    1 &amp;  x_n &amp;  \cdots &amp; x_{n}^{n-1}  \end{pmatrix}
    \begin{pmatrix} a_{0} \\a_1 \\ \vdots\\a_{n-1}\end{pmatrix}=\begin{pmatrix} y_1 \\y_{2}\\\vdots\\y_n\end{pmatrix}</me> The matrix on the left is called a <em>Vandermonde matrix</em>.
          </p>
    
          <p>
            As we have seen before, wherever there is a matrix there is a linear map.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            Fix <m>x_1, \ldots, x_n\in \F</m>. The <em>evaluation map</em> is the linear map <m>\text{ev}:V_{n-1}\to \F^n</m> given by <me>\text{ev}(p(x))=(p(x_1),p(x_2),\ldots, p(x_n)).</me>
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="remark"-->
          <p>
            A basis for <m>V_{n-1}</m>, the subspace of polynomials of degree at most <m>n-1</m> in <m>\F[X]</m>, is <m>B=\{1, X,\ldots, X^{n-1}\}</m>. We will use the standard basis <m>B'</m> for <m>\F^n</m>. Then the matrix for the evaluation map in these bases is the Vandermonde matrix <me>[\text{ev}]_B^{B'}=\begin{pmatrix}
    1 &amp;  x_1 &amp;  \cdots &amp; x_1^{n-1} \\
    1 &amp;  x_2 &amp;  \cdots &amp; x_2^{n-1} \\
    \vdots &amp;  \ddots &amp; \vdots &amp; \vdots\\
    1 &amp;  x_n &amp;  \cdots &amp; x_{n}^{n-1}  \end{pmatrix}.</me>
          </p><!--</div attr= class="remark">-->
    
          <p>
            Next we describe a different basis for <m>V_{n-1}</m>.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
             The <em>Lagrange basis</em> for the vector space <m>V_{n-1}</m> with respect to the nodes <m>x_1,\ldots,x_n\in \F</m> consists of the polynomials <me>\ell_i(x)=\prod_{1\leq j\leq n, j\neq i} \frac{x-x_j}{x_i-x_j}</me> which satisfy <me>\label{eq:lagrange}
    \ell_i(x_j)=\begin{cases} 0 &amp; i\neq j\\ 1 &amp; i=j. \end{cases}</me>
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="proposition"-->
          <p>
            Let <m>x_1,\ldots,x_n\in \F</m> be distinct and let <m>y_1,\ldots,y_n\in \F</m>. There is a unique polynomial <m>f\in \F[x]</m> of degree at most <m>n-1</m> such that <m>f(x_i) = y_i</m> for all <m>1\leq i\leq n</m>, written in terms of the Lagrange basis as <me>f(x)=y_1\ell_1+y_2\ell_2+\cdots +y_n\ell_n.</me>
          </p><!--</div attr= class="proposition">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We first verify that <m>f</m> satisfies <m>f(x_i) = y_i</m> for all <m>1\leq i\leq n</m>. This follows from <xref ref="eq:lagrange" />: <me>f(x_i)=y_1\ell_1(x_i)+y_2\ell_2(x_i)+ \cdots+ y_i\ell_i(x_i)+\cdots +y_n\ell_n(x_i)=y_1\cdot 0+\cdots+y_i\cdot 1+\cdots+y_n\cdot 0 =y_i.</me> Now consider the evaluation map <m>\text{ev}:V_{n-1}\to \F^n</m> given by <m>\text{ev}(p(x))=(p(x_1),p(x_2),\ldots, p(x_n))</m>. We have just shown that this map is surjective and we know <m>\dim V_{n-1}=n=\dim \F^n</m>. According to <xref ref="cor:injsurjbij" /> we deduce that <m>\text{ev}</m> is bijective, so the uniqueness of <m>f</m> follows.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="remark"-->
          <p>
            If we set <m>L</m> to be the Lagrange basis of <m>V_{n-1}</m> and <m>B'</m> to be the standard basis of <m>\F^n</m> then the matrix for the evaluation map is the identity matrix thanks to <xref ref="eq:lagrange" />. This explains the important role this basis plays in interpolation.
          </p><!--</div attr= class="remark">-->

        </section>

      </chapter>

      <chapter xml:id="ch-constructions">
        <title>Constructions of Vector Spaces</title>
        
        <section xml:id="sec-direct-sums">
          <title>Direct Sum of Vector Spaces</title>
          
          <introduction>
            <p>
              We introduce now two constructions of new vector spaces from old.
            </p>
          </introduction>     
    
          <subsection xml:id="internal-direct-sum">
            <title>Internal direct sum</title>
    
            <p>
              Let's first notice that we can add and scale subspaces of a vector space just like we add and scale vectors. To add two subspaces we will in fact adding together elements from the subspaces in all possible ways. Our notation is as follows.
            </p>
    
    <!-- div attr= class="definition"-->
            <p>
              If <m>A,B</m> are subsets of a vector space <m>V</m> over <m>\F</m> and <m>\l \in \F</m> we define <me>A+B = \setof{\vv{a}+\b}{\vv{a}\in A, \b\in B} \qquad \l A = \setof{\l \vv{a}}{\vv{a}\in A}.</me> We make corresponding definitions for more complicated expressions such as <m>A+B+(-12)C</m>.
            </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="lemma"-->
            <p>
              If <m>U</m> and <m>V</m> are subspaces of a vector space <m>W</m> and <m>\l \in \F</m> then <m>U+V</m> and <m>\l U</m> are subspaces of <m>W</m>.
            </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
            <p>
              That <m>U+V</m> is a subspace was proven on homework. Since <m>U</m> is a subspace <m>\l U=U</m> if <m>\l\neq 0</m> and <m>0U=\{\0_W\}</m>. In either case <m>\l U</m> is a subspace of <m>W</m>.
            </p><!--</div attr= class="proof">-->
    
            <p>
              By induction, it follows that any linear combination of subspaces of <m>W</m> is a subspace of <m>W</m>.
            </p>
    
            <p>
              Recall that we introduced the idea of vectors <m>\v_1, \ldots, \v_n</m> being linearly independent to model the fact that ever vector isn <m>\linspan\{\v_1, \ldots, \v_n\}</m> can we written <em>uniquely</em> as a linear combination of <m>\v_1, \ldots, \v_n</m>. Now we want to define a notion similar to linear independence for subspaces.
            </p>
    
    <!-- div attr= class="definition"-->
            <p>
              If <m>U</m> and <m>V</m> are subspaces of a vector space <m>W</m> we say that <m>U+V</m> is an <em>internal direct sum</em> and we denote <m>U\oplus V=U+V</m> if every vector <m>\w\in U+V</m> can be written uniquely as <m>\w=\u+\v</m> with <m>\u\in U</m> and <m>\v\in V</m>.
            </p>
    
            <p>
              More generally, if <m>V_1,\ldots, V_n</m> are subspaces of a vector space <m>W</m> we say that <m>V_1+\cdots+ V_n</m> is an <em>internal direct sum</em> and we denote it <m>V_1\oplus \cdots \oplus V_n</m> if every vector <m>\w\in V_1+\cdots+ V_n</m> can be written uniquely as <m>\w=\v_1+\cdots+\v_n</m> with <m>\v_i\in V_i</m>.
            </p>
    
            <p>
              We can even have infinite direct sums: suppose that <m>(V_i)_{i\in I}</m> is a family of subspaces of a vector space <m>W</m> and any vector <m>\w=\v_1+\cdots+\v_n</m> with <m>\v_i\in V_i</m> can be written uniquely in this form we say we have a direct sum <m>\bigoplus_{i\in I} V_i</m>.
            </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
            <p>
              Let <m>W=\R^3</m> and let <m>U=\{(x,y,0)\mid x,y\in \R\}</m> and <m>V=\{(0,0,0,z)\mid z\in \R\}</m>. Then <m>\R^3=U\oplus V</m>.
            </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="example"-->
            <p>
              Let <m>W=M_{n\times n} (\R)</m> and let <m>U</m> and <m>V</m> be the subspaces of symmetric and antisymmetric matrices respectively. In detail <m>A\in U</m> if and only if <m>A=A^T</m> and <m>A\in V</m> if and only if <m>A=-A^T</m>, where <m>T</m> denotes transpose. Then <m>W=U\oplus V</m> as we can write <me>A=\frac{1}{2}(A+A^T) +\frac{1}{2}(A-A^T)</me> and moreover if <m>A=B+C</m> with <m>B\in U</m> and <m>C\in V</m> then we have <m>A^T=B^T+C^T=B-C</m>. From <me>\begin{aligned}
    A &amp;=B+C\\
    A^T&amp;=B-C
    \end{aligned}</me> we obtain <m>B=\frac{1}{2}(A+A^T)</m> and <m>C=\frac{1}{2}(A-A^T)</m>.
            </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="example"-->
            <p>
              Let <m>T_i</m> be a one-dimensional vector space over <m>\R</m> whose basis vector we call <m>X^i</m>. In short <m>T_i=\R X^i</m>. Then we have <me>\R[X] = \Dsum_{i=0}^\infty T_i.</me>
            </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="theorem"-->
            <p>
               Let <m>U</m> and <m>V</m> be subspaces of a vector space <m>W</m>. Then <m>U+V</m> is a direct sum if and only if <m>U\cap V=\{\0_W\}</m>.
            </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
            <p>
              Suppose <m>U+V</m> is a direct sum and let <m>\w\in U\cap V</m>. Then <m>\w=\0+\w=\w+\0</m> are two different ways of writing <m>\w</m> as a sum of a vector in <m>U</m> and a vector in <m>V</m> unless <m>\w=\0</m>.
            </p>
    
            <p>
              Suppose <m>U\cap V=\{\0_W\}</m>. If <m>\u+\v=\u'+\v'</m> with <m>\u,\u'\in U</m> and <m>\v, \v'\in V</m> then <m>\u-\u'=\v'-\v</m>. The left hand side is in <m>U</m> and the right hand side is in <m>V</m>. So we must have <m>\u-\u'=\v'-\v\in U\cap V</m>, therefore <m>\u-\u'=\v'-\v=\0</m>. So <m>\u=\u'</m> and <m>\v=\v'</m>.
            </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="remark"-->
            <p>
              One place where we (silently) thought about an internal direct sum was in the proof of <xref ref="thm:ranknullity" />. Looking at that proof we see that we really showed that if <m>T:V \to W</m> then <m>V</m> is the internal direct sum of <m>\ker(T)</m> and <m>\linspan(\set{\vs[\ell]})</m>.
            </p><!--</div attr= class="remark">-->
    
    <!-- div attr= class="theorem"-->
            <p>
              Let <m>U</m> and <m>V</m> be subspaces of a vector space <m>W</m>. If <m>B</m> is a basis for <m>U</m> and <m>B'</m> is a basis for <m>V</m> then <m>B\cup B'</m> is a basis for <m>U\oplus V</m>. In particular, <me>\dim U\oplus V=\dim U+\dim V.</me>
            </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
            <p>
              The last part follows from the formula <m>\dim U + V=\dim U+\dim V-\dim (U\cap V)</m> proven on the homework and which gives <m>U\cap V=\{\0\}</m> so that <m>\dim (U\cap V)=0</m>.
            </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="theorem"-->
            <p>
              Let <m>U</m> is a subspace of a vector space <m>W</m> then there exists a subspace <m>V</m> of <m>W</m> so that <m>U\oplus V=W</m>.
            </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
            <p>
              Let <m>B_U</m> be a basis for <m>U</m>. Since <m>B_U</m> is an independent set, there exists a basis <m>B</m> of <m>W</m> so that <m>B_u\subseteq B</m>. Set <m>B_V=B\setminus B_U</m> and <m>V=\linspan{B_V}</m>. Then <m>V</m> is a subspce of <m>W</m>.
            </p>
    
            <p>
              Any <m>\w\in W</m> can be written as <m>\w=\l_1\b_1+\cdots+\l_n\b_n</m> for some <m>\l_1,\ldots, \l_n\in \F</m> and <m>\b_1,\ldots, \b_n\in B</m>. After renumbering we may assume <m>\b_1,\ldots, b_t\in B_U</m> and <m>\b_{t+1},\ldots, b_n\in B_V</m> so that <m>\w=\u+\v</m> where <m>\u=\l_1\b_1+\cdots+\l_t\b_t\in U</m> and <m>\v=\l_1\b_{t+1}+\cdots+\l_n\b_n\in V</m>. We have shown <m>W=U+V</m>.
            </p>
    
            <p>
              Now suppose <m>\w\in U\cap V</m>. Then <m>\w=\l_1\b_1+\cdots+\l_t\b_t</m> for some <m>\b_1,\ldots, b_t\in B_U</m> and also <m>\w=\l_1\b_{t+1}+\cdots+\l_n\b_n</m> for some <m>\b_{t+1},\ldots, b_n\in B_V</m>. Therefore we have <me>\l_1\b_1+\cdots+\l_t\b_t-\l_{t+1}\b_{t+1}-\cdots -\l_n\b_n=\0</me> with <m>\b_1,\ldots, \b_n\in B</m> distinct. Since <m>B</m> is linearly independent we conclude <m>\l_i=0</m> for all <m>1\leq i\leq n</m> whence <m>\w=\0</m>. We have shown that <m>U\cap V=\{\0\}</m> and it follows from that <m>W=U\oplus V</m>.
            </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="remark"-->
            <p>
              The subspace <m>V</m> in the above theorem is not unique. For example set <m>U=\{(x,0,0)\mid x\in \R\}\subseteq W=\R^3</m>. Then both <m>V=\{(0,y,z)\mid y,z\in \R\}</m> and <m>V'=\{(x,x,z)\mid x,z\in \R\}</m> satisfy <m>U\oplus V=U\oplus V'=\R^3</m>.
            </p><!--</div attr= class="remark">-->
    
          </subsection>
    
          <subsection xml:id="external-direct-sum-and-direct-product">
            <title>External direct sum and direct product</title>
    
            <p>
              Suppose that <m>(V_i)_{i\in I}</m> is a family of vector spaces over a field <m>\F</m>. We would like to exhibit a vector space <m>W</m> that contains (copies of) each of the <m>V_i</m> without assuming that they are subspaces of the same vector field to begin with. Since <m>W</m>contains the <m>V_i</m> it must of course also contain linear combinations of vector from the <m>V_i</m>. We will arrange that these linear combinations cannot <q>accidentally</q> produce any sort of cancellation. In particular for instance we want it to be the case that if <m>\v\in V_i</m> and <m>\w\in V_j</m> then the only way to have <m>\v+\w=\0</m> is for each of <m>\v</m> and <m>\w</m> to be <m>\0</m>.
            </p>
    
    <!-- div attr= class="definition"-->
            <p>
              We define <me>\Dsum_{i\in I} V_i = \setof{(\v_i)_{i\in I}}{\text{$\forall i\in I,\v_i\in V_i$ and only finitely many of the $\v_i$ are non-zero}}.</me> This is the <em>external direct sum</em> of the <m>V_i</m>. The operations are performed coordinate-wise: <me>(\v_i)_{i\in I}+(\w_i)_{i\in I} = (\v_i+\w_i)_{i\in I},</me> and similarly for scalar multiplication by <m>\l\in \F</m> <me>\l(\v_i)_{i\in I}= (\l \v_i)_{i\in I},</me> It is easy to check that with these operations <m>\Dsum_{i\in I} V_i</m> is a vector space.
            </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="definition"-->
            <p>
               It is sometimes desirable to eliminate the restriction that only finitely many of the <m>\v_i</m> are non-zero. Then we get the direct product. <me>\prod_{i\in I} V_i = \setof{(\v_i)_{i\in I}}{\forall i\in I,\v_i\in V_i}.</me>
            </p><!--</div attr= class="definition">-->
    
            <p>
              In both <m>\Dsum_{i\in I} V_i</m> and <m>\prod_{i\in I} V_i</m> there is a copy of <m>V_i</m>. We can identify <m>V_i</m> with the set of vectors that are <m>\0</m> everywhere except in the <m>i^{\text{th}}</m> component. The direct sum is spanned by the union of these copies of the <m>V_i</m>, but (at least when <m>I</m> is infinite) the direct product is not.
            </p>
    
    <!-- div attr= class="lemma"-->
            <p>
              In the cases where our two definitions of direct sum both apply (i.e., when considering <m>U+W</m> where <m>U,W</m> are subspaces of <m>V</m>) the two definitions agree. In particular with the external direct sum on the left and the internal direct sum on the right the following map is an isomorphism: <me>\begin{aligned}
            S \colon U\dsum W &amp;\to U\dsum W \\
            (\u,\w) &amp;\mapsto \u+\w
        
    \end{aligned}</me>
            </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
            <p>
              The fact that <m>\range(S)</m> is all of <m>U+W</m> is clear. To prove that <m>\ker(S)=\set{(\0,\0)}</m> consider a pair <m>(\u,\w)\in U \dsum W</m> such that <m>\u+\w=\0</m>. We need to show that <m>\u=\w=\0</m>. Well, since <m>\u=-\w</m> we have <m>\u\in U\cap W=\set{\0}</m> so <m>\u=0</m> and hence <m>\w=\0</m>.
            </p><!--</div attr= class="proof">-->
    
          </subsection>

        </section>

        <section xml:id="sec-quotients">
          <title>Quotients of Vector Spaces</title>
          
         <!-- div attr= class="definition"-->
			<p>
				Let <m>V</m> be a vectors space. We say that <m>A</m> is an affine subspace of <m>V</m> if there is a vector <m>\v\in V</m> and a linear subspace <m>W</m> with <m>A = \v+W</m>. In this situation we say that <m>A</m> is <em>parallel</em> to <m>W</m>.
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="example"-->
			<p>
				Let <m>X</m> be the set of solutions to a linear system of equations <m>A\x=\b</m>. Then <m>X</m> is an affine space parallel to <m>W</m>, the set of solutions to the homogeneous linear system of equations <m>A\x=\0</m>.
			</p><!--</div attr= class="example">-->

<!-- div attr= class="remark"-->
			<p>
				The vector space <m>W</m> is uniquely determined by <m>A</m> by the formula <m>W=A-A</m>. The vector <m>\v</m> is not uniquely determined by <m>A</m>.
			</p><!--</div attr= class="remark">-->

<!-- div attr= class="definition"-->
			<p>
				If <m>W</m> is a subspace of <m>V</m> the it is possible to make the collection of all affine subspaces parallel to <m>W</m> into a vector space called the <em>quotient vector space</em> <m>V/W</m>. We let: <me>\begin{aligned}
        V / W &amp;\defeq \setof{A}{\text{$A$ is an affine subspace of $V$ parallel to $W$}} \\
        A + B &amp;\defeq \setof{\vv{a}+\b}{\vv{a}\in A, \b\in B}, &amp;&amp; \text{for $A,B \in V/W$} \\
        \l A  &amp;\defeq \begin{cases}
            \setof{ \l \vv{a}}{\vv{a}\in A} &amp; \text{$\l \not= 0$} \\
            W &amp; \l=0,
        \end{cases} &amp;&amp; \text{for $A\in V/W$, $\l\in\F$.}
    
\end{aligned}</me>
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="theorem"-->
			<p>
				With the above definitions <m>V/W</m> is a vector space.
			</p><!--</div attr= class="theorem">-->

<!-- div attr= class="proof"-->
			<p>
				We first need to prove that <m>+, \cdot</m> are maps to <m>V/W</m>, or in other words that <m>A+B</m> and <m>\l B</m> are affine subspaces parallel to <m>W</m>. To prove affine-ness set <m>A=\v+W</m> and <m>B=\v'+W</m> and note that <me>A+B=(\v+W)+(\v'+W)=(\v+\v')+(W+W)=(\v+\v')+W.</me> and if <m>\l\neq 0</m> <me>\l A=\l(\v+W)=\l\v+\l W=\l\v +W.</me> and also if <m>\l=0</m> <me>0 A= W=\0+W=0\v +W.</me> To show that <m>V/W</m> is a vector space we would now have to check the axioms of a vector space. I will not do this explicitly.
			</p><!--</div attr= class="proof">-->

<!-- div attr= class="remark"-->
			<p>
				It follows from above that we can define the operations in <m>V/W</m> alternatively by <me>(\u+W) + (\v+W) \defeq (\u+\v)+W \qquad \l(\u + W) \defeq \l\u + W.</me> The cost of this is that since many choices of <m>\u</m> define that same affine subspace <m>\u+W</m> one needs to check, for instance, that if <m>\u+W = \u'+W</m> and <m>\v+W = \v'+W</m> then <m>(\u+\v)+W = (\u'+\v')+W</m>. We are spared this task. It is easy however to see that our definition agrees with theirs since, using our definition of <m>+</m>, <me>(\u+W) + (\v+W) = \u+W+\v+W = \u+\v+W+W = (\u+\v)+W.</me> A similar argument proves that the two different definitions of <m>\l A</m> agree.
			</p><!--</div attr= class="remark">-->

<!-- div attr= class="example"-->
			<p>
				 If we let <m>V = \R[X]</m> and <m>W=\setof{p . (x^2+1)}{p \in \R[X]}</m> then the quotient <m>V/W</m> is the vector space of real polynomials, except that we have erased the difference between <m>X^2+1</m> and <m>0</m>, or to put it another way, between <m>X^2</m> and <m>-1</m>. Thus, for instance, <me>X^2 + W = X^2 -1(X^2+1) + W = 1+W ,</me> so <m>X^2=1</m> in this space. We have just constructed the complex numbers!
			</p><!--</div attr= class="example">-->

			<p>
				As in the previous example, one way of thinking about the quotient vector space <m>V/W</m> is that we are erasing all distinctions between vectors that differ by an element of <m>W</m>. In particular if <m>T:V\to U</m> is a linear map with kernel containing <m>W</m> (so <m>T(\w)=\0</m> for all <m>\w\in W</m>) then such a distinction has no effect on the value of <m>T</m> and we can treat <m>T</m> as being a linear map on <m>T/W</m>. The details are contained in the following lemma.
			</p>

<!-- div attr= class="theorem"-->
			<p>
				 The following conditions are equivalent for a subspace <m>W\of V</m> and a linear map <m>T:V\to U</m>.
			</p>

<!-- div attr= class="compactenum"-->
			<p>
				<m>T(\w)=\0</m> for all <m>\w\in W</m> (i.e., <m>W\subseteq \ker(T)</m>)
			</p>

			<p>
				<m>T</m> is constant on affine subspaces of <m>V</m> parallel to <m>W</m>
			</p>

			<p>
				There exists a linear map <m>\overline{T}:V/W\to U</m> satisfying <m>\overline{T}(\v+W) = T(\v)</m> for all <m>\v\in V</m>.
			</p><!--</div attr= class="compactenum">--><!--</div attr= class="theorem">-->

			<p>
				Let's use this theorem to prove that the quotient vector space in is indeed <m>\C</m>.
			</p>

<!-- div attr= class="example"-->
			<p>
				Let <m>W=\setof{p . (x^2+1)}{p \in \R[X]}</m>. There is a vector space isomorphism <me>\R[X]/W \cong \C</me> We start with the map <m>T:   \R[X]\to \C, T(p(x))=p(i)</m>, that is given by evaluating a polynomial at the complex number <m>i</m>. Now if <m>p(X) (x^2+1)\in W</m> then we have <m>T(p(x)(x^2+1))=p(i) (i^2+1)=0</m> since <m>i^2=-1</m>. Thus we have shown that <m>W\subseteq \ker(T)</m>. By the previous theorem there is a linear map <me>\overline{T}: \R[X]/W\to \C, \qquad\overline{T}( p(x)+W)= p(i).</me> I claim this map is a bijection. To see it is surjective, if <m>a+bi\in \C</m> then we see that <m>\overline{T}( a+bx+W)=a+bi</m>. To see that <m>\overline{T}</m> is injective note that if <m>p(x)+W\in \ker(\overline{T})</m> then <m>p(i)=0</m> so <m>p(-i)=0</m> since the roots of polynomials with real coefficients come in conjugate pairs. Thus <m>x^2+1=(x-i)(x+i)\mid p</m>, that is <m>p\in W</m>. Therefore <m>p+W=W=\0_{V/W}</m>. We see that <m>\overline{T}</m> is also injective.
			</p><!--</div attr= class="example">-->

<!-- div attr= class="proof"-->
			<p>
				<em>Proof of .</em> We'll prove a) implies b) implies c) implies a). Firstly, let's assume a) and consider an affine subspace <m>A=\v+W</m>. Then for all <m>\x=\v+\w\in A</m> we have <m>T(\x) = T(\v)+T(\w)=T(\v)</m>, so <m>T</m> is constant on <m>A</m>. To show that b) implies c) consider a linear map <m>T</m> that is constant on affine subspaces parallel to <m>W</m> and define <m>\overline{T}:V/W\to U</m> by letting <m>\overline{T}(A)</m> be the common value of <m>T(\v)</m> for all <m>\v\in A</m>. Linearity of <m>\overline{T}</m> is clear since for <m>\l,\mu\in \F</m> and <m>A,B\in V/W</m> we can pick <m>\u\in A</m> and <m>\v\in B</m> and compute the common value of <m>T</m> on <m>\l A+\mu B</m> as follows: <me>\overline{T}(\l A + \mu B) = T(\l \u + \mu \v) = \l T(\u) = \mu T(\v) = \l \overline{T}(A) + \mu \overline{T}(B).</me> Finally, to prove that c) implies a) note that <m>W</m> is the zero vector in <m>V/W</m> so if such a linear map <m>\overline{T}</m> exists then for <m>\w\in W</m> we have <me>T(\w) = \overline{T}(\w+W) = \overline{T}(W) = \0.</me>
			</p><!--</div attr= class="proof">-->

			<p>
				We can give an algebraic version of the Rank-Nullity Theorem that says more than the equality of two numbers; it gives the isomorphism of two vector spaces.
			</p>

<!-- div attr= class="theorem"-->
			<p>
				 For any linear map <m>T:V\to W</m> we have <me>V/\ker(T) \iso \range(T).</me>
			</p><!--</div attr= class="theorem">-->

<!-- div attr= class="proof"-->
			<p>
				The isomorphism is the map <m>\overline{T}:V/\ker(T)\to \range(T)</m> whose existence is guaranteed by <xref ref="thm:quotient_linear_map" />. It is surjective since if <m>\w = T(\v) \in \range(T)</m> then <m>\overline{T}(\v+\ker(T)) = T(\v)=\w</m>, and injective since if <m>\overline{T}(\v+\ker(T))=T(\v)=\0</m> then <m>\v\in \ker(T)</m> so <m>\v+\ker(T) = \ker(T)</m> is the zero vector.
			</p><!--</div attr= class="proof">-->

<!-- div attr= class="example"-->
			<p>
				Consider the linear map <m>T:\R^2\to \R</m> given by <m>T(x,y)=x</m>. Then <m>\ker(T)=\{(0,y)\mid y\in \R\}</m> is the <m>y</m>-axis. The set <m>V/\ker(T)</m> is the set of all lineas in the plane parallel to the <m>y</m>-axis (vertical lines). The range of <m>T</m> can be identified with the <m>x</m>-axis. There is a bijection between vertical lines and points on the <m>x</m>-axis given by sending a line to its <m>x</m> intercept. This map is the isomorphism claimed by .
			</p><!--</div attr= class="example">-->

			<p>
				The following is a closer statement to Rank-Nullity which we can make for any quotient vector space.
			</p>

<!-- div attr= class="theorem"-->
			<p>
				If <m>V</m> is finite dimensional and <m>W</m> is a subspace of <m>V</m> then <me>\dim( V/W) = \dim(V) - \dim(W).</me>
			</p><!--</div attr= class="theorem">-->

<!-- div attr= class="proof"-->
			<p>
				It is definitely possible to give a rather long but very direct proof of this, but to illustrate the power of the Rank-Nullity Theorem (<xref ref="thm:ranknullity" />) we'll give a shorter proof. Consider the (linear) map <me>\begin{aligned}
        \pi \colon V &amp;\to V/W \\
                        \v &amp;\mapsto \v + W.
    
\end{aligned}</me> Note that <m>\pi</m> is clearly surjective since for any <m>\v+W\in V/W</m> we have <m>\pi(\v) = \v+W</m>. Similarly the kernel of <m>\pi</m> is the set of <m>\v\in V</m> such that <m>\v+W = W</m>. This happens exactly when <m>\v\in W-W=W</m>, so <m>\ker(\pi)=W</m>. Thus <xref ref="thm:ranknullity" /> gives us <me>\begin{aligned}
        \dim(V) &amp;= \dim(V/W) + \dim(W), \\
        \shortintertext{i.e.,}
        \dim(V/W) &amp;= \dim(V) - \dim(W). \qedhere
    
\end{aligned}</me>
			</p><!--</div attr= class="proof">-->

        </section>

      </chapter>

      <chapter xml:id="ch-bilinear-quadratic">
        <title>Bilinear and Quadratic Functions</title>
        
        <section xml:id="sec-clustering">
          <title>Application: <m>k</m>-means Clustering</title>
          
          <p>
            <fn>Adapted from notes by Sebastien Roch <url href="https://people.math.wisc.edu/~roch/mmids/notes.html">https://people.math.wisc.edu/<m>\sim</m>roch/mmids/notes.html</url></fn> 
            Imagine that you are an evolutionary biologist studying irises and that you have collected measurements on a large number of iris samples. Your goal is to identify different <url href="https://en.wikipedia.org/wiki/Species">species</url> within this collection. (<url href="https://www.w3resource.com/machine-learning/scikit-learn/iris/index.php">Source</url>)
          </p>
    
          <p>
            Here is a <url href="https://en.wikipedia.org/wiki/Iris_flower_data_set">classical iris dataset</url> first analyzed by the statistician <url href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald A. Fisher</url>. We will upload the data in the form of a data table (similar to a spreadsheet) , where the columns are different measurements (or features) and the rows are different samples. Below, we load the data using <url href="https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv#"><c>pandas.read_csv</c></url> and show the first <m>5</m> lines of the dataset (see <url href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"><c>DataFrame.head</c></url>).
          </p>
    
    <!-- div attr= class="tcolorbox"-->
          <pre>\PY{c+c1}{\PYZsh{} Python 3}
    \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
    \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
    \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
    \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}</pre><!--</div attr= class="tcolorbox">-->
    
    <!-- div attr= class="tcolorbox"-->
          <pre>\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iris\PYZhy{}measurements.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}</pre><!--</div attr= class="tcolorbox">-->
    
    <!-- div attr= class="tcolorbox"-->
          <pre>Id  PetalLengthCm  PetalWidthCm  SepalLengthCm  SepalWidthCm
    0   1            1.4           0.2            5.1           3.5
    1   2            1.4           0.2            4.9           3.0
    2   3            1.3           0.2            4.7           3.2
    3   4            1.5           0.2            4.6           3.1
    4   5            1.4           0.2            5.0           3.6</pre><!--</div attr= class="tcolorbox">-->
    
          <p>
            There are <m>150</m> samples (as can be seen by using <url href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html"><c>DataFrame.shape</c></url> which gives the dimensions of the DataFrame as a tuple). Each row in this data set can be seen as a vector (or point) in <m>\R^4</m> since it has four entries which are real numbers.
          </p>
    
    <!-- div attr= class="tcolorbox"-->
          <pre>\PY{n}{df}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}</pre><!--</div attr= class="tcolorbox">-->
    
    <!-- div attr= class="tcolorbox"-->
          <pre>150</pre><!--</div attr= class="tcolorbox">-->
    
          <p>
            Let's first extract the columns into a Numpy array using <url href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html"><c>DataFrame.to_numpy()</c></url>, and visualize the petal data. Below, each point is a sample. This is called a <url href="https://en.wikipedia.org/wiki/Scatter_plot">scatter plot</url>.
          </p>
    
    <!-- div attr= class="tcolorbox"-->
          <pre>\PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PetalLengthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PetalWidthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
            \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SepalLengthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SepalWidthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}numpy}\PY{p}{(}\PY{p}{)}</pre><!--</div attr= class="tcolorbox">-->
    
    <!-- div attr= class="tcolorbox"-->
          <pre>\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PetalLengthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PetalWidthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}</pre><!--</div attr= class="tcolorbox">-->
    
    <!-- div attr= class="center"-->
          <figure>
      <image source="scater1.png"/>
            <caption>image</caption>
    </figure><!--</div attr= class="center">-->
    
          <p>
            We observe a clear cluster of samples on the bottom left. What is a <url href="https://en.wikipedia.org/wiki/Cluster_analysis">cluster</url>? Intuitively, it is a group of samples that are close to each other, but far from every other sample. In this case, it may be an indication that these samples come from a separate species.
          </p>
    
          <p>
            Now let's look at the full dataset. Visualizing the full <m>4</m>-dimensional data is not straightforward. One way to do this is to consider all pairwise scatter plots.
          </p>
    
    <!-- div attr= class="tcolorbox"-->
          <pre>\PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n+nb}{vars}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PetalLengthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PetalWidthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SepalLengthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SepalWidthCm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{height}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}</pre><!--</div attr= class="tcolorbox">-->
    
    <!-- div attr= class="center"-->
          <figure>
      <image source="scater2.png"/>
            <caption>image</caption>
    </figure><!--</div attr= class="center">-->
    
          <p>
            <!-- linebreak -->
          </p>
    
          <p>
            What would be useful is a method that <em>automatically</em> identifies clusters <em>whatever the dimension of the data</em>. In this chapter, we will discuss a standard way to do this: <m>k</m>-means clustering.
          </p>
    
          <p>
            Clustering is the following fundamental problem in data science: we are given <m>n</m> vectors <m>\mathbf{x}_1,\ldots,\mathbf{x}_n</m> in <m>\mathbb{R}^d</m>. and we want to partition these data points into <m>k</m> disjoint subsets - or clusters - with small pairwise distances within clusters and large pairwise distances across clusters.
          </p>
    
    
    
          <p>
            Fix a number of clusters <m>k</m>. Formally, we define a clustering as a partition.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            A partition of <m>[n] = \{1,\ldots,n\}</m> of size <m>k</m> is a collection of non-empty subsets <m>C_1,\ldots,C_k \subseteq [n]</m> that:
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            are pairwise disjoint, i.e., <m>C_i \cap C_j = \emptyset</m>, <m>\forall i \neq j</m> and
          </p>
    
          <p>
            cover all of <m>[n]</m>, i.e., <m>\cup_{i=1}^k C_i = [n]</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="definition">-->
    
    <!-- div attr= class="remark"-->
          <p>
            We number the clusters <m>C_1,\ldots,C_k</m> for notational convenience, but their order is meaningless. Two partitions are the same if they are the same family of subsets.
          </p><!--</div attr= class="remark">-->
    
          <p>
            For each cluster we pick a representative or center <m>\boldsymbol{\mu}_i \in \mathbb{R}^d</m> of the cluster <m>C_i</m>. Note that <m>\boldsymbol{\mu}_i</m> need not be one of the <m>\mathbf{x}_j</m>'s. Intuitively, clusters must posses the property that each of the points <m>\x_j</m> in the cluster <m>C_i</m> must be close in Euclidean distance to the center of the cluster, <m>\boldsymbol{\mu}_i</m>.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            Let <m>\v=(v_1, \ldots, v_d)</m> be a vector in <m>\R^d</m>. Then the <em>Euclidean norm</em> of <m>\v</m> is <me>\|\v\|=\sqrt{v_1^2+v_2^2+\cdots +c_d^2}</me> The <em>Euclidean distance</em> between points <m>\x</m> and <m>\boldsymbol{\mu}</m> in <m>\R^d</m> is <me>\|\mathbf{x} - \boldsymbol{\mu}\|=\sqrt{(x_1-\mu_1)^2+(x_2-\mu_2)^2+\cdots +(x_d-\mu_d)^2}.</me>
          </p><!--</div attr= class="definition">-->
    
          <p>
            The aggregate of the distances of the points <m>\x_j</m> in our data sets to the centers of the clusters is defined below.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            Fix vectors <m>\x_1, \ldots, \x_n\in \R^d</m>. For a partition <m>C_1,\ldots,C_k</m> of <m>[n]</m> and cluster representatives <m>\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k\in \R^d</m> the <em><m>k</m>-means objective function</em> is <me>\label{eq:kmeans}
    G(C_1,\ldots,C_k, \boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k) =\sum_{i=1}^k \sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i\|^2.</me>
          </p><!--</div attr= class="definition">-->
    
          <p>
            Our goal is to find a partition <m>C_1,\ldots,C_k</m> and also the cluster representetives <m>\boldsymbol{\mu}_i</m> that minimize <xref ref="eq:kmeans" />. The <m>k</m>-means algorithm is a popular <em>heuristic</em> to solve this problem. It is based on the idea that the following two sub-problems are easy to solve:
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            finding the optimal cluster representatives for a fixed partition;
          </p>
    
          <p>
            finding the optimal partition for a fixed set of cluster representatives.
          </p><!--</div attr= class="compactenum">-->
    
          <p>
            The <m>k</m>-means algorithm then alternates between the two steps until progress falls below a given tolerance. This is reasonable since our goal as stated above is to solve the minimization problem <me>\label{eq:minkmeans}
    \min_{C_1,\ldots,C_k} \min_{\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_k \in \mathbb{R}^d} 
    \sum_{i=1}^k \sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i\|^2</me> where <m>C_1,\ldots,C_k</m> ranges over all partitions of <m>[n]</m> of size <m>k</m>. Fixing partition <m>C_1,\ldots,C_k</m> and miniminizing over <m>\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_k \in \mathbb{R}^d</m> corresponds to solving the first problem above, while fixing <m>\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_k \in \mathbb{R}^d</m> and miniminizing over partitions <m>C_1,\ldots,C_k</m> corresponds to solving the second problem.
          </p>
    
          <p>
            To elaborate on the first step above, we review an elementary fact about quadratic functions.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
             Let <m>q(x) = a x^2 + b x + c</m> where <m>a &gt; 0</m> and <m>x \in \mathbb{R}</m>. The unique global minimum of <m>q</m> is attained at <m>x^* = -\frac{b}{2a}.</m>
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We rewrite <m>q</m> as <me>\begin{aligned}
    q(x) 
    &amp;= &amp;a \left(x^2 + 2 \frac{b}{2a} x\right) + c\\
    &amp;=&amp; a \left(x^2 + 2\frac{b}{2a} x + \left(\frac{b}{2a}\right)2\right) - a \left(\frac{b}{2a}\right)^2 + c\\
    &amp;= &amp;a (x - x^*)^2 + c - \frac{b^2}{4a}.
    \end{aligned}</me>
          </p>
    
          <p>
            Since <m>(x - x^*)^2\geq 0</m> we see that this quantity is minimized when <m>x=x^*</m> and any other <m>x</m> gives a higher value for <m>q</m>. The step on the second line above is called <url href="https://en.wikipedia.org/wiki/Completing_the_square"><em>Completing the Square</em></url>. <m>\square</m>
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="lemma"-->
          <p>
            Fix a partition <m>C_1,\ldots,C_k</m>. The representatives <m>\boldsymbol{\mu}_i</m> which minimize the objective function
          </p>
    
          <p>
            <me>G(C_1,\ldots,C_k; \boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k)
    = \sum_{i=1}^k \sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i\|^2,</me>
          </p>
    
          <p>
            are the centroids <me>\boldsymbol{\mu}_i^* = \frac{1}{|C_i|} \sum_{j\in C_i} \mathbf{x}_j.</me>
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Using the notation <m>\mathbf{x}_j = (x_{j1},\ldots,x_{jd})^T</m> and similarly for <m>\boldsymbol{\mu}_i</m>, note that we can expand the <m>k</m>-means objective as
          </p>
    
          <p>
            <me>\begin{aligned}
    \sum_{i=1}^k \sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i\|^2
    &amp;= \sum_{i=1}^k \sum_{j \in C_i} \sum_{m=1}^d (x_{jm} - \mu_{im})^2\\
    &amp;= \sum_{i=1}^k \sum_{m=1}^d \left[\sum_{j \in C_i} (x_{jm} - \mu_{im})^2\right].
    \end{aligned}</me>
          </p>
    
          <p>
            The expression in square brackets is a quadratic function in <m>\mu_{im}</m>
          </p>
    
          <p>
            <me>q_{im}(\mu_{im})
    = \left\{\sum_{j \in C_i} x_{jm}^2\right\} + \left\{- 2 \sum_{j \in C_i} x_{jm}\right\} \mu_{im}  + \left\{|C_i| \right\} \mu_{im}^2,</me>
          </p>
    
          <p>
            and therefore, by <xref ref="lem:minquadratic" /> is minimized at
          </p>
    
          <p>
            <me>\mu_{im}^* 
    = - \frac{- 2 \sum_{j \in C_i} x_{jm}}{2 |C_i|}
    = \frac{1}{|C_i|} \sum_{j \in C_i} x_{jm}.</me>
          </p>
    
          <p>
            Since each term <m>q_{im}(\mu_{im})</m> in the sum over <m>i, m</m> making up the objective function <m>G</m> is strictly minimized at <m>\boldsymbol{\mu}_1^*,\ldots, \boldsymbol{\mu}_k^*</m>, so is <m>G</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="lemma"-->
          <p>
            Fix the representatives <m>\boldsymbol{\mu}_1,\ldots,\boldsymbol{\mu}_k</m>. The partition that minimizes the objective function <me>G(C_1,\ldots,C_k; \boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k)
    = \sum_{i=1}^k \sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i\|^2,</me> is obtained as follows. For each <m>j</m>, find the <m>\boldsymbol{\mu}_i</m> that minimizes <m>\|\mathbf{x}_j - \boldsymbol{\mu}_i\|</m> (picking one arbitrarily in the case of ties) and assign <m>\mathbf{x}_j</m> to <m>C_i</m>.
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            If <m>c</m> is the cluster assignment associated to <m>C_1,\ldots,C_k</m>, then we can re-write the objective as <me>\sum_{i=1}^k \sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i\|^2
    = \sum_{j=1}^n \|\mathbf{x}_j - \boldsymbol{\mu}_{c(j)}\|^2,</me> By definition, when the <m>\boldsymbol{\mu}_i</m>'s are fixed, each term in the sum on the right-hand side is minimized separately by the assignment in the statement. Hence so is the sum itself. Note that we used the fact that the square root is non-decreasing to conclude that minimizing <m>\|\mathbf{x}_j - \boldsymbol{\mu}_i\|^2</m> or its square root <m>\|\mathbf{x}_j - \boldsymbol{\mu}_i\|</m> are equivalent.
          </p><!--</div attr= class="proof">-->
    
          <p>
            We now see that the <m>k</m>-means algorithm approaches a (local) solution to the problem <xref ref="eq:minkmeans" />.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
            The sequence of objective function values produced by the <m>k</m>-means algorithm is weakly decreasing and hence converges to a (local) minimum of the function <xref ref="eq:kmeans" />.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            By the Optimal Representatives and Optimal Clustering lemmas, each step does not increase the objective. A sequence of real numbers that is weakly decreasing and bounded below (in our case by 0, as the distances are all non-negative) converges.
          </p><!--</div attr= class="proof">-->
    
          <p>
            The <m>k</m>-means algorithm is only a heuristic. In particular, it is not guaranteed to find the global minimum of the <m>k</m>-means objective. However, it is guaranteed to improve the objective at every iteration, or more precisely, not to make it worse.
          </p>

        </section>

        <section xml:id="sec-bilinear-functions">
          <title>Bilinear Functions</title>
          
          <!-- div attr= class="example"-->
			<p>
				The multiplication function <m>m:\R^2\to \R, m((x,y))=xy</m> is not linear since, for instance, <m>m((2,3)+(1,5)) = 24 \not= m(2,3)+m(1,5)</m>. However the definition of a field requires that <m>m</m> be linear in each argument: <me>\begin{aligned}
    m(\lambda x+\mu x',y)&amp;=\lambda \,m(x,y)+\mu \, m(x',y),\\
    m(x,\lambda y+\mu y')&amp;=\lambda \,m(x,y)+\mu \, m(x,y').
\end{aligned}</me>
			</p><!--</div attr= class="example">-->

<!-- div attr= class="definition"-->
			<p>
				If <m>U,V</m> are vector spaces over a field <m>\F</m> then we denote by <m>U\times V</m> their direct product (which is the same as their external direct sum) <me>U\times V=\{(u,v)\mid u\in U, v\in V\}.</me> This is a vector space with componentwise addition and saclar multiplication. See Definition <xref ref="def:directproduct" />.
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="definition"-->
			<p>
				If <m>U,V,W</m> are vector spaces over a field <m>\F</m> then <m>B:U\times V\to W</m> is <em>bilinear</em> if it is linear in each argument. That is to say for all <m>\u,\u'\in U</m>, <m>\v,\v'\in V</m> and <m>\l,\mu\in \F</m> we have <me>\begin{aligned}
        B(\lambda \u+\mu \u',\v) &amp;= \lambda \, B(\u,\v)+\mu \, B(\u',\v) \\
        B(\u,\lambda \v+\mu \v') &amp;= \lambda \, B(\u,\v)+\mu \, B(\u,\v') 
    
\end{aligned}</me>
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="example"-->
<!-- div attr= class="compactenum"-->
			<p>
				The dot product <me>\begin{aligned}
            \cdot \colon \R^n\times \R^n &amp;\to \R^n \\
            (\u,\v) &amp;\mapsto \sum_1^n u_i v_i
        
\end{aligned}</me> is one of the most important bilinear maps there is; it defines distances and angles in <m>\R^n</m>, thereby introducing geometry into linear algebra.
			</p>

			<p>
				Applying linear maps to vectors (evaluating linear maps) is a bilinear function: <me>\begin{aligned}
            e \colon L(V,W)\times V &amp;\to W \\
            (T,\v) &amp;\mapsto T(\v)
        
\end{aligned}</me> is bilinear since <m>T(\l\u+\mu\v)=\l T(\u)+\mu T(\v)</m> and also <m>(\l T+\mu S)(\v) = \l T(\v)+\mu S(\v)</m>.
			</p>

			<p>
				Exaluating non-linear maps though, for example polynomials is not a bilinear map. Specifically the map <m>(p,x)\mapsto p(x)</m> from <m>\R[X]\times \R</m> to <m>\R</m> is <em>not</em> bilinear. It is linear in <m>p</m>, but not in <m>x</m> since, for example, if <m>p=X^2</m> then <m>4=p(1+1) \not= p(1)+p(1)=2</m>.
			</p><!--</div attr= class="compactenum">--><!--</div attr= class="example">-->

<!-- div attr= class="definition"-->
			<p>
				Given a vector space <m>V</m> over a field <m>\F</m>, a <em>bilinear form</em> on <m>V</m> is a bilinear map <m>B:V\times V\to \F</m>.
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="example"-->
			<p>
				 If <m>A\in M_{n\by n}(\F)</m> then we can define a bilinear form as follows: <me>\begin{aligned}
        B_A \colon \F^n\times \F^n &amp;\to \F \\
        (\u,\v) &amp;\mapsto \u\trans A \v
    
\end{aligned}</me>
			</p><!--</div attr= class="example">-->

			<p>
				Conversely, bilinear forms can always be represented by matrices.
			</p>

<!-- div attr= class="definition"-->
			<p>
				If <m>B:V\times V\to\F</m> is a bilinear form and <m>D=\set{\v_1,\v_2,\dots,\v_n}</m> is an ordered basis for <m>V</m> then the <em>matrix for <m>B</m> with respect to <m>D</m></em> is the <m>n\by n</m> matrix <m>[B]_D</m> with entries <me>([B]_D)_{ij} = B(\v_i,\v_j).</me> We have <me>\begin{aligned}
        B\left(\sum_1^n \lambda_i \v_i,\sum_1^n \mu_j \v_j\right)&amp;=\sum_{i=1}^n \sum_{j=1}^n \lambda_i\mu_j B(\v_i,\v_j)\\
        &amp;=\sum_{i,j=1}^n \lambda_i \,([B]_D)_{ij}\, \mu_j\\
        &amp;=\row{\l_1 &amp; \l_2 &amp; \cdots &amp; \l_n}\, [B]_D \col{\mu_1\\ \mu_2\\ \vdots \\ \mu_n} . 
    
\end{aligned}</me> In other words if we set <m>A=[B]_D</m> then <m>B</m> behaves like the bilinear map <m>B_A</m> in Example <xref ref="def:BA" />.
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="definition"-->
			<p>
				If <m>B:V\times V\to W</m> is bilinear we say that <m>B</m> is <em>symmetric</em> if <m>B(\v,\w)=B(\w,\v)</m> for all <m>\v,\w\in V</m> and <em>anti-symmetric</em> if <m>B(\v,\w)=-B(\w,\v)</m> for all <m>\v,\w\in V</m>.
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="example"-->
			<p>
				Dot product is a symmetric bilinear form since <m>\u\cdot\v=\v\cdot \u</m>.
			</p><!--</div attr= class="example">-->

        </section>

        <section xml:id="sec-quadratic-functions">
          <title>Quadratic Functions</title>
          
          <p>
            We are now in a position to define what we mean by a quadratic function on a vector space. It's what you get from a bilinear function by plugging in the same thing for both arguments. More formally:
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            A <em>quadratic function</em> on a vector space <m>V</m> is a function <m>Q:V\to W</m> such that there exists a bilinear function <m>B:V\times V\to W</m> with <m>Q(\v)=B(\v,\v)</m> for all <m>\v\in V</m>. We say that <m>Q</m> is the quadratic function <em>determined by <m>B</m></em>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="lemma"-->
          <p>
            If <m>\F</m> is a field in which <m>2\not=0</m> then for any quadratic function <m>Q:V\to W</m> there is a unique <em>symmetric</em> bilinear function <m>B:V\times V\to W</m> such that <m>Q</m> is the quadratic function determined by <m>B</m>.
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We show uniqueness first: assume there exists a symmetric bilinear function <m>B:V\times V\to W</m> such that <m>Q(\v)=B(\v,\v)</m>. Then we have, for <m>\v,\w\in V</m>, <me>Q(\v+\w) = B(\v+\w,\v+\w) = B(\v,\v) + 2B(\v,\w) + B(\w,\w) = Q(\v) + 2B(\v,\w) + Q(\w).</me> Hence <me>B(\v,\w) = \frac{Q(\v+\w)-Q(\v)-Q(\w)}2</me> is determined by the values of <m>Q</m>.
          </p>
    
          <p>
            For existence, define <m>B:V\times V\to W</m> to be given by the rule <me>B(\v,\w) = \frac{Q(\v+\w)-Q(\v)-Q(\w)}2.</me> One checks that this rule defines a function that is bilinear and symmetric and that <m>Q(\v)=B(\v,\v)</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="example"-->
          <p>
             Consider the matrix <me>A = \mat[1&amp;2&amp;-6\\2&amp;9&amp;8\\-6&amp;8&amp;-1] \in M_{3\by 3}(\R).</me> As in we may construct a bilinear form <me>B_A:\R^3\times \R^3\to \R, \quad B_A(\u,\v)=\u^TA\v.</me> which determines a quadratic function by the formula <m>Q(\v)=B(\v,\v)=\v^tA\v</m> for all <m>\v\in\R^3</m>. The matrix of <m>B</m> with respect to the standard basis <m>D=\{\e_1,\e_2,\e_3\}</m> is <m>[B]_D=A</m>. Let <m>\v=(x,y,z)</m>. Then <me>Q(x,y,z)=\begin{bmatrix} x &amp; y&amp; z \end{bmatrix}  \mat[1&amp;2&amp;-6\\2&amp;9&amp;8\\-6&amp;8&amp;-1]\begin{bmatrix} x \\ y \\ z \end{bmatrix}=x^2 + 9y^2 - z^2 + 4xy - 12xz + 16yz.</me>
          </p>
    
          <p>
            We can complete the square to get <me>\begin{aligned}
        Q(x,y,z) &amp;=x^2 + 9y^2 - z^2 + 4xy - 12xz + 16yz \\
        &amp;= (x+2y-6z)^2 + 5y^2 +40yz - 37z^2 \\
                &amp;= (x+2y-6z)^2 + 5(y + 4z)^2 - 117z^2.
        
    \end{aligned}</me>
          </p>
    
          <p>
            This procedure of completing the square allows us to express any quadratic function on <m>\R^n</m> as a linear combination of squares of linear expressions. What this amounts to is that if we had taken a different basis <m>\set{\v_1,\v_2,\v_3}</m> for <m>\R^3</m> with the property that <me>(x,y,z) = (x+2y-6z) \v_1 + (y+4z) \v_2 + z \v_3,</me> then that basis<fn>			<p>
            <m>\v_1=(1,0,0), \v_2=(-2,1,0),\v_3=(14,-4,1)</m>
          </p></fn> would have the property that <m>Q(a\v_1+b\v_2+c\v_3) = a^2+5b^2-117c^2</m>. For the purposes of understanding <m>Q</m> this would be a much better basis. By choosing a different basis <m>D'=\set{\v_1,\v_2,\v_3}</m> we changed the matrix of <m>B</m> to <me>[B]_{D'}=   \mat[1&amp;0&amp;0\\0&amp;5&amp;0\\0&amp;0&amp;-117].</me> With respect to the basis <m>D''=\set{\v_1,\v_2/\sqrt5,\v_3/\sqrt{117}}</m> we have the matrix <me>[B]_{D''}=  \mat[1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;-1].</me>
          </p><!--</div attr= class="example">-->
    
          <p>
            We now see that for any symmetric bilinear form one can find a basis for which the matrix representing the form is a diagonal matrix.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>B</m> is a symmetric bilinear form on a finite dimensional vector space <m>V</m> over a field <m>\F</m> in which <m>2\not=0</m> then <m>V</m> has a basis <m>D=\{\v_1,\ldots, \v_n\}</m> with respect to which the matrix <m>[B]_D</m> is diagonal. Moreover there exist scalars <m>\l_1,\ldots, \l_n\in \F</m> so that the quadratic form defined by <m>B</m> satisfies <me>Q(x_1\v_1+\cdots+x_n\v_n)=\l_1x_1^2+\cdots+\l_nx_n^2.</me>
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We prove the result by induction on <m>\dim(V)</m>. If <m>\dim(V)\le 1</m> then the result is trivial since all matrices are diagonal. If <m>\dim(V)&gt;1</m> and <m>B \equiv 0</m> then we can pick any basis and our matrix will be the zero matrix, and in particular diagonal. Suppose then that <m>\dim(V)&gt;1</m> and <m>B</m> is not identically <m>0</m>. Let <m>Q</m> be the associated quadratic form. If <m>Q</m> were identically <m>0</m> then so also <m>B</m> would be, so there exists <m>\v\in V</m> such that <m>Q(\v)=B(\v,\v)=\l_1\neq 0</m>. Define <me>U = \setof{\w\in V}{B(\v,\w)=0}.</me> The bilinearity of <m>B</m> allows us to check that <m>U</m> is a subspace of <m>V</m>. In fact we have <m>V = \linspan(\v) \dsum U</m>. To prove this note first that if <m>\w\in \linspan(\v) \cap U</m> then <m>\w=\l\v</m> for some <m>\l</m> and also <me>B(\v,\w)= B(\v,\l\v) = \l B(\v,\v) = 0,</me> which implies that <m>\l=0</m> (since <m>B(\v,\v)\not=0</m>). Now consider an arbitrary <m>\w\in V</m>. If we set <m>\w'=\w- \left(B(\v,\w)/B(\v,\v)\right)\cdot\v</m> and <m>\l=B(\v,\w)/B(\v,\v)</m> then <me>B(\v,\w') = B(\v,\w) - \left(B(\v,\w)/B(\v,\v) \right)B(\v,\l\v)=0,</me> so <m>\w'\in U</m>. Thus <m>\w= \l\v + \w' \in \linspan(\v)+U</m>. Now we know that <m>\dim(U) &lt; n</m>, so by induction there is a basis <m>D'=\{\v_2,\ldots,\v_n\}</m> of <m>U</m> with respect to which the matrix of <m>B</m> (restricted to <m>U</m>) is diagonal, say <me>[B\mid_U]_{D'}=\begin{bmatrix}
        \l_2 &amp; 0 &amp;\cdots &amp; 0\\
        0 &amp; \l_3 &amp;\cdots &amp; 0\\
    \vdots &amp; \vdots &amp;\cdots &amp; \vdots\\
    0 &amp; 0 &amp;\cdots &amp; \l_n\\
        \end{bmatrix}</me> By we get that <m>D = \set{\v} \cup D'</m> is a basis for <m>V</m> with respect to which the matrix of <m>B</m> is <me>[B\mid_U]_{D'}=\begin{bmatrix}
        \l_1 &amp; 0 &amp; 0 &amp;\cdots &amp; 0\\
        0 &amp; \l_2 &amp; 0 &amp; \cdots &amp; 0\\
        0 &amp; 0 &amp; \l_3 &amp;  \cdots &amp; 0\\
    \vdots &amp; \vdots &amp;\cdots &amp; \vdots\\
    0 &amp; 0 &amp; 0&amp; \cdots &amp; \l_n\\
        \end{bmatrix}</me> and the result is proved.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="remark"-->
          <p>
            It is natural to turn the proof of <xref ref="thm:form_diag" /> into an algorithm that starts with a basis for <m>V</m> and converts it into a basis such that <m>B</m> has a diagonal matrix. One step of the algorithm is the following <me>\set{\vs} \ \longrightarrow\  \set{\v_1,\v_2-\frac{B(\v_1,\v_2)}{B(\v_1,\v_1)}\v_1,
                                                          \v_3-\frac{B(\v_1,\v_3)}{B(\v_1,\v_1)}\v_1, \dots,
                                                          \v_n-\frac{B(\v_1,\v_n)}{B(\v_1,\v_1)}\v_1}.</me> Unfortunately this process only works if you can guarantee that at least one of the <m>\v_i</m> has <m>Q(\v_i)\not=0</m>. Sadly there are examples where <m>Q(\v_1)=Q(\v_2)=\dots=Q(\v_n)=0</m> but <m>B\not\equiv0</m>. For instance if <m>Q((x,y)) = 2xy</m> then <m>Q((1,0))=Q((0,1))=0</m>, but <m>Q</m> is not identically <m>0</m> on <m>\R^2</m>. One situation where this algorithm does work is when <m>Q</m> (or equivalently B) is <em>positive definite</em>, meaning that for all <m>\v\not=0</m> we have <m>Q(\v)&gt;0</m>. In this context this algorithm is known as the Gram-Schmidt algorithm.
          </p><!--</div attr= class="remark">-->
    
    <!-- div attr= class="definition"-->
          <p>
            A bilinear form <m>B</m> on a real vector space <m>V</m> is called <em>positive definite</em> if the associated quadratic function <m>Q</m> satisfies <m>Q(\v)&gt;0</m> for all <m>\v\in V</m> with <m>\v\not=\0</m>.
          </p><!--</div attr= class="definition">-->

        </section>

        <section xml:id="sec-inner-product-ortho">
          <title>Inner Product Spaces and Orthogonality</title>
          
          <p>
            All the vector spaces in this section will be vector spaces over one of the fields <m>\R</m> or <m>\C</m>.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            We will use the <em>complex conjugation</em> operation on <m>\C</m> given by <me>\overline{a+bi}=a-bi \text{ for }a,b\in\R</me> and the absolute value function <me>|a+bi|=\sqrt{a^2+b^2}=\sqrt{(a+bi)(\overline{a+bi})}.</me>
          </p><!--</div attr= class="definition">-->
    
          <p>
            Note that <m>\overline{a}=a</m> for any <m>a\in\R</m>. In fact for <m>z\in \C</m> we have <m>z=\overline{z}</m> if and only if <m>z\in\R</m>.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            An <em>inner product</em> on vector space <m>V</m> is a function <m>\langle - , -\rangle: V\times V\to \F</m> which is
          </p>
    
          <p><ol>
            <li>
                  <p>
            conjugate-symmetric: for all <m>\u,\v\in V</m> we have <m>\langle \u,\v\rangle =\overline{\langle \v,\u \rangle}</m>
          </p>
            </li>
    
            <li>
                  <p>
            linear in the first entry: for all <m>\u,\u', \v\in V</m> and <m>\l,\mu\in \F</m> we have <me>\langle\l \u+\mu \u',\v\rangle =\l \langle \u,\v\rangle+\mu\langle  \u',\v\rangle.</me>
          </p>
            </li>
    
            <li>
                  <p>
            positive definite for all <m>\v\in V</m> such that <m>\v\neq \0</m> we have <m>\langle \v,\v\rangle&gt;0</m>
          </p>
            </li>
    
          </ol></p>
    
          <p>
            The vector space <m>V</m> together with its inner product <m>\ip{,}</m> is called an <em>inner product space</em>. If <m>V</m> is an inner product space over the field of real numbers <m>\R</m> we call <m>V</m> a <em>Euclidean space</em>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="definition"-->
          <p>
            Suppose <m>V</m> is an inner product space and denote the inner product of <m>\v,\w\in V</m> by <m>\ip{\v,\w}</m>. The corresponding quadratic function can be used as a way of measuring lengths of vectors in <m>V</m>. We define the <em>norm</em> of a vector <m>\v\in V</m> to be <me>\norm{\v} = \sqrt{\ip{\v,\v}}.</me> Note that since the inner product is positive definite we have <m>\norm{\v}\geq 0</m> and moreover <m>\norm{\v}&gt;0</m> for all <m>\v\not=\0</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="lemma"-->
          <p>
            For all <m>\u,\v,\v'\in V</m> and <m>\l\in \F</m> we have
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            <m>\langle \v,\0\rangle =\langle \0, \v \rangle=0</m>
          </p>
    
          <p>
            <m>\langle \v,\u+\u'\rangle= \langle \v,\u\rangle+\langle \v,\u'\rangle</m>
          </p>
    
          <p>
            <m>\langle \v,\l \u\rangle=\overline{\l}\langle \v,\u\rangle</m>
          </p>
    
          <p>
            <m>\langle \v,\v\rangle \in\R</m>
          </p>
    
          <p>
            <m>\norm{\l\v}=|\l|\norm{\v}</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Exercise.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="remark"-->
          <p>
            If <m>\F=\R</m> Properties (b) and (c) of an inner product imply that it is a bilinear function. Thus when <m>V</m> is a Euclidean space, the inner product is a symmetric bilinear function.
          </p><!--</div attr= class="remark">-->
    
    <!-- div attr= class="example"-->
          <p>
             If <m>\F=\R</m> or <m>\C</m>, the dot product <m>\langle \v,\w\rangle=v_1\overline{w_1}+v_2\overline{w_2}+\cdots +v_n\overline{w_n}</m> is the standard inner product on <m>\F^n</m>. For this inner product <m>\norm{\v}</m> is just the usual length of <m>\v</m>.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="example"-->
          <p>
             The following is an inner product on <m>\mathcal{C}[0,1]</m>, the real vector space of continuous functions <m>f:[0,1]\to\R</m>: <me>\ip{p,q} = \definite p(x)q(x) dx{0}1.</me> Checking the conditions is straightforward.
          </p>
    
          <p><ul>
            <li>
                  <p>
            <em>Symmetry</em> means <m>\ip{p,q} =\ip{q,p}</m> for all <m>p,q\in \mathcal{C}[-1,1]</m>. (Since <m>\F=\R</m> we don't need to conjugate.)
          </p>
    
          <p>
            This holds since multiplication of real numbers is commutative so that <m>p(x)q(x)= q(x)p(x)</m> for all <m>x\in [0,1]</m> and therefore <m>\definite p(x)q(x) dx{0}1= \definite q(x)p(x) dx{0}1</m>, that is <m>\ip{p,q} =\ip{q,p}</m>.
          </p>
            </li>
    
            <li>
                  <p>
            <em>Linearity</em> means <m>\ip{\l p+\mu p',q}=\l \ip{p,q}  +\mu \ip{p',q}</m> <me>\begin{aligned}
    \ip{\l p+\mu p',q} &amp;= \definite \left( \l p(x)+\mu p'(x)\right)q(x) dx{0}1\\
    &amp;= \definite \l p(x)q(x)+\mu p'(x)q(x) dx{0}1 &amp; \text{ by distributivity in }\R\\
    &amp;= \l \definite p(x)q(x) dx{-1}1 + \mu \definite p'(x)q(x) dx{0}1 &amp; \text{ by linearity of the integral }\\
    &amp;= \l \ip{p,q}  +\mu \ip{p',q}.
    \end{aligned}</me>
          </p>
            </li>
    
            <li>
                  <p>
            <em>Positive definiteness</em> means that <m>\ip{p,p} &gt;0</m> whenever <m>p\neq \0</m>.
          </p>
    
          <p>
            This is a consequence of the fact that <m>p(x)^2\geq 0</m> and so <m>\ip{p,p}=\definite p(x)^2 dx{-1}1 \geq 0</m> for all <m>p\in\mathcal{C}[-1,1]</m>. Moreover, if <m>p\neq \0</m> there exists <m>x\in[0,1]</m> so that <m>p(x)^2&gt;0</m> and in fact by continuity <m>p(x)^2&gt;0</m> on some interval within <m>[0,1]</m>. This gives that <m>\ip{p,p}=\definite p(x)^2 dx{-1}1&gt;0</m> whenever <m>p\neq \0</m>.
          </p>
            </li>
    
          </ul></p><!--</div attr= class="example">-->
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>\u,\v</m> are elements of an inner product space <m>V</m> we say that <m>\u</m> and <m>\v</m> are othogonal (to each other) if <m>\langle \u,\v\rangle =0</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>V</m> is an inner product space then a list of vectors <m>\v_1, \ldots, \v_n\in V</m> is a <em>orthonormal list</em> (ONL) if <me>\langle \v_i, \v_j \rangle =\begin{cases}
     0 &amp; \text{ if } i\neq j\\
    1 &amp; \text{ if } i= j
    \end{cases}</me> An <em>orthonormal basis</em> (ONB) is an orthonormal list which is also a basis for <m>V</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
          <p>
            If <m>\F=\R</m> or <m>\C</m> then the standard basis <m>\e_1,\ldots, \e_n</m> of <m>\F^n</m> is a ONB with respect to the standard inner product.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="theorem"-->
          <p>
             Any finite dimensional inner product space <m>V</m> has an ONB. Moreover a ONB can be obtained using the following algorithm:
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            Start with any basis <m>\{\b_1,\ldots, \b_n\}</m> of <m>V</m>
          </p>
    
          <p>
            Set <m>\v_1=\b_1/\norm{\b_1}</m>.
          </p>
    
          <p>
            For i from 1 to <m>n-1</m> set <me>\begin{aligned}
    \label{GS}
    \hat{\v}_{i+1} &amp;=&amp;\b_{i+1}-\langle \b_{i+1},\v_1\rangle\cdot \v_1-\cdots -\langle \b_{i+1},\v_i\rangle\cdot \v_i\\
    \v_{i+1} &amp;=&amp; \hat{\v}_{i+1} /\norm{\hat{\v}_{i+1} } \nonumber .
    \end{aligned}</me> Then <m>\{\v_1,\ldots, \v_n\}</m> is a ONB for <m>V</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We prove by induction on <m>i\geq 1</m> that <m>\v_1,\ldots, \v_i</m> is a ONL that satisfies <m>\linspan\{\v_1,\ldots, \v_i\}=\linspan\{\b_1,\ldots, \b_i\}</m>.
          </p>
    
          <p>
            <em>Base case:</em> <m>i=1</m> since <m>\v_1=\b_1/\norm{\b_1}</m> we deduce that <m>\linspan\{\v_1\}=\linspan\{\b_1\}</m>. Moreover <me>\langle\v_1, \v_1 \rangle =\langle \b_1/\norm{\b_1}, \b_1/\norm{\b_1} \rangle =\frac{1}{\norm{\b_1}^2} \langle \b_1, \b_1\rangle =1.</me>
          </p>
    
          <p>
            <em>Induction step:</em> assume that <m>\v_1,\ldots, \v_i</m> is a ONL that satisfies <m>\linspan\{\v_1,\ldots, \v_i\}=\linspan\{\b_1,\ldots, \b_i\}</m>. Therefore <m>\v_1,\ldots, \v_i\in \linspan\{\b_1,\ldots, \b_i\}</m>. The definition of <m>\v_{i+1}</m> shows that <m>\v_{i+1}\in \linspan\{\v_1,\ldots, \v_i, \b_{i+1}\}=\linspan\{\b_1,\ldots, \b_i,\b_{i+1}\}</m>. Since <m>\v_1,\ldots, \v_{i+1}\in \linspan\{\b_1,\ldots, \b_i, \b_{i+1}\}</m> we conclude that <m>\linspan\{\v_1,\ldots, \v_{i+1}\} \subseteq \linspan\{\b_1,\ldots, \b_{i+1}\}</m>.
          </p>
    
          <p>
            Conversely the inductive hypothesis gives <m>\b_1,\ldots, \b_i\in \linspan\{\v_1,\ldots, \v_i\}</m>. Moreover solving for <m>\b_{i+1}</m> from the equation defining <m>\hat{v}_{i+1}</m> shows that <m>\b_{i+1}\in  \linspan\{\v_1,\ldots, \v_i, \v_{i+1}\}</m>. Since <m>\b_1,\ldots, \b_{i+1}\in \linspan\{\v_1,\ldots, \v_i, \v_{i+1}\}</m> we conclude that <m>\linspan\{\b_1,\ldots, \b_{i+1}\} \subseteq \linspan\{\v_1,\ldots, \v_{i+1}\}</m>.
          </p>
    
          <p>
            We may now conclude that <m>\linspan\{\v_1,\ldots, \v_{i+1}\} = \linspan\{\b_1,\ldots, \b_{i+1}\}</m>.
          </p>
    
          <p>
            To show <m>\v_1,\ldots, \v_{i+1}</m> is a ONL we compute by the inductive hypothesis that <me>\langle \v_\ell,\v_j\rangle=\begin{cases}
     0 &amp; \text{if } \ell\neq j\\
     1 &amp; \text{if } \ell = j.
     \end{cases}</me> for <m>1\leq \ell,j  \leq i</m>. Now for <m>1\leq j\leq i</m> we have <me>\begin{aligned}
     \langle \hat{\v}_{i+1}, \v_j\rangle &amp;=&amp;  \langle \b_{i+1}, \v_j\rangle-\langle \b_{i+1},\v_1\rangle \langle\v_1, \v_j\rangle-\cdots -\langle \b_{i+1},\v_i\rangle \langle\v_i, \v_j\rangle \\
     &amp;=&amp;  \langle \b_{i+1}, \v_j\rangle -  \langle \b_{i+1}, \v_j\rangle \langle \v_j,\v_j\rangle = \langle \b_{i+1}, \v_j\rangle- \langle \b_{i+1}, \v_j\rangle\cdot 1\\
     &amp;=&amp;0.
    \end{aligned}</me> Since <m>\hat{\v}_{i+1}=\norm{\hat{\v}_{i+1}}\v_{i+1}</m> we have <me>0=\langle \hat{\v}_{i+1}, \v_j\rangle=\langle \norm{\hat{\v}_{i+1}}\v_{i+1}, \v_j\rangle = \norm{\hat{\v}_{i+1}}\langle \v_{i+1},\v_j\rangle.</me> In view of <m>\norm{\hat{\v}_{i+1}} \neq 0</m> we conclude <m>\langle \v_{i+1},\v_j\rangle=0</m>. Finally, <m>\langle \v_{i+1},\v_{i+1}\rangle=1</m> by the same argument as in the base case.
          </p>
    
          <p>
            The case <m>i=n</m> shows that <m>\{\v_1,\ldots, \v_n\}</m> is a spanning set for <m>V</m>. Since <m>\{\b_1,\ldots, \b_n\}</m> is a basis of <m>V</m> we know that <m>\dim V=n</m>. By this implies that <m>\{\v_1,\ldots, \v_n\}</m> is a basis for <m>V</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="example"-->
          <p>
            Find a ONB for the Euclidean vector space <m>V_2=\{a+bX+cX^2\mid a,b,c \in \R\}</m> of polynomials of degree at most two with inner product <m>\langle p,q\rangle = \definite p(x)q(x) dx{0}1.</m>
          </p>
    
          <p>
            We start with the basis <m>\b_1=1, \b_2=X, \b_3=X^2</m>. Then we set <me>\v_1=\frac{\b_1}{\norm{\b_1}}=1.</me> Further <me>\hat{\v}_2=\b_2-\langle \b_2, \v_1\rangle \cdot \v_1= X-  \definite X dx{0}1 \cdot 1=X-\frac{1}{2},</me> <me>\norm{\hat{\v}_2}= \definite( X-\frac{1}{2})^2dx{0}1=\frac{1}{3} (X-\frac{1}{2})^3\mid_0^1=\frac{1}{12},</me> <me>\v_2=\sqrt{12}(X-\frac{1}{2}).</me> In the next step we have <me>\begin{aligned}
    \hat{\v}_3 &amp;=&amp; \b_3-\langle \b_3, \v_1\rangle \cdot \v_1 - \langle \b_3, \v_2\rangle \cdot \v_2\\
    &amp;=&amp; X^2-  \definite X^2 dx{0}1 \cdot 1 - \definite X^2 \sqrt{12}(X-\frac{1}{2}) dx{0}1 \cdot \sqrt{12}(X-\frac{1}{2})\\
    &amp;=&amp;X^2-\frac{1}{3}-12 \definite (X^3-\frac{1}{2}X^2) dx{0}1 \cdot (X-\frac{1}{2})\\
    &amp;=&amp;X^2-\frac{1}{3}- (X-\frac{1}{2})=X^2-X+\frac{1}{6}.
    \end{aligned}</me> <me>\norm{\hat{\v}_3}=180</me> <me>\v_3=\sqrt{180}(X^2-X+\frac{1}{6}).</me> The desired <m>ONB</m> is <m>\{1, \sqrt{12}(X-\frac{1}{2}), \sqrt{180}(X^2-X+\frac{1}{6})\}</m>.
          </p><!--</div attr= class="example">-->
    
          <p>
            Having a ONB presents the advantage that it make it easy to write any vector in <m>V</m> as a linear combination of the ONB. We can even give a closed formula for the coefficients of such a linear combination.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
            If <m>\{\v_1,\ldots, \v_n\}</m> is a ONB for an inner product space <m>V</m> and <m>\u\in V</m> then the unique way to write <m>\u</m> as a linear combination of the <m>\v_i</m>s is <me>\u = \ip{\u,\v_1} \cdot \v_1+\cdots + \ip{\u,\v_n} \cdot \v_n.</me>
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Since <m>\{\v_1,\ldots, \v_n\}</m> is a basis, in particular it spans <m>V</m>, so there exist <m>\l_1,\ldots, \l_n</m> such that <me>\u =\l_1\v_1+\cdots + \l_n\v_n .</me> Substituting gives <m>\ip{\u,\v_i}=\l_1\ip{\v_1, \v_i}+\cdots + \l_n\ip{\v_n,\v_i}=\l_i</m>, as claimed.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="definition"-->
          <p>
            Let <m>V</m> be an inner product space and let <m>\u, \v\in V</m>. Then the <em>orthogonal projection</em> of <m>\v</m> onto <m>\u</m> is a vector <m>\vv{p}</m> so that <m>\v=\vv{p}+\w</m> with <m>\vv{p}\in \linspan\{\u\}</m> and <m>\w</m> orthogonal to <m>\u</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="qu"-->
          <p>
            The orthogonal projection of <m>\u</m> onto <m>\v</m> is unique and is given by <me>{\proj}_{\u}(\v)=\frac{\langle \u,\v\rangle}{\norm{\u}^2}\cdot\u.</me>
          </p><!--</div attr= class="qu">-->
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>V</m> is an inner product space and <m>U\of V</m> is a subspace then we define the <em>orthogonal complement to <m>U</m></em> to be the set <me>U\perp = \setof{\x\in V}{\text{$\ip{\x,\u}=0$ for all $\u\in U$}}. \qedhere</me>
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="qu"-->
          <p>
            The set <m>U\perp</m> is a subspace of <m>V</m>; this is a consequence of the linearity of <m>\ip{,}</m> in the first input.
          </p><!--</div attr= class="qu">-->
    
    <!-- div attr= class="lemma"-->
          <p>
             If <m>U</m> is a subspace of a finite dimensional inner product space <m>V</m>, then <me>V = U\oplus U\perp.</me>
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            To show that <m>U+ U\perp</m> is a direct sum we use . The fact that <m>U</m> and <m>U\perp</m> have trivial intersection is a consequence of the fact that if <m>\u \in U\cap U\perp</m> then <m>\ip{\u,\u} = 0</m> and hence <m>\u=0</m> by positive definiteness.
          </p>
    
          <p>
            To show that <m>U+U\perp</m> is all of <m>V</m>, start by picking an orthonormal basis <m>\set{\us[r]}</m> for <m>U</m>. Given <m>\v\in V</m> set <me>\w = \v - \sum_{i=1}^r \ip{\v,\u_i} \u_i.</me> Note that <m>\u:=\v-\w= \sum_{i=1}^r \ip{\v,\u_i} \u_i \in U</m> so it suffices to show that <m>\w\in U\perp</m>. For this in turn it suffices to show that <m>\ip{\w,\u_j}=0</m> for <m>j=1,2\dots,r</m>. We have <me>\ip{\w,\u_j} = \ip{\v,\u_j} - \sum_{i=1}^r \ip{\v,\u_i}\ip{\u_i,\u_j} = \ip{\v,\u_j} - \ip{\v,\u_j} = 0.</me> We have shown <m>\v=\u+\w</m> where <m>\u\in U</m> and <m>\w\in U\perp</m>, so <m>\v\in U+U\perp</m>. Thus <m>V\subseteq U+U\perp</m>. Since the opposite containment is true, we conclude <m>V= U+U\perp</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="definition"-->
          <p>
            shows that if <m>U</m> is a subspace of a finite dimensional inner product space <m>V</m>, then for each <m>\v\in V</m> there exist unique vectors <m>\u \in U</m> and <m>\w\in U\perp</m> (which depend on <m>\v</m>) so that <m>\v=\u+\w</m>. Then the <em>projection of <m>\v</m> onto <m>U</m></em> is <m>{\proj}_{U}(\v)=\u</m> and the <em>projection map</em> from <m>V</m> to <m>U</m> is the function <m>P:V\to U, P(\v)= {\proj}_{U}(\v)=\u</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="remark"-->
          <p>
            If <m>\set{\us[r]}</m> is an orthonormal basis for <m>U</m> then shows that <me>\label{eq:projection}
     {\proj}_{U}(\v)=\sum_{i=1}^r \ip{\v,\u_i} \u_i.</me>
          </p><!--</div attr= class="remark">-->

        </section>

        <section xml:id="sec-least-squares">
          <title>Application: Least Squares (Linear Regression)</title>
          
          <p>
            Often people want to solve linear equations that can't be solved. For instance given a large collection of points <m>\set{(x_i,y_i)}_{i=1}^n</m> they want to find a straight line that goes through the points. Suppose the straight line has equation <m>y=\alpha x+\beta</m> for some real numbers <m>\alpha, \beta</m>. Then we want to solve for <m>\alpha,\beta</m> in the equations <me>\begin{aligned}
    \begin{cases}
        \alpha x_1 + \beta &amp;= y_1 \\
        \alpha x_2 +  \beta &amp;= y_2 \\
                 &amp;\vdots \\
        \alpha x_n +  \beta &amp;= y_n.
        \end{cases}
    \end{aligned}</me> If we write <m>M</m> for the matrix <me>M=  \mat[x_1&amp;1\\x_2&amp;1\\\vdots&amp;\vdots\\x_N&amp;1]</me> we can write the system above in matrix form as given below <me>\label{eq:regression}
    M\begin{bmatrix} \alpha \\ \beta \end{bmatrix}= \begin{bmatrix} y_1\\ \vdots \\ y_n \end{bmatrix}.</me>
          </p>
    
    <!-- div attr= class="center"-->
          <figure>
      <image source="Linear_least_squares_example2"/>
            <caption>image</caption>
    </figure><!--</div attr= class="center">-->
    
          <p>
            Of course typically it is not possible to solve <xref ref="eq:regression" />, meaning that this system has no solution <m>\alpha, \beta</m> because the points we started with are not actually on any line. So instead we wish to find the nearest thing in terms of minimizing distances. More formally, let's consider the actual <m>y</m>-coordinates of the <m>n</m> points on some line which correspond to <m>x_1, \ldots, x_n</m>. Say these <m>y</m>-values are <m>p_1,\ldots, p_n</m> and set <m>\vv{p}=\begin{bmatrix} p_1\\ \vdots\\ p_n\end{bmatrix}</m>. The vectors we can get in this way are exactly <m>\vv{p}=\alpha\x+\beta = M\col{\alpha \\ \beta}</m>. We are interested in minimizing the distance from <m>\vv{y}</m> to <m>\vv{p}</m>, that is <m>\norm{\vv{y}-\vv{p}}</m>. This is equivalent to minimizing <m>\norm{\vv{y}-\vv{p}}^2</m>.
          </p>
    
          <p>
            The least squares problem is then <me>\label{leastsquares}
    \begin{cases}
    \text{ minimize } \norm{\vv{y}-\vv{p}}^2=(y_1-p_1)^2+\cdots +(y_n-p_n)^2\\
    \text{ subject to } \vv{p}\in \left\{  M\col{\alpha \\ \beta} \mid \alpha, \beta \in \R \right\}=:U.
    \end{cases}</me>
          </p>
    
          <p>
            It turns out that the unique optimal solution to this problem is <me>\label{eq:leastsquaressol}
    \vv{p}=\proj_{U}\vv{y}. %\text{ and } (\alpha, \beta)= T_M^{-1}(\p^*).</me> We justify this below. To compute the projection <m>\vv{p}</m> from <xref ref="eq:leastsquaressol" /> we may employ <xref ref="eq:projection" /> which requires computing an othonormal basis of <m>V</m>.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
             If <m>V</m> is an inner product space, <m>U\of V</m> is a subspace, and <m>\vv{y}\in V</m> then the unique point <m>\u\in U</m> with <m>\norm{\vv{y}-\u}</m> smallest is <m>\vv{p}=\proj_U\vv{y}</m>.
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            First note that there is, by <xref ref="lem:UUperp" />, a unique way of writing <m>\vv{y}= \vv{p}+\w</m> with <m>\vv{p}\in U</m>, <m>\w\in U\perp</m>. Now suppose that <m>\u</m> is any element of <m>U</m>. We have <me>\begin{aligned}
            \norm{\vv{y}-\u}^2 &amp;= \norm{(\vv{y}-\vv{p})+(\vv{p}-\u)}^2 \\
                &amp;= \norm{\vv{y}-\vv{p}}^2 + \norm{\vv{p}-\u}^2+ 2\ip{\vv{y}-\vv{p},\vv{p}-\u}\\
                &amp;\ge \norm{\vv{y}-\vv{p}}^2,
        
    \end{aligned}</me> with equality only when <m>\u=\vv{p}</m>. The second equality in the above display uses the fact that <m>\vv{y}-\vv{p}\in U\perp</m> and <m>\vv{p}-\u\in U</m>.
          </p><!--</div attr= class="proof">-->

        </section>

        <section xml:id="sec-riesz">
          <title>Riesz Representation Theorem</title>
          
          <p>
            If <m>\ip{,}</m> is an inner product on <m>V</m> and we fix <m>\v\in V</m> then the map <m>\w\mapsto \ip{\v,\w}</m> is an element of <m>V\dual</m> (since in fact this is true for any bilinear form). More is true though; in fact any function in <m>V\dual</m> can be represented this way. This is the content of the following theorem.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>\ip{,}</m> is an inner product on a finite dimensional vector space <m>V</m> and <m>f\in V\dual</m> then there exists a unique vector <m>\v_f\in V</m> such that for all <m>\w\in V</m> we have <me>f(\w) = \ip{\w,\v_f}.</me> Moreover, the map <m>\varphi:V^*\to V</m> given by <m>\varphi(f)\mapsto \v_f</m> is a bijection.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            By <xref ref="thm:GS" /> there is a ONB <m>B=\set{\vs}</m> for <m>V</m> such that for all <m>i,j\in [n]</m> we have <me>\ip{\v_i,\v_j} = \begin{cases}
                1 &amp; i=j \\
                0 &amp; i\not= j
            \end{cases}.</me> Consider what <m>f</m> does to the <m>\v_i</m>. For some <m>\mu_i\in \R</m> we have <m>f(\v_i) = \mu_i</m>. Define <m>\v_f = \sum_1^n\overline{\mu_i} \v_i</m>. Since <m>B</m> is a basis we know that for any <m>\w\in V</m> there exist <m>\l_i\in \F</m> such that <m>\w=\Slv</m>, and therefore <me>f(\w) = f\left(\Slv\right) = \sum_1^n \l_i f(\v_i) = \sum_1^n \l_i \mu_i = \sum_1^n \l_i \ip{\v_i,\v_f} = \ip{\w,\v_f}.</me> Suppose that vectors <m>\v,\v'</m> are both candidates to be <m>\v_f</m>; in other words for all <m>\w\in V</m> we have <me>f(\w) = \ip{\w,\v} = \ip{\w,\v'}.</me> In particular <m>\ip{\w,\v-\v'}=0</m> for all <m>\w\in V</m>. Choosing <m>\w=\v-\v'</m> gives <m>\norm{\v-\v'}=0</m> and therefore <m>\v=\v'</m>.
          </p>
    
          <p>
            The proof so far has justified that the rule <m>f\mapsto \v_f</m> defines a function <m>\varphi:V^*\to V</m> and that this function is given by <me>\varphi(f)=\v_f=\sum_1^n\overline{f(\v_i)} \v_i.</me> Now let's try to define an inverse to this function. Consider a function <m>\psi:V\to V^*</m> given by <m>\psi(\sum \l_i \v_i)</m> is the unique linear function <m>g\in V^*</m> so that <m>g(\v_i)=\overline{\l_i}</m>.
          </p>
    
          <p>
            Now if <m>f\in V^*</m>, then <m>(\psi\circ\varphi)(f)=\psi(\sum_1^n\overline{f(\v_i)} \v_i)</m> is the unique linear that maps <m>\v_i \mapsto \overline{\overline{f(\v_i)}}=f(\v_i)</m>. But <m>f</m> also maps <m>v_i\mapsto f(\v_i)</m> so by uniqueness we have <m>(\psi\circ\varphi)(f)=f</m>.
          </p>
    
          <p>
            If <m>\v=\sum \l_i \v_i\in V</m>, then <m>(\varphi\circ \psi)(\v)=\sum_1^n\overline{\psi(\v)(\v_i)} \v_i=\sum_1^n\overline{\overline{\l_i}} \v_i=\sum_1^n \l_i\v_i=\v.</m>
          </p>
    
          <p>
            We have thus shown that <m>\psi</m> is the inverse of <m>\varphi</m>, so <m>\varphi</m> is bijective.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="remark"-->
          <p>
            The function <m>\varphi</m> in is not linear if <m>\F=\C</m>, but it is linear if <m>\F=\R</m>. So in the case of real inner product spaces this function gives a canonical vector space isomorphism <m>V^*\cong V</m>.
          </p><!--</div attr= class="remark">-->
    
    <!-- div attr= class="example"-->
          <p>
             We know that the evaluation function <m>e_{3/2} : V_{10}\to \R</m> defined by <m>e_{3/2}(p)=p(3/2)</m> is a linear map from <m>V_n</m> to <m>\R</m>. The Riesz Representation Theorem tells us that there is some fixed polynomial <m>q</m> such that for all <m>p\in V_{10}</m> we have <me>p\left(\frac32\right) = e_{3/2}(p) = \ip{p,q} = \definite p(x) q(x) dx{-1}1.</me> It turns out that this polynomial is <me>\begin{aligned}
            q(x) = \frac{736696275275961}{134217728} &amp;\, x^{10} + \frac{17640939135105}{16777216} \, x^{9} 
                        -\frac{1717751356101495}{134217728} \, x^{8} \\ &amp;- \frac{9177061112505}{4194304} \, x^{7} 
                        +\frac{693697040221185}{67108864} \, x^{6} \\ &amp;+ \frac{12562845410115}{8388608} \, x^{5}  
                        -\frac{225576769693275}{67108864} \, x^{4} \\ &amp;-\frac{1563597220185}{4194304} \, x^{3} 
                        +\frac{50390971636005}{134217728} \, x^{2} \\ &amp;+\frac{409434133185}{16777216} \, x - \frac{876496780467}{134217728}.
        
    \end{aligned}</me>
          </p><!--</div attr= class="example">-->

        </section>

        <section xml:id="sec-adjoints">
          <title>Adjoints</title>
          
          <p>
            Suppose <m>V,W</m> are real vector spaces each equipped with an inner product, and that <m>T\in L(V,W)</m>. We define the <em>adjoint</em> of <m>T</m>, written <m>T^*:W\to V</m> as follows. Given a vector <m>\w\in W</m> we define a linear map in <m>V\dual</m> by <me>\v \mapsto \ip{T(\v),\w}.</me> This is clearly linear (once we check that we can in fact compute <m>\ip{T(\v),\w}</m> since <m>T(\v)\in W</m>). Therefore, by the Riesz Representation Theorem, there is a unique vector, that we denote by <m>T^*(\w)</m>, with the property that for all <m>\v\in V</m> we have <me>\ip{T(\v),\w} = \ip{\v,T^*(\w)}</me> The next lemma verifies that <m>T^*:W\to V</m> is linear.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="lemma"-->
          <p>
            Given <m>T:V\to W</m> between finite dimensional Euclidean spaces the map <m>T^*:W\to V</m> is linear.
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We need to check that for <m>\w,\w'\in W</m> and <m>\l,\mu\in \R</m> the vector <m>\l T^*(\w)+\mu T^*(\w')</m> has the property that for for all <m>\v\in V</m> <me>\ip{T(\v),\l\w+\mu\w'} = \ip{\v,\l T^*(\w)+\mu T^*(\w')}.</me> To verify this we compute as follows: <me>\begin{aligned}
            \ip{T(\v),\l\w+\mu\w'} &amp;= \l\ip{T(\v),\w} + \mu\ip{T(\v),\w'} \\
                &amp;= \l\ip{\v,T^*(\w)} + \mu\ip{\v,T^*(\w')} \\
                &amp;= \ip{\v,\l T^*(\w)+\mu T^*(\w')}.
        
    \end{aligned}</me> The second equality used the definition of <m>T^*(\w)</m> and <m>T^*(\w')</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="example"-->
          <p>
            Consider the subspace <m>V =\linspan\set{\sin(kx),\cos(kx)\mid 0\leq k\leq n} \of C[0,2\pi]</m>. Let the linear map <m>D:V\to V</m> be the differentiation map <m>D(f) = f'</m>. I claim that <m>D^*:V\to V</m> is given by <me>D^* = -D.</me> To prove this we have to show that for all <m>f\in F_n</m> and <m>g\in F_n</m> we have <m>\ip{D(f),g} = \ip{f,-D(g)}</m>. This is simply integration by parts: <me>\begin{aligned}
            \ip{D(f),g} &amp;= \definite f'(x) g(x) dx0{2\pi} \\
                &amp;= \left. f(x) g(x) \vphantom{\Big(}\right|_0^{2\pi} - \definite f(x) g'(x) dx0{2\pi} \\
                &amp;= -\ip{f,D(g)} = \ip{f,-D(g)}.
        
    \end{aligned}</me>
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="definition"-->
          <p>
            For a matrix <m>A</m> with entries in <m>\C</m> we define <m>A^*=(\overline{A})^T</m> to be the conjugate-transpose of <m>A</m>. Specifically, if <m>A=[a_{ij}]</m> then <m>A^*=[\overline{a_{ji}}]</m>. Transposeing commutes with conjugation so we could also write <m>A^*=(\overline{A^T})</m>.
          </p>
    
          <p>
            If the entries of <m>A</m> are in <m>\R</m> then <m>A^*=A^T</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
          <p>
            Consider the vector spaces <m>\C^n</m> and <m>\C^m</m> with their standard inner product . The adjoint of the linear map <m>M_A:\C^n\to \C^m, M_A(\x)=A\x</m> is the linear map <m>M_{A^*}:\C^m\to \C^n, M_{A^*}:(\x)=A^*\x</m>. To see this note that for <m>\v\in \C^n</m> and <m>\w\in \C^m</m> <me>\begin{aligned}
            \ip{M_A(\v),\w} &amp;= \ip{A\v,\w} \\
                &amp;= (A\v)\trans \overline{ \w} \\
                &amp;= \v\trans A\trans \overline{ \w}\\
                &amp;= \v\trans \overline{\overline{A\trans}\w}\\
                &amp;= \ip{\v,\overline{A\trans} \w}.
        
    \end{aligned}</me>
          </p><!--</div attr= class="example">-->
    
          <p>
            We will prove a version of the Rank-Nullity formula that gives, for inner product spaces, an even clearer picture of the situation.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
             If <m>V,W</m> are inner product spaces and <m>T\in L(V,W)</m> then <m>T^{**} = T</m>.
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            If <m>\v\in V</m> then the vector <m>T^{**}</m> is the unique vector in <m>W</m> satisfying <me>\ip{T^*(\w),\v}  = \ip{\w,T^{**} (\v)}</me> for all <m>\w\in W</m>. We'll show that <m>T(\v)</m> has this property; for all <m>\w\in W</m> <me>\begin{aligned}
            \ip{T^*(\w),\v} &amp;=\overline{ \ip{\v,T^*(\w)}} \\
                &amp;=\overline{  \ip{T(\v),\w}} \\
                &amp;= \overline{ \overline{\ip{\w,T(\v)}}}=\ip{\w,T(\v)}.
        
    \end{aligned}</me> Thus <m>T^{**} (\v)=T(\v)</m> for all <m>\v\in V</m>.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>\v,\w\in V</m>, an inner product space, then we say that two subspaces <m>U,W</m> of <m>V</m> are <em>orthogonal</em> if <m>\u,\w</m> are orthogonal for all <m>\u\in U</m>, <m>\w\in W</m>. If <m>V=U\dsum W</m> and <m>U,W</m> are orthogonal then we write <me>V = U \odsum W. \qedhere</me>
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>V,W</m> are finite dimensional inner product spaces and <m>T\in L(V,W)</m> then <me>\begin{aligned}
            V &amp;= \ker(T) \odsum \range(T^*) \\
            W &amp;= \range(T) \odsum \ker(T^*).
        
    \end{aligned}</me> Moreover <m>\rank(T) = \rank(T^*)</m>.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            First note that by <xref ref="lem:transtrans" /> we need prove only one of the two equalities, since the second is simply what the first says about <m>T^*</m>. We start by showing that <m>\ker(T)</m> is orthogonal to <m>\range(T^*)</m>, and that therefore in particular their intersection is <m>\set{\0}</m>. Take <m>\u\in \ker(T)</m> and <m>\v=T^*(\w)\in \range(T^*)</m>. Then <me>\ip{\u,\v} = \ip{\u,T^*(\w)} = \ip{T(\u),\w} = \ip{\0,\w} = 0.</me>
          </p>
    
          <p>
            To prove that <m>\ker(T)+\range(T^*)</m> is all of <m>V</m> we're going to give a dimension argument and use that <m>\rank(T) = \rank(T^*)</m>, or in other words that <m>\dim(\range(T)) = \dim(\range(T^*))</m>. We'll start by proving that <m>\rank(T^*)\le \rank(T)</m>. Consider then the linear map <me>\begin{aligned}
            \tilde{T}\colon \range(T^*) &amp;\to \range(T) \\
            \v &amp;\mapsto T(\v)
        
    \end{aligned}</me> This map has trivial kernel, exactly by the argument above: a vector <m>\v</m> that was in <m>\range(T^*)</m> and had <m>T(\v)=\0</m> would be in <m>\ker(T)\cap \range(T^*)=\set{\0}</m>. By <xref ref="thm:dim_monotone" /> we get <m>\dim(\range(T^*))\le \dim\range(T)</m>. Now using <xref ref="lem:transtrans" /> again we have <m>\rank(T)=\rank(T^{**} )\le \rank(T^*)</m>. This tells us that <m>\ker(T) \odsum \range(T^*)</m> is a subspace of <m>V</m> having dimension <m>\dim(V)</m>, so the two spaces are equal.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="example"-->
          <p>
            Suppose that <m>U</m> is a subspace of an inner product space <m>V</m> and we consider the inclusion map <me>\begin{aligned}
            T \colon U &amp;\to V \\
            \u &amp;\mapsto \u.
        
    \end{aligned}</me> What is <m>T^*</m>? It is the <em>orthogonal projection</em> onto <m>U</m>. Since <m>V = U\odsum U\perp</m> we can represent any vector <m>\v\in V</m> uniquely as <m>\v=\u+\w</m> where <m>\u\in U</m> and <m>\w\in U\perp</m>. The projection map is the map <m>P:V\to U</m> sending <m>\v</m> to <m>\u</m>. To prove that <m>T^* = P</m> consider <m>\u'\in U</m> and <m>\v\in V</m> with <m>\v=\u+\w</m> as before. Thus <m>T(\u')=\u'</m> and <m>P(\v)=\u</m>. We need to show that <m>\ip{T(\u'),\v} = \ip{\u',P(\v)}</m>: <me>\ip{T(\u'),\v} = \ip{\u',\u+\w} = \ip{\u',\u} + \ip{\u',\w} = \ip{\u',\u} = \ip{\u',P(\v)},</me> as required.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="lemma"-->
          <p>
            Let <m>T:V\to W</m> be an injective linear map on a finite dimensional inner product space. Then the map <m>T^* T:V\to V</m> is bijective.
          </p><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            To show the map is bijective it suffices to show it is injective, that is, <m>\ker(T^* T)=\set{\0}</m>. Suppose that <m>T^* T(\v)=\0</m>. Then <me>\ip{T(\v),T(\v)} = \ip{\v,T^* T(\v)} = \ip{\v,\0} =0,</me> so <m>T(\v)=0</m>. Since <m>T</m> is injective we deduce that <m>\v=\0</m>, so we're done.
          </p><!--</div attr= class="proof">-->
    
          <p>
            Returning to our example of fitting a straight line to our set <m>\set{(x_i,y_i)}_1^N</m> of points we see that <m>\vv{p}=\proj_{\range(M)}\vv{y}</m> is the closest point in <m>\range(M)</m> to <m>\vv{y}</m>. Thus to solve for <m>\alpha</m> and <m>B</m> we should solve <m>M\col{\alpha \\\beta } = \vv{p}</m>.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
            The solution to the least squares problem in is <me>\col{\alpha \\ \beta} = (M\trans M)\inv M\trans \vv{y}.</me>
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Let <m>T:V\to V</m> be the linear map given by <m>T(\v)=M\v</m>.
          </p>
    
          <p>
            The final piece of the puzzle is the fact that by <xref ref="thm:four_subspaces" /> we know exactly what <m>\range(T)\perp</m> is: it's just <m>\ker(T^*)</m> and over the real numbers <m>T^*(\w)=M^T\w</m>. Thus we can compute as follows using that <m>\vv{y}=\vv{p}+\vv{n}</m> with <m>\vv{n}\in \range(T)\perp</m>. <me>\begin{aligned}
        M\col{A\\B} &amp;= \vv{p}\\
            &amp;= \vv{y}-\vv{n}\\
        M\trans M \col{A\\B} &amp;= M\trans (\vv{y}-\vv{n}) \\
            &amp;= M\trans\vv{y}.
        \intertext{By the previous lemma $M^TM$ is invertible so there is a unique solution}
        \col{A\\B} &amp;= (M\trans M)\inv M\trans \vv{y}\\
        \vv{p}= M\col{A\\B} &amp;= M (M\trans M)\inv M\trans \vv{y}.
    \end{aligned}</me>
          </p><!--</div attr= class="proof">-->

        </section>

      </chapter>

      <chapter xml:id="ch-eigen-revisited">
        <title>Eigenvectors and Eigenvalues</title>
        
        <section xml:id="sec-simplest-matrix">
          <title>Simplest Matrix of a Linear Transformation</title>
          
          <p>
            Here is a theorem we could have proved a long time ago.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>V,W</m> are finite dimensional and <m>T:V\to W</m> is linear then there are bases <m>B,B'</m> for <m>V,W</m> respectively such that the matrix of <m>T</m> has the form <me>[T]_B^{B'}=\mat[1\\&amp;1\\&amp;&amp;\ddots\\&amp;&amp;&amp;1&amp;\phantom{0}&amp;\phantom{0}&amp;\phantom{0}\\\\\\]</me> where all entries are <m>0</m> except for <m>r</m> <m>1</m>s on the main diagonal, where <me>r=\rank(T)=\dim(\range(T)).</me>
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We already know the appropriate bases; they are those used in Theorem <xref ref="thm:ranknullity" />. There we took a basis <m>\set{\us[n-r]}</m> for <m>\ker(T)</m>, a basis <m>\set{\ws[r]}</m> for <m>\range(T)</m> and picked preimages <m>\v_i</m> such that <m>T(\v_i)=\w_i</m> for <m>i=1,2,\dots,r</m>. Extend <m>\set{\ws[r]}</m> (arbitrarily) to a basis <m>B'</m> of <m>W</m>, and set <m>B = \set{\vs[r],\us[n-r]}</m>. With respect to these bases the matrix of <m>T</m> has the form above. The fact that <m>T(\v_i)=\w_i</m> gives the first <m>r</m> columns and the fact the <m>T(\u_j)=\0</m> gives the next <m>n-r</m>.
          </p><!--</div attr= class="proof">-->
    
          <p>
            Though this theorem appears to completely answer the question of the simplest form of the matrix of an linear map (and in a beautiful way) it is actually substantially less valuable than it appears at first sight. The most natural cases of linear maps come from <m>L(V,V)</m>; linear maps from a space to itself. For these we want to think about only one basis for <m>V</m>, that is, we want to have <m>B=B'</m>, and use that basis to express both the input vector <m>\v</m> and the output vector <m>T(\v)</m>. It turns out the simple form in cannot always be achieved for <m>[T]_B^B</m>. This leads us into deeper waters, which we explore in this chapter.
          </p>

        </section>

        <section xml:id="sec-eigen-again">
          <title>Eigenvectors and Eigenvalues</title>
          
          <p>
            The most general tool we have to help us understand high powers of linear maps is the of eigenvectors.
          </p>
    
    <!-- div attr= class="definition"-->
          <p>
            For a vector space <m>V</m> we denote <m>L(V,V)</m> the vector space of linear maps <m>T:V\to V</m>. We call such a map a linear <em>operator</em> on <m>V</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
          <p>
            The identity map <m>I:V\to V</m>, <m>I(v)=v, \forall v\in V</m> is an element of <m>L(V,V)</m>.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="definition"-->
          <p>
            If <m>V</m> is a vector space and <m>T\in L(V,V)</m> then we say that <m>\v\in V\wo\set{\0}</m> is an <em>eigenvector</em> of <m>T</m> if <m>T\v=\l \v</m> for some scalar <m>\l\in\F</m>. If <m>\v</m> is an eigenvector and <m>T\v=\l\v</m> we say that <m>\l</m> is the <em>eigenvalue</em> associated to <m>\v</m>.
          </p><!--</div attr= class="definition">-->
    
    <!-- div attr= class="example"-->
    <!-- div attr= class="compactenum"-->
          <p>
            If <m>V</m> is the vector space of infinitely differentiable functions from <m>\R\to \R</m> and <m>D:V\to V</m> mapping <m>f</m> to <m>f'</m> is the differentiation map then any exponential function <m>f(x) = Ab^x</m> is an eigenvector of <m>D</m> with eigenvalue <m>\ln(b)</m>.
          </p>
    
          <p>
            Consider the <em>left shift map</em> on <m>\R^\N</m> defined by <me>\begin{aligned}
                L \colon \R^\N &amp;\to \R^\N \\
                (a_i)_1^\infty &amp;\mapsto (a_{i+1})_{i=1}^\infty
            
    \end{aligned}</me> Again we have that exponential sequences are eigenvectors: if <m>a_i = Ab^{i-1}</m> then <m>(a_i)_1^\infty</m> is an eigenvector with eigenvalue <m>b</m>. Even <m>b=0</m> gives an eigenvector; <m>(1,0,0,\dots)\in \ker(L)</m> and is thus an eigenvector with eigenvalue <m>0</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="example">-->
    
          <p>
            The following simple observation is surprisingly important in finding eigenvectors. It shifts the focus of attention away from the eigenvector onto the eigenvalue.
          </p>
    
    <!-- div attr= class="lemma"-->
          <p>
            For a linear map <m>T\in L(V,V)</m>, the following are equivalent:
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            <m>T</m> has an eigenvector with eigenvalue <m>\l\in \F</m>
          </p>
    
          <p>
            <m>\ker(T-\l I) \not=\set{\0}</m>
          </p>
    
          <p>
            <m>T-\l I</m> is not injective.
          </p><!--</div attr= class="compactenum">-->
    
          <p>
            If the vector space <m>V</m> is finite dimensional then the statements above are further equivalent to
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            <m>T-\l I</m> is not injective,
          </p>
    
          <p>
            <m>T-\l I</m> is not surjective.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="lemma">-->
    
    <!-- div attr= class="proof"-->
          <p>
            We have <me>T\v = \l\v \quad\iff\quad T\v-\l \v = \0 \quad\iff\quad (T-\l I)\v  = \0 \quad\iff\quad \v\in \ker(T-\l I).</me> This shows the equivalence of 1. and 2. For 2. and 3., see . For the equivalence of 3., 4., and 5. see .
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="example"-->
          <p>
            If <me>A = \mat[2&amp;1\\4&amp;-1]</me> then the map <m>T=M_A:\R^2\to \R^2</m> has an eigenvector of eigenvalue <m>\l</m> exactly if the matrix <me>A - \l I = \mat[2-\l&amp;1\\4&amp;-1-\l]</me> has rank less than <m>2</m>. We have a quick way of determining (this is a pun) whether the matrix <m>\mat[a&amp;b\\c&amp;d]</m> has rank less than <m>2</m>. This happens precisely when <m>ad-bc=0</m>. Thus <m>\l</m> is an eigenvalue of <m>T</m> exactly if <me>(2-\l)(-1-\l) - 4 = \l^2 - \l -6 = (\l-3)(\l+2) = 0.</me> Thus the eigenvalues of <m>T</m> are precisely <m>3,-2</m>. To find the corresponding eigenvectors we need to find <m>\ker(T-\l I)</m> for each of <m>\l=3,-2</m>. We have <me>\ker(A-3I) = \ker\mat[-1&amp;1\\4&amp;-4] = \linspan\set{\col{1\\1}} \qquad \ker(A+2I) = \ker\mat[4&amp;1\\4&amp;1] = \linspan\set{\col{1\\-4}}.</me> If we choose the basis <m>B=\{\v,\w\}</m> where <m>\v=\col{1\\1}</m> and <m>\w=\col{1\\-4}</m> then the matrices of <m>T</m> and <m>T^n</m> with respect to this basis are <me>[T]_B^B=    \mat[3&amp;0\\0&amp;-2] \qquad \text{and}  \qquad [T^n]_B^B= \mat[3^n&amp;0\\0&amp;(-2)^n]</me> respectively.
          </p><!--</div attr= class="example">-->
    
    <!-- div attr= class="example"-->
          <p>
             Consider the <em>right shift map</em> on <m>\R^\N</m> defined by <me>\begin{aligned}
            R \colon \R^\N &amp;\to \R^\N \\
            (a_i)_{i=1}^\infty &amp;\mapsto \left(\begin{cases}
                a_{i-1} &amp; i \ge 2 \\
                0       &amp; i = 1
            \end{cases} \right)_{i=1}^\infty
        
    \end{aligned}</me> This map has no eigenvectors. First consider <m>\l\not=0</m> and a solution to <m>R\v=\l\v</m>. If <m>\v=(v_i)_{i=1}^\infty \not=\0</m> then let <m>i</m> be minimal such <m>v_i\not=0</m>. We have <me>0 = (R\v)_i = \l v_i \not=0.</me> This contradiction establishes that <m>\v=\0</m>, or in other words that <m>\l</m> is not a eigenvalue of <m>R</m>. On the other hand if <m>\l=0</m> we'd be looking for <m>\v\in \ker(R)</m>. It is clear however that <m>\ker(R)=\set{\0}</m>; if <m>\v=(v_i)_{i=1}^\infty</m> has <m>v_i\not=0</m> then <m>(R\v)_{i+1} \not= 0</m>.
          </p><!--</div attr= class="example">-->
    
          <p>
            To compute eigenvalues it is very helpful to endow the vector space <m>L(V,V)</m> with extra structure.
          </p>
    
    <!-- div attr= class="proposition"-->
          <p>
            Given a vector space <m>V</m> over a field <m>\F</m>, the set <me>L(V,V)=\{T:V\to V \mid T \text{ linear} \}</me> endowed with the operations of addition <m>(T+S)(\v)=T(\v)+S(\v)</m>, scalar multiplication <m>(\l T)(\v)=\lT(\v)</m>, and composition <m>(T\circ S)(\v)=T(S(\v))</m> (written from now on as multiplication <m>TS</m>) is a <m>\F</m>-<em>algebra</em> meaning that it satisfies:
          </p>
    
    <!-- div attr= class="compactenum"-->
          <p>
            <m>L(V,V)</m> with its addition and scalar multiplication is a <m>\F</m>-vector space
          </p>
    
          <p>
            Composition is associative: <m>A (B C) = (AB)C</m> for all <m>A, B,C\in L(V,V)</m>
          </p>
    
          <p>
            Composition distributes over addition: <m>A(B+C) = AB+AC</m> and <m>(A+B)C = AC+BC</m> for all <m>A,B,C\in L(V,V)</m>
          </p>
    
          <p>
            Composition respects scalar multiplication: <m>(\l A)(\mu B) = (\l\mu) AB</m> for all <m>\l,\mu\in \F</m>, <m>A,B\in L(V,V)</m>.
          </p><!--</div attr= class="compactenum">--><!--</div attr= class="proposition">-->
    
    <!-- div attr= class="definition"-->
          <p>
            Given any <m>T\in L(V,V)</m> and any polynomial <m>p\in \F[X], p(x)=a_0+a_1X+a_2X^2+\cdots a_dX^d</m> we can sensibly define <m>p(T)\in L(V,V)</m> in the following way: for <m>n\geq 0</m> set <m>T^n=T\circ T\circ \cdots \circ T</m> (<m>n</m> times) and define <me>p(T)=a_0+a_1T+a_2T^2+\cdots a_dT^d.</me>
          </p><!--</div attr= class="definition">-->
    
          <p>
            A very important fact about polynomials in <m>\C[X]</m> is the following.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
            Any monic polynomial <m>p(X)\in \C[X]</m> of degree <m>k\geq 1</m> factors into linear factors; there exist <m>\l_1, \l_2, \ldots, \l_k</m> such that <me>p(X) = (X -\l_1)(X - \l_2) \cdots (X - \l_k).</me>
          </p><!--</div attr= class="theorem">-->
    
          <p>
            The following theorem begins to explore the connection between eigenvalues of <m>T</m> and polynomials satisfied by <m>T</m>.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>V</m> is an <m>n</m>-dimensional <m>\C</m>-vector space and <m>T\in L(V,V)</m> then <m>T</m> has an eigenvector.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Since <m>\dim L(V,V) = n^2</m> the set <me>\set{I,T,T^2,\dots,T^{n^2}}</me> is linearly dependent. In other words there are scalars <m>(\mu_i)_{i=0}^k</m> with (without loss of generality) <m>\mu_k=1</m> such that <me>\sum_{i=0}^k \mu_i T^i = \0 \in L(V,V).</me> Consider then the polynomial <m>p = \sum_{i=0}^k \mu_i X^i \in \C[X]</m>. All monic polynomials (of degree at least <m>1</m>) over <m>\C</m> factor into linear factors; there exist <m>\l_1,\l_2,\dots,\l_k</m> such that <me>p = (X-\l_1)(X-\l_2)\dots(X-\l_k).</me> Thus <me>p(T) = (T-\l_1 I)(T-\l_2 I)\dots(T-\l_k I) = \0</me> Now if <m>\v</m> is <em>any</em> non-zero vector in <m>V</m> we have <m>p(T)\v=\0</m>. Let <m>j</m> be minimal such that <me>\w = (T-\l_{j+1} I)(T-\l_{j+2}I)\dots(T-\l_k I) \v \not= \0.</me> (Note that we might have <m>j=k</m>, but certainly <m>j&gt;0</m>.) Then by the minimality of <m>j</m> we have <me>(T-\l_{j}I) \w = \0,</me> so <m>\w</m> is an eigenvector of <m>T</m>, and <m>\l_{j}</m> is the corresponding eigenvalue.
          </p><!--</div attr= class="proof">-->
    
          <p>
            It is quite possible, as the following example shows, to have an operator on <m>\C^n</m> with only one eigenvector (up to scalar multiples).
          </p>
    
    <!-- div attr= class="example"-->
          <p>
             Consider the map (whose matrix with respect to the usual basis has <m>1</m>s on the diagonal immediately above the main diagonal and <m>0</m>s everywhere else) <me>\begin{aligned}
            S \colon \C^n &amp;\to \C^n \\
            (a_i)_1^{n} &amp;\mapsto \left( \begin{cases}
                a_{i+1} &amp; i &lt; n \\
                0 &amp; i=n .
            \end{cases} \right)_{i=1}^n
        
    \end{aligned}</me> The action of <m>S</m> on the basis vectors <m>\e_1,\e_2,\dots,\e_n</m> is to map <me>\e_n\mapsto \e_{n-1} \mapsto \e_{n-2} \mapsto \dots \mapsto \e_2 \mapsto \e_1 \mapsto \0.</me> Clearly <m>\e_1=(1,0,\dots,0)</m> is an eigenvector with eigenvalue <m>0</m>. The only eigenvectors of <m>S</m> are multiples of <m>\e_1</m>. To see this note that, by an argument precisely analogous to that in <xref ref="ex:right_shift" />, there are no non-zero eigenvalues. If <m>\v=(v_i)_1^n\in \ker(S)</m> then <m>S\v = \0</m> implies <m>v_2=v_3=\dots=v_n=0</m>; in other words <m>\ker(S)=\linspan(\e_1)</m>.
          </p><!--</div attr= class="example">-->
    
          <p>
            We are not able, in light of <xref ref="ex:nilpotent" />, to prove that every linear map <m>T:V\to V</m> on a finite dimensional <m>\C</m>-vector space is diagonalizable. The closest we can come in general is a situation somewhat similar to that example.
          </p>
    
    <!-- div attr= class="theorem"-->
          <p>
            If <m>V</m> is a finite-dimensional <m>\C</m>-vector space and <m>T\in L(V,V)</m> then <m>V</m> has a basis <m>\set{\vs}</m> with respect to which the matrix of <m>T</m> is upper triangular.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            The result is trivially true for <m>\dim(V)=1</m>. Suppose then that <m>\dim(V)&gt;1</m>. Let <m>\v_1</m> be an eigenvector of <m>T</m> with eigenvalue <m>\l</m> and set <m>W=\linspan\set{\v_1}</m>. Consider the map <me>\begin{aligned}
            \tilde{T}\colon V/W &amp;\to V/W \\
            \v+W &amp;\mapsto T\v+W
        
    \end{aligned}</me> This map is well defined since whenever <m>\v-\v'\in W</m> there is some <m>\mu \in \C</m> such that <m>\v-\v'=\mu\v_1</m>. Thus <me>T\v - T\v' = T(\v-\v') = T(\mu\v_1) = \mu T\v_1 = \mu\l \v_1 \in W,</me> thus <m>T\v+W = T\v'+W</m>. Pick <m>\set{\v_2,\v_3,\dots,\v_n}</m> such that <m>\set{\v_2+W,\v_3+W,\dots,\v_n+W}</m> is a basis for <m>V/W</m> with respect to which the basis of <m>\tilde{T}</m> is upper triangular. Now consider the basis <m>\set{\v_1,\v_2,\dots,\v_n}</m> of <m>V</m>. With respect to this basis the matrix of <m>\tilde{T}</m> with respect to <m>\set{\v_2+W,\v_3+W,\dots,\v_n+W}</m> appears in the lower right hand corner. This is because if <me>\begin{aligned}
            T\v_j + W = \tilde{T}(\v_j+W) &amp;= \sum_{i=2}^n a_{ij} (\v_i+W) = \left(\sum_{i=2}^n a_{ij} \v_i \right) + W\\
        \intertext{then for some $a_{1j}\in \C$ we have}
            T\v_j &amp;= a_{1j}\v_1 + \sum_{i=2}^n a_{ij} \v_i.
        
    \end{aligned}</me> Since the first column of the matrix only has a non-zero entry in the first row and, by assumption, the matrix of <m>\tilde{T}</m> is upper triangular, we have that the matrix of <m>T</m> is upper triangular.
          </p><!--</div attr= class="proof">-->
    
    <!-- div attr= class="theorem"-->
          <p>
             If <m>T:V\to V</m> is linear and <m>\vs[k]</m> are eigenvectors with corresponding distinct eigenvalues <m>\ls[k]</m> then the <m>\v_i</m> are linearly independent.
          </p><!--</div attr= class="theorem">-->
    
    <!-- div attr= class="proof"-->
          <p>
            If there are non-trivial linear combinations of the <m>\v_i</m> that are zero take one, say <m>\sum_1^k \mu_i \v_i = \0</m>, with the fewest possible non-zero coefficients. Without loss of generality we may suppose that <m>\mu_1\not=0</m>. Now apply the linear map <m>T-\l_1I</m>. We have <me>\0 = (T-\l_1I) \sum_1^k \mu_i \v_i = \sum_1^k \mu_i(\l_i-\l_1) \v_i.</me> This linear combination of the <m>\v_i</m> has fewer non-zero coefficients than the original (since the coefficient of <m>\v_1</m> is now <m>0</m>) so all the coefficients must be <m>0</m>. But then all of the original coefficients must have been <m>0</m> except <m>\mu_1</m>. But <m>\mu_1\v_1\not= \0</m> since <m>\mu_1\not=0</m> and <m>\v_1\not=\0</m> (it is an eigenvector). This contradiction proves that the <m>\v_i</m> are linearly independent.
          </p><!--</div attr= class="proof">-->
    
          <p>
            The following corollary says that sometimes this is enough to show that <m>T</m> is diagonalizable.
          </p>
    
    <!-- div attr= class="corollary"-->
          <p>
            Suppose that <m>T:V\to V</m> is linear and that <m>V</m> has dimensional <m>n&lt;\infty</m>. If <m>T</m> has <m>n</m> distinct eigenvalues <m>\ls</m> then <m>T</m> is diagonalizable.
          </p><!--</div attr= class="corollary">-->
    
    <!-- div attr= class="proof"-->
          <p>
            Let <m>B = \set{\vs}</m> where each <m>\v_i</m> is an eigenvector with eigenvalue <m>\l_i</m>. By <xref ref="thm:distinct" /> we know <m>B</m> is linearly independent, and since it has the correct size it must be a basis.
          </p><!--</div attr= class="proof">-->
    
          <p>
            We have seen a few ways of looking for eigenvalues. Considering <m>ad-bc</m> worked well for <m>2\by 2</m> matrices. We can find eigenvalues for <m>T:\C^n\to \C^n</m> also by solving polynomial equations. We will turn now to an alternative avenue.
          </p>

        </section>

        <section xml:id="sec-spectral">
          <title>Self-Adjoint Maps and the Spectral Theorem</title>
          
          <!-- div attr= class="definition"-->
			<p>
				A linear map <m>T:V\to V</m> on a Euclidean space <m>V</m> is called <em>self-adjoint</em> if <m>T^* = T</m>.
			</p><!--</div attr= class="definition">-->

<!-- div attr= class="example"-->
			<p>
				If <m>A\in M_{n\by n}(\R)</m> is a symmetric matrix then <m>T=M_A:\R^n\to \R^n</m> is self-adjoint, where we condider <m>\R^n</m> with its standard inner product. We have <me>\ip{T\v,\w} = \ip{A\v,\w} = (A\v)\trans \w = \v A\trans \w = \ip{\v,A\trans \w} = \ip{\v,A\w}.</me>
			</p><!--</div attr= class="example">-->

<!-- div attr= class="example"-->
			<p>
				Let <m>V = C_{00}^\infty[0,1]</m>, the vector space<fn>			<p>
				This example is not actually an example, since <m>V</m> is not finite dimensional, and hence not Euclidean. Ot the other hand the eample is too nice not to include.
			</p></fn> of infinitely differentiable functions <m>f:[0,1]\to \R</m> satisfying <m>f(0)=f(1)=0</m>, with inner product <me>\ip{f,g} = \definite f(x) g(x) dx01.</me> The linear map <m>L:V\to V</m> defined by <m>Lf = f''</m> is self-adjoint as proven on homework.
			</p><!--</div attr= class="example">-->

			<p>
				One of the nicest things about self-adjoint maps is that they are always diagonalizable—there always exists an basis of eigenvectors. Even better, the eigenvectors can be chosen to be orthogonal.
			</p>

<!-- div attr= class="theorem"-->
			<p>
				 If <m>T</m> is a self-adjoint map on a finite dimensional inner product space <m>V</m> over <m>\C</m>, then there is an orthonormal basis for <m>V</m> consisting of eigenvectors of <m>T</m>.
			</p><!--</div attr= class="theorem">-->

			<p>
				Next we need a lemma that extends allows us to do induction on the dimension.
			</p>

<!-- div attr= class="lemma"-->
			<p>
				 If <m>V</m> is an inner product space, <m>T\in L(V,V)</m> is self-adjoint, and <m>W\of V</m> is a subspace that is <m>T</m>-invariant (meaning that <m>T\w\in W</m> for all <m>\w\in W</m>), then <m>W\perp</m> is also <m>T</m>-invariant.
			</p><!--</div attr= class="lemma">-->

<!-- div attr= class="proof"-->
			<p>
				Suppose <m>v\in W\perp</m>. To show <m>T\v\in W\perp</m> we need to show that <m>\ip{T\v,\w}=0</m> for all <m>\w\in W</m>. But <me>\ip{T\v,\w} = \ip{\v,T\w} = 0,</me> since <m>\v\in W\perp</m> and <m>T\w\in W</m>.
			</p><!--</div attr= class="proof">-->

			<p>
				We now prove <xref ref="thm:spectral" />, using induction on <m>\dim(V)</m>.
			</p>

<!-- div attr= class="proof"-->
			<p>
				<em>Proof of <xref ref="thm:spectral" />.</em> If <m>\dim(V)\le 1</m> the result is trivial, so suppose <m>\dim(V)&gt;1</m> and <m>\v</m> is an eigenvector of <m>T</m> with eigenvalue <m>\l</m> (guaranteed by ). Set <m>W = \linspan(\v)</m>. By <xref ref="lem:selfadjointWperp" /> we know <m>T:W\perp\to W\perp</m>, so by induction there is an orthonormal basis of <m>W\perp</m> consisting of eigenvectors of <m>T</m>. Adjoining <m>\v/\norm{\v}</m> to this ONB of <m>W\perp</m> we get an orthonormal basis for <m>V</m> consisting of eigenvectors of <m>T</m>.
			</p><!--</div attr= class="proof">-->

        </section>

      </chapter>
      
    </part>

    <!--
    <part xml:id="part-theory">
      <title>Theory of Linear Transformations</title>
   

      <!-
      ********************************
      Theory of Linear Transformations
      ********************************
      ->

      <chapter xml:id="ch-determinants">
        <title>Determinants: Revisited</title>
        
        <p>Coming soon to an OER near you!</p>

      </chapter>

      <chapter xml:id="ch-inner-product-spaces">
        <title>Inner Product Spaces</title>
        
        <p>Coming soon to an OER near you!</p>

      </chapter>

      <chapter xml:id="ch-change-of-bases">
        <title>Change of Bases</title>
        
        <p>Coming soon to an OER near you!</p>

      </chapter>

      <chapter xml:id="ch-spectral-theorem">
        <title>The Spectral Theorem</title>
        
        <p>Coming soon to an OER near you!</p>

      </chapter>

      <chapter xml:id="ch-theory-extras">
        <title>Extras</title>

        <section xml:id="sec-modlues">
          <title>Modules</title>

          <p>Coming soon to an OER near you!</p>
          
        </section>
        
      </chapter>
      
    </part>
    -->

    <backmatter xml:id="backmatter">
      <title>Backmatter</title>

      <appendix>
        <title>Foundational Knowledge</title>

        <section xml:id="sec-sets-functions">
          <title>Sets, Functions, Constructions</title>

          <subsection xml:id="subsec-sets">
            <title>Sets</title>
            
            <p>Coming soon to an OER near you!</p>          </subsection>

          <subsection xml:id="subsec-functions">
            <title>Functions</title>
            
            <p>Coming soon to an OER near you!</p>          </subsection>

          <subsection xml:id="subsec-set-constructions">
            <title>Set Constructions</title>

            <subsubsection xml:id="subsubsec-subsets">
              <title>Subsets</title>

              <p>Coming soon to an OER near you!</p>            </subsubsection>

            <subsubsection xml:id="subsubsec-product-sets">
              <title>Product Sets</title>
              
              <p>Coming soon to an OER near you!</p>            </subsubsection>

            <subsubsection xml:id="subsubsec-quotient-sets">
              <title>Quotient Sets</title>
              
              <p>Coming soon to an OER near you!</p>            </subsubsection>
            

          </subsection>          
        </section>

        <section xml:id="sec-groups-rings-fields">
          <title>Groups, Rings, Fields</title>

          <p>hi</p>
          
        </section>


      </appendix>

      <appendix>
        <title>Notation</title>

        <notation-list/>
      </appendix>

      <appendix xml:id="appendix-list-definitions">
        <title>List of Definitions</title>
      
        <list-of elements="definition" divisions="chapter" empty="yes"/>
      </appendix>

      <appendix xml:id="appendix-list-results">
        <title>List of Results</title>
      
        <list-of elements="theorem lemma proposition corollary" divisions="chapter" empty="yes"/>
      </appendix>

      <colophon>
        <p> This book was authored in <pretext />. </p>
      </colophon>

      

    </backmatter>

  </book>

</pretext>