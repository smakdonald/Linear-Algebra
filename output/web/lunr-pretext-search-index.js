var ptx_lunr_search_style = "textbook";
var ptx_lunr_docs = [
{
  "id": "colophon-1",
  "level": "1",
  "url": "colophon-1.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": "   smakdonald.github   https:\/\/smakdonald.github.io\/index.html   copyright  "
},
{
  "id": "sec-linear-systems-matrix-equations",
  "level": "1",
  "url": "sec-linear-systems-matrix-equations.html",
  "type": "Section",
  "number": "1.1",
  "title": "Linear Systems and Matrix Equations",
  "body": " Linear Systems and Matrix Equations           Matrix Basics  Coming soon to an OER near you!    Encoding Linear Systems  Coming soon to an OER near you!     Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-1",
  "level": "2",
  "url": "sec-linear-systems-matrix-equations.html#exercise-1",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-2",
  "level": "2",
  "url": "sec-linear-systems-matrix-equations.html#exercise-2",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-matrix-arithmetic",
  "level": "1",
  "url": "sec-matrix-arithmetic.html",
  "type": "Section",
  "number": "1.2",
  "title": "Matrix Arithmetic",
  "body": " Matrix Arithmetic           Addition and Scalar Multiplication  Coming soon to an OER near you!    Matrix Multiplication  Coming soon to an OER near you!    Invertibility of Square Matrices  Coming soon to an OER near you!     Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-3",
  "level": "2",
  "url": "sec-matrix-arithmetic.html#exercise-3",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-4",
  "level": "2",
  "url": "sec-matrix-arithmetic.html#exercise-4",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-solving-linear-systems",
  "level": "1",
  "url": "sec-solving-linear-systems.html",
  "type": "Section",
  "number": "1.3",
  "title": "Solving Linear Systems",
  "body": " Solving Linear Systems           Gaussian Elimination  Coming soon to an OER near you!    Solving Homogeneous Linear Systems  Coming soon to an OER near you!    Solving Inhomogeneous Linear Systems  Coming soon to an OER near you!     Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-5",
  "level": "2",
  "url": "sec-solving-linear-systems.html#exercise-5",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-6",
  "level": "2",
  "url": "sec-solving-linear-systems.html#exercise-6",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-computing-dets",
  "level": "1",
  "url": "sec-computing-dets.html",
  "type": "Section",
  "number": "2.1",
  "title": "Computing Determinants",
  "body": " Computing Determinants          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-7",
  "level": "2",
  "url": "sec-computing-dets.html#exercise-7",
  "type": "Exercise",
  "number": "2.1.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-8",
  "level": "2",
  "url": "sec-computing-dets.html#exercise-8",
  "type": "Exercise",
  "number": "2.1.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-det-properties",
  "level": "1",
  "url": "sec-det-properties.html",
  "type": "Section",
  "number": "2.2",
  "title": "Properties of Determinants",
  "body": " Properties of Determinants          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-9",
  "level": "2",
  "url": "sec-det-properties.html#exercise-9",
  "type": "Exercise",
  "number": "2.2.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-10",
  "level": "2",
  "url": "sec-det-properties.html#exercise-10",
  "type": "Exercise",
  "number": "2.2.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-vector-space-basics",
  "level": "1",
  "url": "sec-vector-space-basics.html",
  "type": "Section",
  "number": "3.1",
  "title": "Vector Space Basics",
  "body": " Vector Space Basics          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-11",
  "level": "2",
  "url": "sec-vector-space-basics.html#exercise-11",
  "type": "Exercise",
  "number": "3.1.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-12",
  "level": "2",
  "url": "sec-vector-space-basics.html#exercise-12",
  "type": "Exercise",
  "number": "3.1.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-subspaces",
  "level": "1",
  "url": "sec-subspaces.html",
  "type": "Section",
  "number": "3.2",
  "title": "Subspaces, Sums, and Direct Products",
  "body": " Subspaces, Sums, and Direct Products          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-13",
  "level": "2",
  "url": "sec-subspaces.html#exercise-13",
  "type": "Exercise",
  "number": "3.2.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-14",
  "level": "2",
  "url": "sec-subspaces.html#exercise-14",
  "type": "Exercise",
  "number": "3.2.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-span",
  "level": "1",
  "url": "sec-span.html",
  "type": "Section",
  "number": "4.1",
  "title": "Span",
  "body": " Span          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-15",
  "level": "2",
  "url": "sec-span.html#exercise-15",
  "type": "Exercise",
  "number": "4.1.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-16",
  "level": "2",
  "url": "sec-span.html#exercise-16",
  "type": "Exercise",
  "number": "4.1.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "linear-independence",
  "level": "1",
  "url": "linear-independence.html",
  "type": "Section",
  "number": "4.2",
  "title": "Linear Independence",
  "body": " Linear Independence          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-17",
  "level": "2",
  "url": "linear-independence.html#exercise-17",
  "type": "Exercise",
  "number": "4.2.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-18",
  "level": "2",
  "url": "linear-independence.html#exercise-18",
  "type": "Exercise",
  "number": "4.2.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "basis-basics",
  "level": "1",
  "url": "basis-basics.html",
  "type": "Section",
  "number": "4.3",
  "title": "Basis Basics",
  "body": " Basis Basics          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-19",
  "level": "2",
  "url": "basis-basics.html#exercise-19",
  "type": "Exercise",
  "number": "4.3.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-20",
  "level": "2",
  "url": "basis-basics.html#exercise-20",
  "type": "Exercise",
  "number": "4.3.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "vs-dim",
  "level": "1",
  "url": "vs-dim.html",
  "type": "Section",
  "number": "4.4",
  "title": "Dimension of Vector Spaces",
  "body": " Dimension of Vector Spaces          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-21",
  "level": "2",
  "url": "vs-dim.html#exercise-21",
  "type": "Exercise",
  "number": "4.4.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-22",
  "level": "2",
  "url": "vs-dim.html#exercise-22",
  "type": "Exercise",
  "number": "4.4.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-linear-map-basics",
  "level": "1",
  "url": "sec-linear-map-basics.html",
  "type": "Section",
  "number": "5.1",
  "title": "Linear Map Basics",
  "body": " Linear Map Basics          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-23",
  "level": "2",
  "url": "sec-linear-map-basics.html#exercise-23",
  "type": "Exercise",
  "number": "5.1.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-24",
  "level": "2",
  "url": "sec-linear-map-basics.html#exercise-24",
  "type": "Exercise",
  "number": "5.1.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "nullspace-and-range",
  "level": "1",
  "url": "nullspace-and-range.html",
  "type": "Section",
  "number": "5.2",
  "title": "Nullspace and Range",
  "body": " Nullspace and Range           Nullspace  Coming soon to an OER near you!    Range  Coming soon to an OER near you!     Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-25",
  "level": "2",
  "url": "nullspace-and-range.html#exercise-25",
  "type": "Exercise",
  "number": "1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-26",
  "level": "2",
  "url": "nullspace-and-range.html#exercise-26",
  "type": "Exercise",
  "number": "2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "rank-nullity",
  "level": "1",
  "url": "rank-nullity.html",
  "type": "Section",
  "number": "5.3",
  "title": "The Rank-Nullity Theorem",
  "body": " The Rank-Nullity Theorem          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-27",
  "level": "2",
  "url": "rank-nullity.html#exercise-27",
  "type": "Exercise",
  "number": "5.3.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-28",
  "level": "2",
  "url": "rank-nullity.html#exercise-28",
  "type": "Exercise",
  "number": "5.3.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-eigen",
  "level": "1",
  "url": "sec-eigen.html",
  "type": "Section",
  "number": "6.1",
  "title": "Eigenvectors and Eigenvalues",
  "body": " Eigenvectors and Eigenvalues          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-29",
  "level": "2",
  "url": "sec-eigen.html#exercise-29",
  "type": "Exercise",
  "number": "6.1.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-30",
  "level": "2",
  "url": "sec-eigen.html#exercise-30",
  "type": "Exercise",
  "number": "6.1.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-charpoly",
  "level": "1",
  "url": "sec-charpoly.html",
  "type": "Section",
  "number": "6.2",
  "title": "The Characteristic Polynomial",
  "body": " The Characteristic Polynomial          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-31",
  "level": "2",
  "url": "sec-charpoly.html#exercise-31",
  "type": "Exercise",
  "number": "6.2.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-32",
  "level": "2",
  "url": "sec-charpoly.html#exercise-32",
  "type": "Exercise",
  "number": "6.2.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-diagonal-matrices",
  "level": "1",
  "url": "sec-diagonal-matrices.html",
  "type": "Section",
  "number": "6.3",
  "title": "Diagonal Matrices",
  "body": " Diagonal Matrices          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-33",
  "level": "2",
  "url": "sec-diagonal-matrices.html#exercise-33",
  "type": "Exercise",
  "number": "6.3.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-34",
  "level": "2",
  "url": "sec-diagonal-matrices.html#exercise-34",
  "type": "Exercise",
  "number": "6.3.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-inner-products",
  "level": "1",
  "url": "sec-inner-products.html",
  "type": "Section",
  "number": "7.1",
  "title": "Inner Products",
  "body": " Inner Products          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-35",
  "level": "2",
  "url": "sec-inner-products.html#exercise-35",
  "type": "Exercise",
  "number": "7.1.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-36",
  "level": "2",
  "url": "sec-inner-products.html#exercise-36",
  "type": "Exercise",
  "number": "7.1.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-orthogonal-sets-projections",
  "level": "1",
  "url": "sec-orthogonal-sets-projections.html",
  "type": "Section",
  "number": "7.2",
  "title": "Orthogonal Sets and Projections",
  "body": " Orthogonal Sets and Projections          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-37",
  "level": "2",
  "url": "sec-orthogonal-sets-projections.html#exercise-37",
  "type": "Exercise",
  "number": "7.2.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-38",
  "level": "2",
  "url": "sec-orthogonal-sets-projections.html#exercise-38",
  "type": "Exercise",
  "number": "7.2.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-gram-shcmidt-least-squares",
  "level": "1",
  "url": "sec-gram-shcmidt-least-squares.html",
  "type": "Section",
  "number": "7.3",
  "title": "Gram Schmidt and Least Squares",
  "body": " Gram Schmidt and Least Squares          Computations       Coming soon to an OER near you!   Coming soon to an OER near you!     Formal Proofs       Coming soon to an OER near you!   Coming soon to an OER near you!     "
},
{
  "id": "exercise-39",
  "level": "2",
  "url": "sec-gram-shcmidt-least-squares.html#exercise-39",
  "type": "Exercise",
  "number": "7.3.1",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "exercise-40",
  "level": "2",
  "url": "sec-gram-shcmidt-least-squares.html#exercise-40",
  "type": "Exercise",
  "number": "7.3.2",
  "title": "",
  "body": " Coming soon to an OER near you!   Coming soon to an OER near you!  "
},
{
  "id": "sec-algebra",
  "level": "1",
  "url": "sec-algebra.html",
  "type": "Section",
  "number": "1.1",
  "title": "Algebra",
  "body": " Algebra  One of the first examples of vectors people encountered were displacement vectors , or as we'll call them, steps . A step in the plane is the process of moving a certain distance in a certain direction. For instance is the action of moving units right and unit down. So, using a bit of trigonometry, we see that the step moves you a distance in the direction in polar coordinates. (See )  One important thing to notice is that the step might occur when moving from the point to the point , or when moving from the point to the point , or many other possibilities. An apology is necessary here for using the same notation for steps and for points in the plane. It is (as mathematicians have discovered from long experience) both a blessing and a curse to do this. In other words, steps don't have a fixed position, but do have a length and a direction.  The effect of doing the step and then the step is identical with the effect of doing the step . Moreover, this operation feels a lot like adding the steps together. Similarly, adding to itself gives us a step that we very naturally think of as twice , or . It seems natural to think of this operation as that of multiplying the step by the number . (See )  These definitions are clearly natural geometrically—it is a very pleasant fact that when we write down the algebraic version of these definitions they are also very natural.    We consider the set of all steps in the plane, thought of as ordered pairs of real numbers: Given steps and in and a number we define     This is the first appearance of algebra in linear algebra. We define algebraic operations—addition of steps and multiplication of steps by numbers—and think about the way that these operations behave and interact. The following definition is a central one.   If and are steps in and then we say that is a linear combination of and .     More generally if we have steps and numbers then we get a linear combination     From here on out we will switch to using the more conventional name of vector for these steps, but you should be warned that we will end up using vector to refer to many different things. If we can take linear combinations of some objects, we'll call them vectors.  "
},
{
  "id": "def-step",
  "level": "2",
  "url": "sec-algebra.html#def-step",
  "type": "Definition",
  "number": "1.1",
  "title": "",
  "body": "  We consider the set of all steps in the plane, thought of as ordered pairs of real numbers: Given steps and in and a number we define    "
},
{
  "id": "example-1",
  "level": "2",
  "url": "sec-algebra.html#example-1",
  "type": "Example",
  "number": "1.2",
  "title": "",
  "body": " If and are steps in and then we say that is a linear combination of and .  "
},
{
  "id": "def-vector-ish",
  "level": "2",
  "url": "sec-algebra.html#def-vector-ish",
  "type": "Definition",
  "number": "1.3",
  "title": "",
  "body": "  More generally if we have steps and numbers then we get a linear combination    "
},
{
  "id": "sec-geometry",
  "level": "1",
  "url": "sec-geometry.html",
  "type": "Section",
  "number": "1.2",
  "title": "Geometry",
  "body": " Geometry   Let's think about one of the most basic geometric objects there is: a line in the plane. There are two natural ways to specify a line. The description that probably first comes to mind is that of the solution set of a linear equation. The line has slope and goes through ; the line is horizontal and goes through ; and the line is vertical, going through . (See .)  Because of the existence of the third example, it is more consistent to think of the general description of a line as the solution set of where , and are constants (with not both and equalling ). Though this description is very familiar, it's not very geometric .    Why is the set of solutions of the equation a line?    If satisfy we can write and so tells us the point can be reached by taking the step after which we move in the direction of the step either forward of backward times the length of .      Dot Products  As we mentioned at the start, linear algebra connects algebra and geometry, so we'd like to make a connection between these two descriptions. Let's start by thinking about the vertical line through . This is all the points whose coordinate is , or, in other words, all the points whose distance in the direction of the -axis is . Similarly the horizontal line through consists of all the points in the plane whose vertical distance from the origin is . Let's try to generalize this notion of distance along an axis .  Looking at we see that if we pick an axis the measure of distance along that axis is additive : the answer for the sum of two vectors is the sum of the answers for each individually. In other words, letting be the axis, we have It's more obvious that if we scale a vector by a factor of say, then the distance along the axis scales by the same factor. So, if is any real number, we have Again, using a bit of trigonometry, we have that the length of in the direction of is just the length of times , where is the (non-obtuse) angle between and .  Now we have a second question. How do we specify the direction of the axis ? Having spent so much time thinking about vectors, let's do it according to a vector. Let's pick a vector that's in the direction of this axis. Many vectors specify this same direction: if works then so does , or .  If we make things a little more symmetric, as mathematicians are prone to do, everything becomes much nicer, and a calculation that was, all along, geometry, geometry, geometry, suddenly becomes algebra. Let's remind ourselves how. If we write for the length of , recall that the distance of along the axis is .    We define the dot product of steps and to be           the dot product is symmetric, i.e., .     and are perpendicular if and only if            (1) and (2) can be seen from the definition. The third property follows is because we are simply scaling the lengths of and along the axis by a factor of . But since it's additive in and symmetric, the dot product must also be additive in as claimed in (4).  Why does this help? Because we can split our calculation into small calculations that are trivial to do. Let's pick two vectors of length , one along the -axis and one along the -axis. The traditional names for these vectors are and . Now suppose that and . Then, using additivity, we have Every term in this final expression is easy to work out geometrically. All lengths are , all angles are either or . We have, for instance . Similarly, Altogether we get .  To summarize, we have the following definition, that neatly encapsulates both geometry and algebra.      If and are vectors in then we define their dot product to be where is the angle between and and denote the length of and of .     "
},
{
  "id": "question-1",
  "level": "2",
  "url": "sec-geometry.html#question-1",
  "type": "Question",
  "number": "1.4",
  "title": "",
  "body": "  Why is the set of solutions of the equation a line?    If satisfy we can write and so tells us the point can be reached by taking the step after which we move in the direction of the step either forward of backward times the length of .   "
},
{
  "id": "def-dot-product",
  "level": "2",
  "url": "sec-geometry.html#def-dot-product",
  "type": "Definition",
  "number": "1.5",
  "title": "",
  "body": "  We define the dot product of steps and to be    "
},
{
  "id": "prop-dot-product-properties",
  "level": "2",
  "url": "sec-geometry.html#prop-dot-product-properties",
  "type": "Proposition",
  "number": "1.6",
  "title": "",
  "body": "    the dot product is symmetric, i.e., .     and are perpendicular if and only if            (1) and (2) can be seen from the definition. The third property follows is because we are simply scaling the lengths of and along the axis by a factor of . But since it's additive in and symmetric, the dot product must also be additive in as claimed in (4).  Why does this help? Because we can split our calculation into small calculations that are trivial to do. Let's pick two vectors of length , one along the -axis and one along the -axis. The traditional names for these vectors are and . Now suppose that and . Then, using additivity, we have Every term in this final expression is easy to work out geometrically. All lengths are , all angles are either or . We have, for instance . Similarly, Altogether we get .  To summarize, we have the following definition, that neatly encapsulates both geometry and algebra.   "
},
{
  "id": "def-dot-product-2",
  "level": "2",
  "url": "sec-geometry.html#def-dot-product-2",
  "type": "Definition",
  "number": "1.7",
  "title": "",
  "body": "  If and are vectors in then we define their dot product to be where is the angle between and and denote the length of and of .   "
},
{
  "id": "sec-computation",
  "level": "1",
  "url": "sec-computation.html",
  "type": "Section",
  "number": "1.3",
  "title": "Computation",
  "body": " Computation  One of the primary focuses of your first linear algebra course was, almost certainly, the solution of systems of linear equations, using techniques such as row reduction and back substitution. We won't recap these algorithms, though there are exercises designed to refresh your memory about the basic issues. Instead we want to present here a broader perspective on what was going on when you were solving such problems.  Let's consider a representative system of linear equations.     The standard approach to solving such a system, called Gaussian elimination, involves adding or subtracting the equations from one another, possible multiplying some by constants. For instance, we might start the solution process for this system by subtracting the first equation from the second (with the intention of eliminating from the second equation): We eliminate from the third equation also, by computing and we have made progress on solving our system by deducing that any solution to the original system must also satisfy We continue the process by computing and thereby deduce that . We could demonstrate this fact quickly by just writing down the linear combination of our equations We certainly don't advocate this as a sensible way to write down the manual solution of systems of linear equations. However, it is clear that what we are doing in Gaussian elimination is taking linear combinations of things—in this case the things are themselves linear equations. This is the first of many examples where we want to form linear combinations of things that aren't just vectors in two or three dimensions. This leads us to the notion of a vector space .  "
},
{
  "id": "sec-vector-spaces",
  "level": "1",
  "url": "sec-vector-spaces.html",
  "type": "Section",
  "number": "1.4",
  "title": "Vector Spaces",
  "body": " Vector Spaces  One of the two core elements of linear algebra is the notion of a vector space . We'll discuss it informally here and postpone a formal presentation. A vector space is a collection of objects which one can add and multiply by scalars - therefore one can also take linear combinations. The elements of such a collection will be called vectors .   The set is the set of all steps in the plane. We have seen how to take linear combinations of steps.    Similarly we can talk about vectors with more then two entries: for instance, but also , , …, etc . Formally we have and we define where is a real number.    We can go even further and think about sequences of infinite length, such as those that appear in calculus and analysis. We can add and scale such things in the natural way:     A polynomial with real coefficients is an expression of the form , where . The set of all polynomials with real coefficients is denoted a vector space. We already know how to add polynomials and multiply them by real numbers. If and are polynomials with real coefficients and then we have new polynomials and with   We also know how to multiply two polynomials together. Eventually we'll talk about gadgets which are vector spaces and for which we can also multiply vectors together. Such things will be called algebras . For the moment though we're only concerned with scaling vectors—multiplying them by numbers.    If we define to be the set of all matrices with real entries then this forms a vector space. We add matrices of the same dimensions by Similarly, we scale them by     Going even further than , we can consider vector spaces where the things we take linear combinations of (the vectors ) are other functions. For example We know that linear combinations of continuous functions are continuous, so this is a vector space.   "
},
{
  "id": "example-2",
  "level": "2",
  "url": "sec-vector-spaces.html#example-2",
  "type": "Example",
  "number": "1.8",
  "title": "",
  "body": " The set is the set of all steps in the plane. We have seen how to take linear combinations of steps.  "
},
{
  "id": "example-3",
  "level": "2",
  "url": "sec-vector-spaces.html#example-3",
  "type": "Example",
  "number": "1.9",
  "title": "",
  "body": " Similarly we can talk about vectors with more then two entries: for instance, but also , , …, etc . Formally we have and we define where is a real number.  "
},
{
  "id": "example-4",
  "level": "2",
  "url": "sec-vector-spaces.html#example-4",
  "type": "Example",
  "number": "1.10",
  "title": "",
  "body": " We can go even further and think about sequences of infinite length, such as those that appear in calculus and analysis. We can add and scale such things in the natural way:   "
},
{
  "id": "example-5",
  "level": "2",
  "url": "sec-vector-spaces.html#example-5",
  "type": "Example",
  "number": "1.11",
  "title": "",
  "body": " A polynomial with real coefficients is an expression of the form , where . The set of all polynomials with real coefficients is denoted a vector space. We already know how to add polynomials and multiply them by real numbers. If and are polynomials with real coefficients and then we have new polynomials and with   We also know how to multiply two polynomials together. Eventually we'll talk about gadgets which are vector spaces and for which we can also multiply vectors together. Such things will be called algebras . For the moment though we're only concerned with scaling vectors—multiplying them by numbers.  "
},
{
  "id": "example-6",
  "level": "2",
  "url": "sec-vector-spaces.html#example-6",
  "type": "Example",
  "number": "1.12",
  "title": "",
  "body": " If we define to be the set of all matrices with real entries then this forms a vector space. We add matrices of the same dimensions by Similarly, we scale them by   "
},
{
  "id": "example-7",
  "level": "2",
  "url": "sec-vector-spaces.html#example-7",
  "type": "Example",
  "number": "1.13",
  "title": "",
  "body": " Going even further than , we can consider vector spaces where the things we take linear combinations of (the vectors ) are other functions. For example We know that linear combinations of continuous functions are continuous, so this is a vector space.  "
},
{
  "id": "sec-linear-maps",
  "level": "1",
  "url": "sec-linear-maps.html",
  "type": "Section",
  "number": "1.5",
  "title": "Linear Maps",
  "body": " Linear Maps  There's one more crucial ingredient to linear algebra that we need to introduce at this point. Geometry isn't just about points and lines and planes—it's also about operations on such things, such as rotations and reflections. The core of linear algebra at this level is the study of vector spaces and linear maps . These are functions between vector spaces that respect addition and scaling of vectors. Let's see some examples.   Let's take our first example of adding vectors and apply a rotation to each one. In other words let's define a map that rotates any step by (in this case) . The diagram demonstrating that rotates together to give a demonstration that     In (brief) summary: the rotation of a sum is the sum of the rotations. It should be clear that the rotation of a scaling of is simply the scaling of the rotation.    Now let's apply a reflection through the red line in the diagram (which happens to be the line ). We define that reflects any step in this line. The diagram demonstrating that reflects together to give a demonstration that   The reflection of a sum is the sum of the reflections. Scaling also works.    Given a line , the map given by the projection of onto the direction of is a linear map. According to this map is additive, that is, and it is not hard to see that also holds.   We extract the essence of these three examples into the following definition    Given two vectors spaces and (over ) we say that a function is a linear map if for all and we have      Consider the vector space of polynomials in a variable with real coefficients (see ). Consider the map defined by . In other words maps every polynomial to its derivative. This is a linear map. That simply means that if and then In other words the derivative of the sum of polynomials is the sum of the individual derivatives, and the derivative of a constant multiple of a polynomial is that same multiple of the derivative. These standard facts (that apply not only to polynomials but to other differentiable functions) are precisely the condition for to be a linear map.    Consider the matrix given by Define a function by Standard facts about matrix multiplication tell us that this is a linear map: Not only that, but this linear map can be viewed as being constructed using linear combinations. We have The result of computing is the linear combination, using coefficients , and , of the columns of .   "
},
{
  "id": "example-8",
  "level": "2",
  "url": "sec-linear-maps.html#example-8",
  "type": "Example",
  "number": "1.14",
  "title": "",
  "body": " Let's take our first example of adding vectors and apply a rotation to each one. In other words let's define a map that rotates any step by (in this case) . The diagram demonstrating that rotates together to give a demonstration that   "
},
{
  "id": "example-9",
  "level": "2",
  "url": "sec-linear-maps.html#example-9",
  "type": "Example",
  "number": "1.15",
  "title": "",
  "body": " In (brief) summary: the rotation of a sum is the sum of the rotations. It should be clear that the rotation of a scaling of is simply the scaling of the rotation.  "
},
{
  "id": "example-10",
  "level": "2",
  "url": "sec-linear-maps.html#example-10",
  "type": "Example",
  "number": "1.16",
  "title": "",
  "body": " Now let's apply a reflection through the red line in the diagram (which happens to be the line ). We define that reflects any step in this line. The diagram demonstrating that reflects together to give a demonstration that   The reflection of a sum is the sum of the reflections. Scaling also works.  "
},
{
  "id": "example-11",
  "level": "2",
  "url": "sec-linear-maps.html#example-11",
  "type": "Example",
  "number": "1.17",
  "title": "",
  "body": " Given a line , the map given by the projection of onto the direction of is a linear map. According to this map is additive, that is, and it is not hard to see that also holds.  "
},
{
  "id": "def-linear-map",
  "level": "2",
  "url": "sec-linear-maps.html#def-linear-map",
  "type": "Definition",
  "number": "1.18",
  "title": "",
  "body": "  Given two vectors spaces and (over ) we say that a function is a linear map if for all and we have    "
},
{
  "id": "example-12",
  "level": "2",
  "url": "sec-linear-maps.html#example-12",
  "type": "Example",
  "number": "1.19",
  "title": "",
  "body": " Consider the vector space of polynomials in a variable with real coefficients (see ). Consider the map defined by . In other words maps every polynomial to its derivative. This is a linear map. That simply means that if and then In other words the derivative of the sum of polynomials is the sum of the individual derivatives, and the derivative of a constant multiple of a polynomial is that same multiple of the derivative. These standard facts (that apply not only to polynomials but to other differentiable functions) are precisely the condition for to be a linear map.  "
},
{
  "id": "example-13",
  "level": "2",
  "url": "sec-linear-maps.html#example-13",
  "type": "Example",
  "number": "1.20",
  "title": "",
  "body": " Consider the matrix given by Define a function by Standard facts about matrix multiplication tell us that this is a linear map: Not only that, but this linear map can be viewed as being constructed using linear combinations. We have The result of computing is the linear combination, using coefficients , and , of the columns of .  "
},
{
  "id": "sec-tying-together",
  "level": "1",
  "url": "sec-tying-together.html",
  "type": "Section",
  "number": "1.6",
  "title": "Tying Things Together",
  "body": " Tying Things Together   The previous sections discussed issues that had connections and commonalities between them. In this final section we'll think about from the higher vantage point permitted by our other discussions.  We considered the following system of linear equations give us two very important perspectives on this linear system.    Perspective 1: Intersection of Three Planes     In we alluded to the fact that the solutions to a linear equation in form a plane. In we'll see why this is so. Once we know that each equation defines a plane in , we see the set of solutions to the system is the set of all points lying on all three planes, that is, the intersection of the three planes. We anticipate that unless there is something special about these planes (such as two of them being parallel) the three planes should have exactly one common point.    The set of solutions to the equation , where are not all zero, form a plane in 3-space.    Note that the equation can be written using dot product as Let's consider a particular solution to the above equation. For example, if I could pick and similarly if and . Then we have and subtracting this from the above equation gives This means is perpendicular to (or 0). Thinking of as a step with head at and tail at we see it is perpendicular to . There is a unique plane that passes through the point and is perpendicular to and we have just seen that is a solution if and only if it lies in this plane.      Perspective 2: Linear Combinations of Vectors Range of a Transformation     We can rewrite the system as Thus the system is asking which coefficients we can use in a linear combination of three vectors on the left hand side to obtain the vector on the right hand side.  This in turn, as we saw in , can be rephrase in the language of linear transformations. Using the function defined in we can write the system as This can be rephrased as understanding whether the vector on the right hand side is a value of (we will soon phrase this as whether the vector on the right hand side is in the range of ) and if so, which inputs give this value. In other words we are looking for the preimage of the vector on the right hand side with respect to .  To summarize, if we consider the coefficient matrix of the system the perspective of planes consider the dot products of with the rows of , while the perspective of linear combinations considers linear combinations of the columns of with undetermined coefficients. We will see throughout that being able to see such things from alternative viewpoints allows us a much more interesting view of the world of linear algebra around us.   "
},
{
  "id": "p-202",
  "level": "2",
  "url": "sec-tying-together.html#p-202",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": ""
},
{
  "id": "proposition-2",
  "level": "2",
  "url": "sec-tying-together.html#proposition-2",
  "type": "Proposition",
  "number": "1.21",
  "title": "",
  "body": "  The set of solutions to the equation , where are not all zero, form a plane in 3-space.    Note that the equation can be written using dot product as Let's consider a particular solution to the above equation. For example, if I could pick and similarly if and . Then we have and subtracting this from the above equation gives This means is perpendicular to (or 0). Thinking of as a step with head at and tail at we see it is perpendicular to . There is a unique plane that passes through the point and is perpendicular to and we have just seen that is a solution if and only if it lies in this plane.   "
},
{
  "id": "p-206",
  "level": "2",
  "url": "sec-tying-together.html#p-206",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": ""
},
{
  "id": "sec-fields",
  "level": "1",
  "url": "sec-fields.html",
  "type": "Section",
  "number": "2.1",
  "title": "Fields",
  "body": " Fields  We start by discussing where our scalars can come from. Scalars are the numbers that we use to scale vectors. So far we have used only real numbers for our scalars. But there are other possibilities.    A field is a collection of objects (that we think of as numbers) that satisfy the same algebraic properties as with respect to multiplication and addition.  Here then is an explicit list of what we require in order to say that a collection of numbers, equipped with definitions of and , is a field:  A field is a set (that we think of as numbers) together with definitions of and (in other words, together with functions ) such all the following properties hold:    Addition is commutative: for all we have     Addition is associative: for all we have     There is an additive identity: there is a special number such that for all we have     There are additive inverses: for all there is an element that we denote such that .      Multiplication is commutative: for all we have     Multiplication is associative: for all we have     There is a multiplicative identity: there is a special number such that for all we have     There are multiplicative inverse for non-zero numbers: for all there is an element that we denote such that     If you have seen such things before (and do not worry if you haven't) we're just stating that and are Abelian groups.    Multiplication distributes over addition: for all we have .            The fact that the operations of addition and multiplication are functions implicity gives the following additional properties:    Closure under addition: for all we have .    Closure under addition: for all we have .    The most important examples of fields are and but there are others that we will seriously consider.    The complex numbers are the numbers of the form with addition and multiplication defined by Note that by our definition of multiplication   We can think of as a subset of by identifying with . This makes as subfield of . A subfield is a subset of a field which is itself a field with respect to the operations on the larger set.    The rational numbers are a field, indeed they are a subfield of (simply meaning that and the field operations for are the same whether you think of elements of as rational numbers or real numbers). This fact means that conditions A1-A2, M1-M2, and C1 are trivially satisfied, since they're true for real numbers, not just rationals. Since the special real numbers are in fact in we also get A3, M3, and C2 for free. This only leaves A4 and M4; given we know that there are real numbers and such that and , but what we need to know is that there are rational numbers with these properties Of course if we don't need to find a multiplicative inverse for . . Fortunately it is that case that if is rational then so are and . Thus is a field.    The field with two elements , , has as its only elements the two residue classes modulo , with addition and multiplication defined modulo . To be specific, we have the following addition and multiplication tables. There are in fact many interesting examples of vector spaces where we use for our numbers. For instance many schemes of data transmission use the ideas of linear algebra to build error correction into digital communication.     We make some standard abbreviations when computing in fields. We write We also define, for ,      Above we have defined elements in any field . In we have , and in we have .  You might think it would be confusing that is simultaneously the name of an integer and a field element, particularly in fields or where , but context resolves all such confusions, usually without anyone noticing.  The following rules of arithmetic hold in any field.     If is a field an then for For all we have     "
},
{
  "id": "def-field",
  "level": "2",
  "url": "sec-fields.html#def-field",
  "type": "Definition",
  "number": "2.1",
  "title": "",
  "body": "  A field is a collection of objects (that we think of as numbers) that satisfy the same algebraic properties as with respect to multiplication and addition.  Here then is an explicit list of what we require in order to say that a collection of numbers, equipped with definitions of and , is a field:  A field is a set (that we think of as numbers) together with definitions of and (in other words, together with functions ) such all the following properties hold:    Addition is commutative: for all we have     Addition is associative: for all we have     There is an additive identity: there is a special number such that for all we have     There are additive inverses: for all there is an element that we denote such that .      Multiplication is commutative: for all we have     Multiplication is associative: for all we have     There is a multiplicative identity: there is a special number such that for all we have     There are multiplicative inverse for non-zero numbers: for all there is an element that we denote such that     If you have seen such things before (and do not worry if you haven't) we're just stating that and are Abelian groups.    Multiplication distributes over addition: for all we have .          "
},
{
  "id": "remark-1",
  "level": "2",
  "url": "sec-fields.html#remark-1",
  "type": "Remark",
  "number": "2.2",
  "title": "",
  "body": " The fact that the operations of addition and multiplication are functions implicity gives the following additional properties:    Closure under addition: for all we have .    Closure under addition: for all we have .    The most important examples of fields are and but there are others that we will seriously consider.  "
},
{
  "id": "example-14",
  "level": "2",
  "url": "sec-fields.html#example-14",
  "type": "Example",
  "number": "2.3",
  "title": "",
  "body": " The complex numbers are the numbers of the form with addition and multiplication defined by Note that by our definition of multiplication   We can think of as a subset of by identifying with . This makes as subfield of . A subfield is a subset of a field which is itself a field with respect to the operations on the larger set.  "
},
{
  "id": "example-15",
  "level": "2",
  "url": "sec-fields.html#example-15",
  "type": "Example",
  "number": "2.4",
  "title": "",
  "body": " The rational numbers are a field, indeed they are a subfield of (simply meaning that and the field operations for are the same whether you think of elements of as rational numbers or real numbers). This fact means that conditions A1-A2, M1-M2, and C1 are trivially satisfied, since they're true for real numbers, not just rationals. Since the special real numbers are in fact in we also get A3, M3, and C2 for free. This only leaves A4 and M4; given we know that there are real numbers and such that and , but what we need to know is that there are rational numbers with these properties Of course if we don't need to find a multiplicative inverse for . . Fortunately it is that case that if is rational then so are and . Thus is a field.  "
},
{
  "id": "example-16",
  "level": "2",
  "url": "sec-fields.html#example-16",
  "type": "Example",
  "number": "2.5",
  "title": "",
  "body": " The field with two elements , , has as its only elements the two residue classes modulo , with addition and multiplication defined modulo . To be specific, we have the following addition and multiplication tables. There are in fact many interesting examples of vector spaces where we use for our numbers. For instance many schemes of data transmission use the ideas of linear algebra to build error correction into digital communication.  "
},
{
  "id": "def-field-abbreviations",
  "level": "2",
  "url": "sec-fields.html#def-field-abbreviations",
  "type": "Definition",
  "number": "2.6",
  "title": "",
  "body": "  We make some standard abbreviations when computing in fields. We write We also define, for ,    "
},
{
  "id": "remark-2",
  "level": "2",
  "url": "sec-fields.html#remark-2",
  "type": "Remark",
  "number": "2.7",
  "title": "",
  "body": " Above we have defined elements in any field . In we have , and in we have .  You might think it would be confusing that is simultaneously the name of an integer and a field element, particularly in fields or where , but context resolves all such confusions, usually without anyone noticing.  The following rules of arithmetic hold in any field.  "
},
{
  "id": "lem-field-arithmetic",
  "level": "2",
  "url": "sec-fields.html#lem-field-arithmetic",
  "type": "Lemma",
  "number": "2.8",
  "title": "",
  "body": "  If is a field an then for For all we have    "
},
{
  "id": "sec-vector-spaces-again",
  "level": "1",
  "url": "sec-vector-spaces-again.html",
  "type": "Section",
  "number": "2.2",
  "title": "Vector Spaces",
  "body": " Vector Spaces    A vector space over a field is a collection of objects (that we think of as vectors) such that we can form linear combinations where the vectors are from and the scalars are from the field .  A vector space over a field is a set together with operations of vector addition and scalar multiplication (the latter of which we denote by simple concatenation; is the scalar multiple of by ). These must satisfy the following conditions:    Vector addition is commutative: for all we have     Vector addition is associative: for all we have     There is a (vector) additive identity: there exists a special vector such that for all we have .    There are (vector) additive inverses: for all there exists a vector such that       Scalar multiplication respects field addition: for all and we have     Scalar multiplication respects field multiplication: for all and we have     Scalar multiplication respects the field identity: for all we have     Scalar multiplication respects vector addition: for all and we have .        We make a standard abbreviation in the context of a vector space as follows: for .      If is a vector space over the field then the definitions we have made work well with our intuitions in the following ways.    All identities are unique. There is a unique element satisfying A3. There is a unique satisfying M3. There is a unique element satisfying VA3.    All inverses are unique. For , , and there are unique elements and such that     Inverting twice gets you back where you started: for all , , and we have     For all and we have . In particular .    For and we have     if and only if at least one of is      if and only if or .        …of these is traditionally considered good for the soul. Some hints: if both satisfy A3 then what is ? If and satisfy what is ?        The classic vector spaces (over ) are and . With identical checks we see that also , and are vector spaces.    Also vital are subspaces of , such as the plane and the line .     Important remark : note that both the line and the plane given here pass through the origin . A line or plane that does not pass through the origin is not a subspace. Why not?    Similarly , and are vector spaces over and (for any field and any ) is a vector space over .    The set of polynomial is a vector space over and in fact an -algebra. An algebra is a vector space for which we can also multiply two vectors (in addition to multiplying vectors with scalars.      is a vector space over      is a vector space over     The set of matrices whose entries come from is a vector space of , with the usual notions of adding matrices and multiplying them by scalars.    The set is a vector space over . It is a subspace of .      Many of the most important examples of vector spaces come from taking a restricted collection of vectors from some larger vector space.    If is a vector space over and then we say that is a (linear) subspace of if is non-empty and is closed under taking linear combinations. That is to say, for all , we have .      If is a vector space then is a subspace of if and only if and for all , we have .    Suppose is a subspace of . Since is nonempty there exists . Setting and we deduce that . Furthermore setting , and in the definition of subspace gives .  Conversely, suppose we know and for all , we have . Then one can prove by induction on that .  The base case follows by setting and . For the inductive step, assuming we can set , and to conclude .      If is a vector space over and is a subspace of then is a vector space in its own right (with the operations that come with ; we add and scale vectors in just as we do in .)    Most of the vector space axioms are satisfied automatically: VA1, VA2, and SM1-4 work in because they work for every vector in and every scalar in . One key point is that because is closed under taking linear combinations there really is a vector addition map and a scalar multiplication map . The result of adding two vectors in is again a vector in , similarly for scaling.  That leaves only VA3 and VA4. We need to show that and that if then also . Both of these work by choosing appropriate scalings. Since there exists some , so then (because is closed under linear combinations). Similarly, if then .      If is a vector space over and then the span of , denoted , is the set I.e., it is the collection of all linear combinations of vectors in .     If then since the empty linear combination is by convention the vector.    If then . The set of points is a line through the origin of .     The set of polynomials of degree at most is      If is a vector space over and then is a subspace of .    Clearly ; simply take in the definition. Now if and we have, for some and , , , , Thus we have      Example implies that the polynomials of degree at most form a subspace of .    By contrast, the set of polynomials of degree exactly do not form a subspace of . Why not?   "
},
{
  "id": "def-vector-space",
  "level": "2",
  "url": "sec-vector-spaces-again.html#def-vector-space",
  "type": "Definition",
  "number": "2.9",
  "title": "",
  "body": "  A vector space over a field is a collection of objects (that we think of as vectors) such that we can form linear combinations where the vectors are from and the scalars are from the field .  A vector space over a field is a set together with operations of vector addition and scalar multiplication (the latter of which we denote by simple concatenation; is the scalar multiple of by ). These must satisfy the following conditions:    Vector addition is commutative: for all we have     Vector addition is associative: for all we have     There is a (vector) additive identity: there exists a special vector such that for all we have .    There are (vector) additive inverses: for all there exists a vector such that       Scalar multiplication respects field addition: for all and we have     Scalar multiplication respects field multiplication: for all and we have     Scalar multiplication respects the field identity: for all we have     Scalar multiplication respects vector addition: for all and we have .     "
},
{
  "id": "def-vector-space-abbreviation",
  "level": "2",
  "url": "sec-vector-spaces-again.html#def-vector-space-abbreviation",
  "type": "Definition",
  "number": "2.10",
  "title": "",
  "body": "  We make a standard abbreviation in the context of a vector space as follows: for .   "
},
{
  "id": "lem-vector-space-properties",
  "level": "2",
  "url": "sec-vector-spaces-again.html#lem-vector-space-properties",
  "type": "Lemma",
  "number": "2.11",
  "title": "",
  "body": "  If is a vector space over the field then the definitions we have made work well with our intuitions in the following ways.    All identities are unique. There is a unique element satisfying A3. There is a unique satisfying M3. There is a unique element satisfying VA3.    All inverses are unique. For , , and there are unique elements and such that     Inverting twice gets you back where you started: for all , , and we have     For all and we have . In particular .    For and we have     if and only if at least one of is      if and only if or .        …of these is traditionally considered good for the soul. Some hints: if both satisfy A3 then what is ? If and satisfy what is ?   "
},
{
  "id": "example-17",
  "level": "2",
  "url": "sec-vector-spaces-again.html#example-17",
  "type": "Example",
  "number": "2.12",
  "title": "",
  "body": "    The classic vector spaces (over ) are and . With identical checks we see that also , and are vector spaces.    Also vital are subspaces of , such as the plane and the line .     Important remark : note that both the line and the plane given here pass through the origin . A line or plane that does not pass through the origin is not a subspace. Why not?    Similarly , and are vector spaces over and (for any field and any ) is a vector space over .    The set of polynomial is a vector space over and in fact an -algebra. An algebra is a vector space for which we can also multiply two vectors (in addition to multiplying vectors with scalars.      is a vector space over      is a vector space over     The set of matrices whose entries come from is a vector space of , with the usual notions of adding matrices and multiplying them by scalars.    The set is a vector space over . It is a subspace of .     "
},
{
  "id": "def-subspace",
  "level": "2",
  "url": "sec-vector-spaces-again.html#def-subspace",
  "type": "Definition",
  "number": "2.13",
  "title": "",
  "body": "  If is a vector space over and then we say that is a (linear) subspace of if is non-empty and is closed under taking linear combinations. That is to say, for all , we have .   "
},
{
  "id": "lem-subspace-test",
  "level": "2",
  "url": "sec-vector-spaces-again.html#lem-subspace-test",
  "type": "Lemma",
  "number": "2.14",
  "title": "",
  "body": "  If is a vector space then is a subspace of if and only if and for all , we have .    Suppose is a subspace of . Since is nonempty there exists . Setting and we deduce that . Furthermore setting , and in the definition of subspace gives .  Conversely, suppose we know and for all , we have . Then one can prove by induction on that .  The base case follows by setting and . For the inductive step, assuming we can set , and to conclude .   "
},
{
  "id": "lem-subspace-is-vector-space",
  "level": "2",
  "url": "sec-vector-spaces-again.html#lem-subspace-is-vector-space",
  "type": "Lemma",
  "number": "2.15",
  "title": "",
  "body": "  If is a vector space over and is a subspace of then is a vector space in its own right (with the operations that come with ; we add and scale vectors in just as we do in .)    Most of the vector space axioms are satisfied automatically: VA1, VA2, and SM1-4 work in because they work for every vector in and every scalar in . One key point is that because is closed under taking linear combinations there really is a vector addition map and a scalar multiplication map . The result of adding two vectors in is again a vector in , similarly for scaling.  That leaves only VA3 and VA4. We need to show that and that if then also . Both of these work by choosing appropriate scalings. Since there exists some , so then (because is closed under linear combinations). Similarly, if then .   "
},
{
  "id": "def-span",
  "level": "2",
  "url": "sec-vector-spaces-again.html#def-span",
  "type": "Definition",
  "number": "2.16",
  "title": "",
  "body": "  If is a vector space over and then the span of , denoted , is the set I.e., it is the collection of all linear combinations of vectors in .   "
},
{
  "id": "example-18",
  "level": "2",
  "url": "sec-vector-spaces-again.html#example-18",
  "type": "Example",
  "number": "2.17",
  "title": "",
  "body": " If then since the empty linear combination is by convention the vector.  "
},
{
  "id": "example-19",
  "level": "2",
  "url": "sec-vector-spaces-again.html#example-19",
  "type": "Example",
  "number": "2.18",
  "title": "",
  "body": " If then . The set of points is a line through the origin of .  "
},
{
  "id": "example-20",
  "level": "2",
  "url": "sec-vector-spaces-again.html#example-20",
  "type": "Example",
  "number": "2.19",
  "title": "",
  "body": "  The set of polynomials of degree at most is   "
},
{
  "id": "lem-span-is-subspace",
  "level": "2",
  "url": "sec-vector-spaces-again.html#lem-span-is-subspace",
  "type": "Lemma",
  "number": "2.20",
  "title": "",
  "body": "  If is a vector space over and then is a subspace of .    Clearly ; simply take in the definition. Now if and we have, for some and , , , , Thus we have    "
},
{
  "id": "example-21",
  "level": "2",
  "url": "sec-vector-spaces-again.html#example-21",
  "type": "Example",
  "number": "2.21",
  "title": "",
  "body": " Example implies that the polynomials of degree at most form a subspace of .  "
},
{
  "id": "remark-3",
  "level": "2",
  "url": "sec-vector-spaces-again.html#remark-3",
  "type": "Remark",
  "number": "2.22",
  "title": "",
  "body": " By contrast, the set of polynomials of degree exactly do not form a subspace of . Why not?  "
},
{
  "id": "sec-linear-maps-again",
  "level": "1",
  "url": "sec-linear-maps-again.html",
  "type": "Section",
  "number": "2.3",
  "title": "Linear Maps",
  "body": " Linear Maps  We don't need a rough idea of what a linear map is—it's just what we defined it to be earlier.    If are vector spaces over then we say that a function is linear if it preserves linear combinations. That is to say, for all and we have      Prove that Definition is equivalent to requiring that for all and we have and .        given by .    If then we can define a linear map by doing matrix multiplication. To be completely explicit we define, for , It is a fact, that we will prove soon, that every linear map from to is of the form for some matrix .    Let is continuous . Define by . Then is linear by standard facts about integration.        If and are vector spaces over a field , we let denote the set of all linear maps from to , so We have different notation for the special case of where (which is of course a vector space over ). We write . These linear maps are especially important because they correspond to the left hand sides of linear equations.      If are vector spaces over then is also a vector space over when we define, for and ,     The proof is straightforward definition checking. As a representative sample I'll prove that vector addition is associative. For and we have where the central equality is associativity of vector addition in and everything else is the definition of addition in .  The additive identity in is the linear map that sends every vector to , the zero vector in , that is, for all .  There are subspaces naturally associated to a linear map. We define them, then show they are subspaces in an upcoming result      If is a linear map we define the following subspaces associated to it.      Examples of kernels:     for given by where not all of are zero yields that is a plane passing through the origin (note that is a particular solution to ).  If the kernel of the linear map is the set of solutions to the homogeneous system of equations .  If is given by then , the set of constant polynomials       Examples of kernels:  If is given by then the range of is . To see this take any and consider the constant function . Then and . In conclusion, every is in .     If then is a subspace of and is a subspace of .    Consider first vectors . For we have so is closed under taking linear combinations. For we have a criterion for testing membership rather than a way of generating vectors, so the proof looks a little different. Take and . We would like to prove that , and to verify this we simply apply : Thus is closed under taking linear combinations and we are done.     The set of constant polynomials is a subspace of as it is the kernel of the derivative map is given by .  We now see that we can use the notion of kernel and image to analyze properties of linear maps such as being injective or being surjective.     If and are sets and is a function, we say that is surjective provided that for each there exists such that .     Surjective or not surjective?      where is a rotation as in is surjective     where is a rotation as in is surjective     where is a projection as in is NOT surjective. However if we change the target of the map to be the line onto which we project then is surjective. The moral here is that whether a map is surjective or not depends on its codomain.      Here is the relation between surjective and range.    A linear map is surjective if and only if .    According to Definition , . Thus if and only if for every there exists so that if and only if is surjective.    Now we switch to injective.    If and are sets and is a function, we say that is injective provided that whenever then .     Injective or not injective?      where is the derivative map is NOT injective. Indeed the constant polynomials and satisfy but of course .    the map which takes a polynomial to its sequence of coefficients is injective.      There is a close relation between injective and kernel.    A linear map is injective if and only if .    Coming soon to an OER near you!    "
},
{
  "id": "def-linear-map-2",
  "level": "2",
  "url": "sec-linear-maps-again.html#def-linear-map-2",
  "type": "Definition",
  "number": "2.23",
  "title": "",
  "body": "  If are vector spaces over then we say that a function is linear if it preserves linear combinations. That is to say, for all and we have    "
},
{
  "id": "question-2",
  "level": "2",
  "url": "sec-linear-maps-again.html#question-2",
  "type": "Question",
  "number": "2.24",
  "title": "",
  "body": " Prove that Definition is equivalent to requiring that for all and we have and .  "
},
{
  "id": "example-22",
  "level": "2",
  "url": "sec-linear-maps-again.html#example-22",
  "type": "Example",
  "number": "2.25",
  "title": "",
  "body": "     given by .    If then we can define a linear map by doing matrix multiplication. To be completely explicit we define, for , It is a fact, that we will prove soon, that every linear map from to is of the form for some matrix .    Let is continuous . Define by . Then is linear by standard facts about integration.     "
},
{
  "id": "def-lvw",
  "level": "2",
  "url": "sec-linear-maps-again.html#def-lvw",
  "type": "Definition",
  "number": "2.26",
  "title": "",
  "body": "  If and are vector spaces over a field , we let denote the set of all linear maps from to , so We have different notation for the special case of where (which is of course a vector space over ). We write . These linear maps are especially important because they correspond to the left hand sides of linear equations.   "
},
{
  "id": "lem-lvw-vector-space",
  "level": "2",
  "url": "sec-linear-maps-again.html#lem-lvw-vector-space",
  "type": "Lemma",
  "number": "2.27",
  "title": "",
  "body": "  If are vector spaces over then is also a vector space over when we define, for and ,     The proof is straightforward definition checking. As a representative sample I'll prove that vector addition is associative. For and we have where the central equality is associativity of vector addition in and everything else is the definition of addition in .  The additive identity in is the linear map that sends every vector to , the zero vector in , that is, for all .  There are subspaces naturally associated to a linear map. We define them, then show they are subspaces in an upcoming result   "
},
{
  "id": "def-range-nullspace",
  "level": "2",
  "url": "sec-linear-maps-again.html#def-range-nullspace",
  "type": "Definition",
  "number": "2.28",
  "title": "",
  "body": "  If is a linear map we define the following subspaces associated to it.    "
},
{
  "id": "example-23",
  "level": "2",
  "url": "sec-linear-maps-again.html#example-23",
  "type": "Example",
  "number": "2.29",
  "title": "",
  "body": " Examples of kernels:     for given by where not all of are zero yields that is a plane passing through the origin (note that is a particular solution to ).  If the kernel of the linear map is the set of solutions to the homogeneous system of equations .  If is given by then , the set of constant polynomials     "
},
{
  "id": "example-24",
  "level": "2",
  "url": "sec-linear-maps-again.html#example-24",
  "type": "Example",
  "number": "2.30",
  "title": "",
  "body": " Examples of kernels:  If is given by then the range of is . To see this take any and consider the constant function . Then and . In conclusion, every is in .  "
},
{
  "id": "lem-range-ker-subspaces",
  "level": "2",
  "url": "sec-linear-maps-again.html#lem-range-ker-subspaces",
  "type": "Lemma",
  "number": "2.31",
  "title": "",
  "body": "  If then is a subspace of and is a subspace of .    Consider first vectors . For we have so is closed under taking linear combinations. For we have a criterion for testing membership rather than a way of generating vectors, so the proof looks a little different. Take and . We would like to prove that , and to verify this we simply apply : Thus is closed under taking linear combinations and we are done.   "
},
{
  "id": "example-25",
  "level": "2",
  "url": "sec-linear-maps-again.html#example-25",
  "type": "Example",
  "number": "2.32",
  "title": "",
  "body": " The set of constant polynomials is a subspace of as it is the kernel of the derivative map is given by .  We now see that we can use the notion of kernel and image to analyze properties of linear maps such as being injective or being surjective.  "
},
{
  "id": "def-surjective",
  "level": "2",
  "url": "sec-linear-maps-again.html#def-surjective",
  "type": "Definition",
  "number": "2.33",
  "title": "",
  "body": "  If and are sets and is a function, we say that is surjective provided that for each there exists such that .   "
},
{
  "id": "example-26",
  "level": "2",
  "url": "sec-linear-maps-again.html#example-26",
  "type": "Example",
  "number": "2.34",
  "title": "",
  "body": " Surjective or not surjective?      where is a rotation as in is surjective     where is a rotation as in is surjective     where is a projection as in is NOT surjective. However if we change the target of the map to be the line onto which we project then is surjective. The moral here is that whether a map is surjective or not depends on its codomain.     "
},
{
  "id": "prop-",
  "level": "2",
  "url": "sec-linear-maps-again.html#prop-",
  "type": "Proposition",
  "number": "2.35",
  "title": "",
  "body": "  A linear map is surjective if and only if .    According to Definition , . Thus if and only if for every there exists so that if and only if is surjective.   "
},
{
  "id": "def-injective",
  "level": "2",
  "url": "sec-linear-maps-again.html#def-injective",
  "type": "Definition",
  "number": "2.36",
  "title": "",
  "body": "  If and are sets and is a function, we say that is injective provided that whenever then .   "
},
{
  "id": "example-27",
  "level": "2",
  "url": "sec-linear-maps-again.html#example-27",
  "type": "Example",
  "number": "2.37",
  "title": "",
  "body": " Injective or not injective?      where is the derivative map is NOT injective. Indeed the constant polynomials and satisfy but of course .    the map which takes a polynomial to its sequence of coefficients is injective.     "
},
{
  "id": "thm-injective-iff-trivial-kernel",
  "level": "2",
  "url": "sec-linear-maps-again.html#thm-injective-iff-trivial-kernel",
  "type": "Theorem",
  "number": "2.38",
  "title": "",
  "body": "  A linear map is injective if and only if .    Coming soon to an OER near you!   "
},
{
  "id": "sec-bases",
  "level": "1",
  "url": "sec-bases.html",
  "type": "Section",
  "number": "3.1",
  "title": "Linear Independence, Spanning Sets, and Bases",
  "body": " Linear Independence, Spanning Sets, and Bases  We now introduce three related notions that you are probably familiar with from a previous course; linear independence, spanning sets, and bases.  Linear independence has to do with whether one can write the same vector as a linear combination of vectors in in two different ways This leads to considering an equation of the form .   If is a vector space over and then is linearly independent if every linear combination of (distinct) vectors from equaling has all its coefficients equal to . In other words for all , with for we have   We note that the definition applies when (and indeed 0). If then is not linear independent since we can take and and have while .    The set is linearly independent since Similarly is linearly independent.  In the vectors and are linearly independent. Indeed, if and only if which yields .   is linearly independent since if (the right hand side here is the 0 function) then in particular and .   is linearly independent when we think of as a vector space over . Suppose satisfy . If we have . On the other hand if then which we know is impossible.  The set is linearly independent. A linear combination from equaling the function would be a finite sequence such that for all . In other words the polynomial would satisfy for all . Thus every value of would have to be a root of . We know that takes infinitely many values and the only polynomial with infinitely many roots is the polynomial, so our original linear combination must have been trivial.   If is a vector space over and then is linearly dependent if is not linearly independent.    A single vector is linearly dependent if and only if .  Two vectors form a linearly dependent set if and only if one of the vectors is a scalar multiple of the other.  If three vectors form a linearly dependent set it does not follow that one of them must be a scalar multiple of another. For example the set is linearly dependent since but no vector from this set is a scalar multiple of another.  We now show how the property of being linearly independent passes from a set to its subsets.   Let be a vector space over .   If and is linearly independent then is linearly independent.  If is linearly independent then for all there is a unique way of writing as a linear combination from in the following sense: for all there is a unique finite subset and unique corresponding coefficients such that .   The first part is immediate since any linear combination from is also a linear combination from , so any linear combination from equaling is also a linear combination from equaling , so is, by hypothesis, trivial. For the second part suppose that some vector can be represented in two ways: (We've thrown all the vectors used in either representation into the set .) Subtracting the right hand side from the left we have and thus, since is linearly independent, for all . Now, eliminating those for which from the set gives the claimed uniqueness.   Let be a vector space over . Prove that if and is linearly dependent then is linearly dependent.   A subset of a vector space is called spanning (or a spanning set ) if .    The set is a spanning set for .  The set is a spanning set for .  The set of matrices that have all entries equal to 0 except for the entry in row and column which is 1 is a spanning set for .  If is a vector space, then is a (rather large) spanning set for .   Let be a vector space.   If then . In particular if is spanning then so is .  For a subset and a vector we have .   The first is very straightforward. It boils down to observing that if then linear combinations from are in fact linear combinations from .  Now suppose spans then This shows that .  For the second claim, first note that one inclusion, , always holds, by part (a) since . Now that . By definition of span, there exist and so that Since we also have and so that Plugging in to the equation above gives , that is, . We have shown that which completes the proof that .   leaves open the possibility that when we enlarge a linearly independent set by adding any vector it could become linearly dependent. Likewise leaves open the possibility that when we take out any vector from a spanning set, the remaining set could no longer be spanning. We give names to these situations.   A set is maximal linearly independent if is linearly independent and every strict superset is not linearly independent.   A set is minimal spanning if is a spanning set and every strict subset is not spanning.  There are two crucial facts that relate the notions of linear independence and spanning, that are recorded in the following lemma.   Let be subsets of a vector space .   If is a maximal linearly independent set then it spans .  If is a minimal spanning set for then it is linearly independent.   Let be a maximal linearly independent subset of and let . If then certainly . Otherwise is a strict superset of and is therefore not linearly independent. So there is some non-trivial linear combination of vectors from that is . It cannot be that all the vectors involved in this linear combination come from , else would not be linearly independent. Thus for some and some , we have and hence For the second part suppose that is a minimal spanning set and is a non-trivial linear combination from with, without loss of generality, . Then and hence (by part ) of the spanning lemma applied to ) This contradicts the minimality of , so no such linear combination can exist.  The Interaction Lemma allows us to make the following definition.   A basis is a linearly independent spanning set.   The Interaction suggests how to find a basis for a vector space : start with a spanning set and remove vectors until one arrives at a minimal spanning set, which will be a basis. Alternatively start with a linearly independent set and add vectors until one arrives at a maximal linearly independent set. However this process involves choices (not all of which will work). So there can be more than one basis for .   For example, consider the following spanning set for : The following subsets are still spanning (in fact minimal spanning) sets for  Thus all of the above sets are bases for . However the subset of is not a spanning set for since any vector in has the last coordinate equal to 0.  The following theorem comes at the heart of the study of linear independence in vector spaces. It is called a lemma because of its ubiquity. It generalizes .   If is a vector space, is a spanning set, and is a finite linearly independent set then there exists a spanning set with and .   We prove the result by induction on . If then and we can take . Otherwise . Pick and set . By induction there exists a spanning set with with . If then there is nothing further to do; we can take . Otherwise, our plan now is to insert into and kick out one element of , but not an element of , so that the result is still spanning. Since is spanning we have some , such that I claim that for some we have , for otherwise would be a non-trivial linear combination of vectors from equalling , a contradiction to the linearly independence of . Now note that so Thus we can take . It is spanning, and it is easy to check that it satisfies the other conditions.  We define the crucial notion of finite dimensionality and then draw some fundamental conclusions from the Steinitz exchange lemma in the theorem following.   If is a vector space with a finite spanning set then we say is finite dimensional .   Let be a finite dimensional vector space.   All bases of have the same size.  Any spanning set of contains a basis.  Any linearly independent set is contained in a basis.   For a) suppose that and are bases of . By symmetry it suffices to prove that . Since is (in particular) linearly independent, and is spanning, the Steinitz exchange lemma tells us that there exists a spanning set with and . We thus have .  For b), let be spanning. By induction on it is easy to prove that contains a minimal spanning set. [If then itself is clearly minimal; if then either is minimal or it contains a strict subset that is spanning. By induction contains a minimal spanning set, which is a minimal spanning set contained in .] Minimal spanning sets are bases by the Interaction Lemma.  Finally, let be linearly independent and let be a finite spanning set for . (Such an exists by the definition of finite dimensionality.) By part b) there is a basis contained in . By the Steinitz exchange property there is a spanning set with and . By b) we know that contains a basis, but unless that basis is exactly then we would have found a basis for strictly smaller than , contradicting a). Thus is a basis containing .   Let be a finite dimensional vector space with . Then   any spanning set of such that has elements is a basis of   any linearly independent set of such that has elements is a basis of    Suppose a spanning set of has elements. By (2), contains a basis and by (1) has elements. Since has elements the only subset of that has elements is itself so . Thus is a basis since was.  Suppose a lienarly set of has elements. By (3), is contained a basis and by (1) has elements. Since has elements the only subset of that has elements is itself so . Thus is a basis since was.   Let be a finite dimensional vector space and let be a subspace of . Then is a finite dimensional vector space and .   Let be a basis of . Then is a linearly independent subset of and hence also of since . By (3) has a basis so that . Since is finite, so is and moreover .   A basis for is given by the vectors , ehere and has its unique entry equal to 1 in position . This is called the standard basis . Thus .   A basis for as a vector space over is so that . A basis for as a vector space over is so that .   Let's find a basis for the following subspace of : We know that is a subspace since it is the kernel of the linear map By the kernels (nullspaces) are subspaces.  The system is equivalent to the following system in reduced row echelon form in which the variables are dependent (correspond to pivots) and the variables are free. The general solution to this system is By the work above the vectors span and are linearly independent so they form a basis for . In particular, .  "
},
{
  "id": "p-397",
  "level": "2",
  "url": "sec-bases.html#p-397",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "standard basis "
},
{
  "id": "sec-lights-out",
  "level": "1",
  "url": "sec-lights-out.html",
  "type": "Section",
  "number": "3.2",
  "title": "Applications: Lights Out",
  "body": " Applications: Lights Out  There is an electronic game called Lights Out released by Tiger Toys in 1995. The game board consists of twenty-five lights in a grid. Pushing any of the lights causes that light to switch state (from on to off or vice versa ) and also causes all the lights in the same row or column as the one being pushed to switch state. One is presented with a pattern of lights, and the objective is to press some of the lights so as to switch all the lights off.  How does a mathematician play the game? The states of the board are in one-to-one correspondence with the vector space . Define to be the element of with s in every entry of row and also in every entry of column and with all other entries .   If the board is in position then pushing the light at position produces the position .   Clear since adding in turns a into a and a into a .  Thus the problem of solving a Lights Out game with starting position is exactly that of finding a set if positions such that where denotes the zero matrix. Since for all , for all in , the displayed equation above is the same as that is, representing as a -linear combination from . This problem can be solved easily computationally since, as vector spaces, there is no essential difference between and (we will soon say more rigorously that and are isomorphic vector spaces). All the algorithmic techniques, such as Gaussian elimination, discussed in a first linear algebra course work equally well over any field.  "
},
{
  "id": "sec-rep-linear-maps-matrices",
  "level": "1",
  "url": "sec-rep-linear-maps-matrices.html",
  "type": "Section",
  "number": "3.3",
  "title": "Representing Linear Maps by Matrices",
  "body": " Representing Linear Maps by Matrices   Linear maps are determined by a basis  Thinking about bases gives us a lot over control over linear maps, as the following result shows.   If is a basis for the vector space over and is another vector space over then linear maps in are determined uniquely by their values on . To be precise, given an arbitrary function We don't assume that is linear since we don't even have a definition of linearity for such maps. is not a vector space.  there exists a unique linear map such that for all .   The uniqueness is clear since if is written as a linear combination from , so , then we must have that Thus to prove existence we simply need to show that ( ) defines a linear map. Consider and . We'll need to express as linear combinations from ; let be collection of all vectors from that appear in either representation. Suppose [Notice that some of the might be .] Now we have and this is the unique representation of using the basis , except that some of the might be . Thus    Recall that has standard basis . Suppose that I consider a function given by . Then the unique linear function that extends in the sense of is given by Note the beauty of the matter is ensures as defined above is a linear map. We do not need to check this provided we invoke the Theorem.    Linear maps are represented by matrices  It is also true that in some sense any linear map between finite dimensional vectors spaces is really a matrix.   Suppose are finite dimensional vector spaces over and is linear. Given (ordered) bases for and respectively then the matrix of with respect to is the matrix such that for all we have Note that this determines the since has a unique expression as a linear combination from . Note that once we've picked the basis every vector in can be thought of as specified by its coefficients with respect to . If then we can think of as corresponding to the column vector . Similarly the image can be thought of a a column vector if . The matrix is chosen so that    Let's consider the counterclockwise rotation of angle in the plane. Denote this by . Then and , thus the matrix of with respect to the standard basis is    Let's consider the derivative map. Denote this by the vector space of polynomials of degree and let be the derivative map . Then a basis for is and the matrix of with respect to this basis is    "
},
{
  "id": "sec-isomorphisms",
  "level": "1",
  "url": "sec-isomorphisms.html",
  "type": "Section",
  "number": "3.4",
  "title": "Isomorphisms",
  "body": " Isomorphisms  There is a sense in which the vector space of polynomials of degree at most and are similar: each polynomial is determined by its coefficient vector . More formally, as we have seen in , there is a linear transformation  This linear map has the property that for every there exists a unique (the vector of coefficients of ) so that . This makes a bijection.   A function not necessarily linear is called a bijection if it is both injective and surjective.   It is a standard fact that a function is a bijection if and only if there exists a function termed the inverse of which satisfies for all and for all .   Let , be vector spaces over the same field and let be a linear map. We say that is an isomorphism if there exists a linear map so that for all and for all .  We say that vector spaces , are isomorphic and write if there exists an isomorphism  or .  It turns out (it's a small miracle) that a linear function being an isomorphism is the same as being a bijection.   Let , be vector spaces over the same field and let be a linear map. Then is invertible if and only if is a bijection.   By Remark  is a bijection if and only if there exists a function which satisfies for all and for all . To conclude that is an isomorphism we still have to show that the inverse of , is a linear map. Consider and . Then Since is injective we conclude that so is a linear map.  We have seen a lot of example of vector spaces. It turns out that up to isomorphism finite-dimensional vectors spaces are quite simple. Moreover, the isomorphism class of a finite dimensional vector space is completely determined by its dimension.   If is a finite dimensional vector space over , of dimension say, then .   Let be a basis for . Define by This is clearly a linear map. The kernel of is , since if then by the linear independence of the all of the scalars must be . Thus is injective by . Similarly since every vector in can be expressed as a linear combination of the . Thus, by , is invertible, meaning exactly that and are isomorphic.   If are finite dimensional vector spaces over the same field then if and only if .   Homework.  Earlier we have seen that linear maps between vector spaces are represented by matrices. More formally we have:   Let , be vector spaces over the same field with . Then there is a vector space isomorphism     Proof Idea. Fix bases for and respectively. The map that takes a linear map to the matrix of with respect to is an isomorphism (check!).  We can say more about how the isomorphism above interacts with composition of maps.   Let and be linear maps and let be bases for respectively. Then   The function given by is a linear map.  If are the matrices representing and with respect to the given bases then    If you know about rings: the set is actually a ring with respect to the operations of addition of functions and composition of functions . The above lemma says that the map defined in the proof of is also a ring isomorphism .   If you know about commutative diagrams: We may put together Remarks and in the following way: fix bases for and for and let and be the maps constructed as in based on and . Then the following diagram commutes     "
},
{
  "id": "p-415",
  "level": "2",
  "url": "sec-isomorphisms.html#p-415",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "bijection "
},
{
  "id": "p-417",
  "level": "2",
  "url": "sec-isomorphisms.html#p-417",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "isomorphism "
},
{
  "id": "p-418",
  "level": "2",
  "url": "sec-isomorphisms.html#p-418",
  "type": "Paragraph (with a defined term)",
  "number": "",
  "title": "",
  "body": "isomorphic isomorphism "
},
{
  "id": "sec-rank-nullity",
  "level": "1",
  "url": "sec-rank-nullity.html",
  "type": "Section",
  "number": "3.5",
  "title": "Rank-Nullity",
  "body": " Rank-Nullity  As we mentioned in the introduction, we will consider all sorts of linear equations—anything of the form where is a linear map. The most fundamental result about such equations is the Rank-Nullity formula. It relates the dimension of the range of (the set of for which there exists a solution to the equation), also called the rank of , to the dimensions of and .   In we analyzed the linear map and found . Towards this end it was helpful to put the matrix of this linear transformation in RREF using row operations .  Let us consider now the range of , which is a subspace of by . The following is a crucial observation: . Because we have by the Spanning Lemma . Conversely, if then for some which can be written as . Then   Rrecall that are the columns of the matrix of the linear transformation  We want to find a basis for the vector space spanned by the columns of . We can do so by using column ofperations to put in column echelon form We thus see that has basis . Thus .  Note that , where is the domain of . This kind of relationship holds in general by the Rank-Nullity .   If is a linear map and is finite dimensional then    Let be a basis for and a basis for . By definition there exist vectors such that for each we have . We claim that is a basis for , and hence that has dimension . Let's show first that is linearly independent. To that end suppose that and in are such that Applying we get (since each is in the kernel of ) so by the linear independence of the we have . This tells us in turn that , forcing .  Now we show that spans . Given we can represent as a linear combination of the . Now is in since Having shown that is in we get, for some scalars , that and spans .   If and are finite dimensional vector spaces with then the following are equivalent    is injective   is surjective   is bijective    Suppose is injective. Then has . By Rank-Nullity, it follows that . Since and we conclude , in other words is surjective.   Suppose is surjective. Then so . By Rank-Nullity, it follows that and so . Therefore is injective and therefore also bijective.   follows by definition.  "
},
{
  "id": "sec-poly-interpolation",
  "level": "1",
  "url": "sec-poly-interpolation.html",
  "type": "Section",
  "number": "3.6",
  "title": "Application: Polynomial Interpolation",
  "body": " Application: Polynomial Interpolation  Lagrangian polynomial interpolation refers to the following problem:   Let be distinct and let . Find a polynomial of degree at most such that for all .   Polynomial interpolation questions can be rephrased in terms of solutions of linear systems of equations. Write down a general polynomial of degree at most as Then we want to satisfy the system of equations ( ). Written out more explicitly, we need to find such that Alternately, in matrix form, we want to solve the following equation for : The matrix on the left is called a Vandermonde matrix .  As we have seen before, wherever there is a matrix there is a linear map.   Fix . The evaluation map is the linear map given by    A basis for , the subspace of polynomials of degree at most in , is . We will use the standard basis for . Then the matrix for the evaluation map in these bases is the Vandermonde matrix   Next we describe a different basis for .   The Lagrange basis for the vector space with respect to the nodes consists of the polynomials which satisfy    Let be distinct and let . There is a unique polynomial of degree at most such that for all , written in terms of the Lagrange basis as    We first verify that satisfies for all . This follows from : Now consider the evaluation map given by . We have just shown that this map is surjective and we know . According to we deduce that is bijective, so the uniqueness of follows.   If we set to be the Lagrange basis of and to be the standard basis of then the matrix for the evaluation map is the identity matrix thanks to . This explains the important role this basis plays in interpolation.  "
},
{
  "id": "sec-direct-sums",
  "level": "1",
  "url": "sec-direct-sums.html",
  "type": "Section",
  "number": "4.1",
  "title": "Direct Sum of Vector Spaces",
  "body": " Direct Sum of Vector Spaces   We introduce now two constructions of new vector spaces from old.    Internal direct sum  Let's first notice that we can add and scale subspaces of a vector space just like we add and scale vectors. To add two subspaces we will in fact adding together elements from the subspaces in all possible ways. Our notation is as follows.   If are subsets of a vector space over and we define We make corresponding definitions for more complicated expressions such as .   If and are subspaces of a vector space and then and are subspaces of .   That is a subspace was proven on homework. Since is a subspace if and . In either case is a subspace of .  By induction, it follows that any linear combination of subspaces of is a subspace of .  Recall that we introduced the idea of vectors being linearly independent to model the fact that ever vector isn can we written uniquely as a linear combination of . Now we want to define a notion similar to linear independence for subspaces.   If and are subspaces of a vector space we say that is an internal direct sum and we denote if every vector can be written uniquely as with and .  More generally, if are subspaces of a vector space we say that is an internal direct sum and we denote it if every vector can be written uniquely as with .  We can even have infinite direct sums: suppose that is a family of subspaces of a vector space and any vector with can be written uniquely in this form we say we have a direct sum .   Let and let and . Then .   Let and let and be the subspaces of symmetric and antisymmetric matrices respectively. In detail if and only if and if and only if , where denotes transpose. Then as we can write and moreover if with and then we have . From we obtain and .   Let be a one-dimensional vector space over whose basis vector we call . In short . Then we have    Let and be subspaces of a vector space . Then is a direct sum if and only if .   Suppose is a direct sum and let . Then are two different ways of writing as a sum of a vector in and a vector in unless .  Suppose . If with and then . The left hand side is in and the right hand side is in . So we must have , therefore . So and .   One place where we (silently) thought about an internal direct sum was in the proof of . Looking at that proof we see that we really showed that if then is the internal direct sum of and .   Let and be subspaces of a vector space . If is a basis for and is a basis for then is a basis for . In particular,    The last part follows from the formula proven on the homework and which gives so that .   Let is a subspace of a vector space then there exists a subspace of so that .   Let be a basis for . Since is an independent set, there exists a basis of so that . Set and . Then is a subspce of .  Any can be written as for some and . After renumbering we may assume and so that where and . We have shown .  Now suppose . Then for some and also for some . Therefore we have with distinct. Since is linearly independent we conclude for all whence . We have shown that and it follows from that .   The subspace in the above theorem is not unique. For example set . Then both and satisfy .    External direct sum and direct product  Suppose that is a family of vector spaces over a field . We would like to exhibit a vector space that contains (copies of) each of the without assuming that they are subspaces of the same vector field to begin with. Since contains the it must of course also contain linear combinations of vector from the . We will arrange that these linear combinations cannot accidentally produce any sort of cancellation. In particular for instance we want it to be the case that if and then the only way to have is for each of and to be .   We define This is the external direct sum of the . The operations are performed coordinate-wise: and similarly for scalar multiplication by  It is easy to check that with these operations is a vector space.   It is sometimes desirable to eliminate the restriction that only finitely many of the are non-zero. Then we get the direct product.   In both and there is a copy of . We can identify with the set of vectors that are everywhere except in the component. The direct sum is spanned by the union of these copies of the , but (at least when is infinite) the direct product is not.   In the cases where our two definitions of direct sum both apply (i.e., when considering where are subspaces of ) the two definitions agree. In particular with the external direct sum on the left and the internal direct sum on the right the following map is an isomorphism:    The fact that is all of is clear. To prove that consider a pair such that . We need to show that . Well, since we have so and hence .   "
},
{
  "id": "sec-quotients",
  "level": "1",
  "url": "sec-quotients.html",
  "type": "Section",
  "number": "4.2",
  "title": "Quotients of Vector Spaces",
  "body": " Quotients of Vector Spaces   Let be a vectors space. We say that is an affine subspace of if there is a vector and a linear subspace with . In this situation we say that is parallel to .   Let be the set of solutions to a linear system of equations . Then is an affine space parallel to , the set of solutions to the homogeneous linear system of equations .   The vector space is uniquely determined by by the formula . The vector is not uniquely determined by .   If is a subspace of the it is possible to make the collection of all affine subspaces parallel to into a vector space called the quotient vector space  . We let:    With the above definitions is a vector space.   We first need to prove that are maps to , or in other words that and are affine subspaces parallel to . To prove affine-ness set and and note that and if  and also if  To show that is a vector space we would now have to check the axioms of a vector space. I will not do this explicitly.   It follows from above that we can define the operations in alternatively by The cost of this is that since many choices of define that same affine subspace one needs to check, for instance, that if and then . We are spared this task. It is easy however to see that our definition agrees with theirs since, using our definition of , A similar argument proves that the two different definitions of agree.   If we let and then the quotient is the vector space of real polynomials, except that we have erased the difference between and , or to put it another way, between and . Thus, for instance, so in this space. We have just constructed the complex numbers!  As in the previous example, one way of thinking about the quotient vector space is that we are erasing all distinctions between vectors that differ by an element of . In particular if is a linear map with kernel containing (so for all ) then such a distinction has no effect on the value of and we can treat as being a linear map on . The details are contained in the following lemma.   The following conditions are equivalent for a subspace and a linear map .    for all (i.e., )   is constant on affine subspaces of parallel to   There exists a linear map satisfying for all .  Let's use this theorem to prove that the quotient vector space in is indeed .   Let . There is a vector space isomorphism We start with the map , that is given by evaluating a polynomial at the complex number . Now if then we have since . Thus we have shown that . By the previous theorem there is a linear map I claim this map is a bijection. To see it is surjective, if then we see that . To see that is injective note that if then so since the roots of polynomials with real coefficients come in conjugate pairs. Thus , that is . Therefore . We see that is also injective.    Proof of . We'll prove a) implies b) implies c) implies a). Firstly, let's assume a) and consider an affine subspace . Then for all we have , so is constant on . To show that b) implies c) consider a linear map that is constant on affine subspaces parallel to and define by letting be the common value of for all . Linearity of is clear since for and we can pick and and compute the common value of on as follows: Finally, to prove that c) implies a) note that is the zero vector in so if such a linear map exists then for we have   We can give an algebraic version of the Rank-Nullity Theorem that says more than the equality of two numbers; it gives the isomorphism of two vector spaces.   For any linear map we have    The isomorphism is the map whose existence is guaranteed by . It is surjective since if then , and injective since if then so is the zero vector.   Consider the linear map given by . Then is the -axis. The set is the set of all lineas in the plane parallel to the -axis (vertical lines). The range of can be identified with the -axis. There is a bijection between vertical lines and points on the -axis given by sending a line to its intercept. This map is the isomorphism claimed by .  The following is a closer statement to Rank-Nullity which we can make for any quotient vector space.   If is finite dimensional and is a subspace of then    It is definitely possible to give a rather long but very direct proof of this, but to illustrate the power of the Rank-Nullity Theorem ( ) we'll give a shorter proof. Consider the (linear) map Note that is clearly surjective since for any we have . Similarly the kernel of is the set of such that . This happens exactly when , so . Thus gives us   "
},
{
  "id": "sec-clustering",
  "level": "1",
  "url": "sec-clustering.html",
  "type": "Section",
  "number": "5.1",
  "title": "Application: <span class=\"process-math\">\\(k\\)<\/span>-means Clustering",
  "body": " Application: -means Clustering   Adapted from notes by Sebastien Roch https:\/\/people.math.wisc.edu\/ roch\/mmids\/notes.html Imagine that you are an evolutionary biologist studying irises and that you have collected measurements on a large number of iris samples. Your goal is to identify different species within this collection. ( Source )  Here is a classical iris dataset first analyzed by the statistician Ronald A. Fisher . We will upload the data in the form of a data table (similar to a spreadsheet) , where the columns are different measurements (or features) and the rows are different samples. Below, we load the data using pandas.read_csv and show the first lines of the dataset (see DataFrame.head ).   \\PY{c+c1}{\\PYZsh{} Python 3} \\PY{k+kn}{import} \\PY{n+nn}{pandas} \\PY{k}{as} \\PY{n+nn}{pd} \\PY{k+kn}{import} \\PY{n+nn}{numpy} \\PY{k}{as} \\PY{n+nn}{np} \\PY{k+kn}{import} \\PY{n+nn}{matplotlib}\\PY{n+nn}{.}\\PY{n+nn}{pyplot} \\PY{k}{as} \\PY{n+nn}{plt} \\PY{k+kn}{import} \\PY{n+nn}{seaborn} \\PY{k}{as} \\PY{n+nn}{sns}   \\PY{n}{df} \\PY{o}{=} \\PY{n}{pd}\\PY{o}{.}\\PY{n}{read\\PYZus{}csv}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{iris\\PYZhy{}measurements.csv}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)} \\PY{n}{df}\\PY{o}{.}\\PY{n}{head}\\PY{p}{(}\\PY{p}{)}   Id PetalLengthCm PetalWidthCm SepalLengthCm SepalWidthCm 0 1 1.4 0.2 5.1 3.5 1 2 1.4 0.2 4.9 3.0 2 3 1.3 0.2 4.7 3.2 3 4 1.5 0.2 4.6 3.1 4 5 1.4 0.2 5.0 3.6  There are samples (as can be seen by using DataFrame.shape which gives the dimensions of the DataFrame as a tuple). Each row in this data set can be seen as a vector (or point) in since it has four entries which are real numbers.   \\PY{n}{df}\\PY{o}{.}\\PY{n}{shape}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}   150  Let's first extract the columns into a Numpy array using DataFrame.to_numpy() , and visualize the petal data. Below, each point is a sample. This is called a scatter plot .   \\PY{n}{X} \\PY{o}{=} \\PY{n}{df}\\PY{p}{[}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PetalLengthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PetalWidthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SepalLengthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SepalWidthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{]}\\PY{o}{.}\\PY{n}{to\\PYZus{}numpy}\\PY{p}{(}\\PY{p}{)}   \\PY{n}{plt}\\PY{o}{.}\\PY{n}{scatter}\\PY{p}{(}\\PY{n}{X}\\PY{p}{[}\\PY{p}{:}\\PY{p}{,}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{p}{,} \\PY{n}{X}\\PY{p}{[}\\PY{p}{:}\\PY{p}{,}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{)} \\PY{n}{plt}\\PY{o}{.}\\PY{n}{xlabel}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PetalLengthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)} \\PY{n}{plt}\\PY{o}{.}\\PY{n}{ylabel}\\PY{p}{(}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PetalWidthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{)} \\PY{n}{plt}\\PY{o}{.}\\PY{n}{show}\\PY{p}{(}\\PY{p}{)}     image   We observe a clear cluster of samples on the bottom left. What is a cluster ? Intuitively, it is a group of samples that are close to each other, but far from every other sample. In this case, it may be an indication that these samples come from a separate species.  Now let's look at the full dataset. Visualizing the full -dimensional data is not straightforward. One way to do this is to consider all pairwise scatter plots.   \\PY{n}{sns}\\PY{o}{.}\\PY{n}{pairplot}\\PY{p}{(}\\PY{n}{df}\\PY{p}{,} \\PY{n+nb}{vars}\\PY{o}{=}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PetalLengthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{PetalWidthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SepalLengthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{SepalWidthCm}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\\PY{p}{,} \\PY{n}{height}\\PY{o}{=}\\PY{l+m+mi}{2}\\PY{p}{)} \\PY{n}{plt}\\PY{o}{.}\\PY{n}{show}\\PY{p}{(}\\PY{p}{)}     image      What would be useful is a method that automatically identifies clusters whatever the dimension of the data . In this chapter, we will discuss a standard way to do this: -means clustering.  Clustering is the following fundamental problem in data science: we are given vectors in . and we want to partition these data points into disjoint subsets - or clusters - with small pairwise distances within clusters and large pairwise distances across clusters.  Fix a number of clusters . Formally, we define a clustering as a partition.   A partition of of size is a collection of non-empty subsets that:   are pairwise disjoint, i.e., , and  cover all of , i.e., .   We number the clusters for notational convenience, but their order is meaningless. Two partitions are the same if they are the same family of subsets.  For each cluster we pick a representative or center of the cluster . Note that need not be one of the 's. Intuitively, clusters must posses the property that each of the points in the cluster must be close in Euclidean distance to the center of the cluster, .   Let be a vector in . Then the Euclidean norm of is The Euclidean distance between points and in is   The aggregate of the distances of the points in our data sets to the centers of the clusters is defined below.   Fix vectors . For a partition of and cluster representatives the -means objective function is   Our goal is to find a partition and also the cluster representetives that minimize . The -means algorithm is a popular heuristic to solve this problem. It is based on the idea that the following two sub-problems are easy to solve:   finding the optimal cluster representatives for a fixed partition;  finding the optimal partition for a fixed set of cluster representatives.  The -means algorithm then alternates between the two steps until progress falls below a given tolerance. This is reasonable since our goal as stated above is to solve the minimization problem where ranges over all partitions of of size . Fixing partition and miniminizing over corresponds to solving the first problem above, while fixing and miniminizing over partitions corresponds to solving the second problem.  To elaborate on the first step above, we review an elementary fact about quadratic functions.   Let where and . The unique global minimum of is attained at    We rewrite as   Since we see that this quantity is minimized when and any other gives a higher value for . The step on the second line above is called Completing the Square .    Fix a partition . The representatives which minimize the objective function     are the centroids    Using the notation and similarly for , note that we can expand the -means objective as     The expression in square brackets is a quadratic function in      and therefore, by is minimized at     Since each term in the sum over making up the objective function is strictly minimized at , so is .   Fix the representatives . The partition that minimizes the objective function is obtained as follows. For each , find the that minimizes (picking one arbitrarily in the case of ties) and assign to .   If is the cluster assignment associated to , then we can re-write the objective as By definition, when the 's are fixed, each term in the sum on the right-hand side is minimized separately by the assignment in the statement. Hence so is the sum itself. Note that we used the fact that the square root is non-decreasing to conclude that minimizing or its square root are equivalent.  We now see that the -means algorithm approaches a (local) solution to the problem .   The sequence of objective function values produced by the -means algorithm is weakly decreasing and hence converges to a (local) minimum of the function .   By the Optimal Representatives and Optimal Clustering lemmas, each step does not increase the objective. A sequence of real numbers that is weakly decreasing and bounded below (in our case by 0, as the distances are all non-negative) converges.  The -means algorithm is only a heuristic. In particular, it is not guaranteed to find the global minimum of the -means objective. However, it is guaranteed to improve the objective at every iteration, or more precisely, not to make it worse.  "
},
{
  "id": "figure-1",
  "level": "2",
  "url": "sec-clustering.html#figure-1",
  "type": "Figure",
  "number": "5.1",
  "title": "",
  "body": "  image  "
},
{
  "id": "figure-2",
  "level": "2",
  "url": "sec-clustering.html#figure-2",
  "type": "Figure",
  "number": "5.2",
  "title": "",
  "body": "  image  "
},
{
  "id": "sec-bilinear-functions",
  "level": "1",
  "url": "sec-bilinear-functions.html",
  "type": "Section",
  "number": "5.2",
  "title": "Bilinear Functions",
  "body": " Bilinear Functions   The multiplication function is not linear since, for instance, . However the definition of a field requires that be linear in each argument:    If are vector spaces over a field then we denote by their direct product (which is the same as their external direct sum) This is a vector space with componentwise addition and saclar multiplication. See Definition .   If are vector spaces over a field then is bilinear if it is linear in each argument. That is to say for all , and we have     The dot product is one of the most important bilinear maps there is; it defines distances and angles in , thereby introducing geometry into linear algebra.  Applying linear maps to vectors (evaluating linear maps) is a bilinear function: is bilinear since and also .  Exaluating non-linear maps though, for example polynomials is not a bilinear map. Specifically the map from to is not bilinear. It is linear in , but not in since, for example, if then .   Given a vector space over a field , a bilinear form on is a bilinear map .   If then we can define a bilinear form as follows:   Conversely, bilinear forms can always be represented by matrices.   If is a bilinear form and is an ordered basis for then the matrix for with respect to is the matrix with entries We have In other words if we set then behaves like the bilinear map in Example .   If is bilinear we say that is symmetric if for all and anti-symmetric if for all .   Dot product is a symmetric bilinear form since .  "
},
{
  "id": "sec-quadratic-functions",
  "level": "1",
  "url": "sec-quadratic-functions.html",
  "type": "Section",
  "number": "5.3",
  "title": "Quadratic Functions",
  "body": " Quadratic Functions  We are now in a position to define what we mean by a quadratic function on a vector space. It's what you get from a bilinear function by plugging in the same thing for both arguments. More formally:   A quadratic function on a vector space is a function such that there exists a bilinear function with for all . We say that is the quadratic function determined by .   If is a field in which then for any quadratic function there is a unique symmetric bilinear function such that is the quadratic function determined by .   We show uniqueness first: assume there exists a symmetric bilinear function such that . Then we have, for , Hence is determined by the values of .  For existence, define to be given by the rule One checks that this rule defines a function that is bilinear and symmetric and that .   Consider the matrix As in we may construct a bilinear form which determines a quadratic function by the formula for all . The matrix of with respect to the standard basis is . Let . Then   We can complete the square to get   This procedure of completing the square allows us to express any quadratic function on as a linear combination of squares of linear expressions. What this amounts to is that if we had taken a different basis for with the property that then that basis    would have the property that . For the purposes of understanding this would be a much better basis. By choosing a different basis we changed the matrix of to With respect to the basis we have the matrix   We now see that for any symmetric bilinear form one can find a basis for which the matrix representing the form is a diagonal matrix.   If is a symmetric bilinear form on a finite dimensional vector space over a field in which then has a basis with respect to which the matrix is diagonal. Moreover there exist scalars so that the quadratic form defined by satisfies    We prove the result by induction on . If then the result is trivial since all matrices are diagonal. If and then we can pick any basis and our matrix will be the zero matrix, and in particular diagonal. Suppose then that and is not identically . Let be the associated quadratic form. If were identically then so also would be, so there exists such that . Define The bilinearity of allows us to check that is a subspace of . In fact we have . To prove this note first that if then for some and also which implies that (since ). Now consider an arbitrary . If we set and then so . Thus . Now we know that , so by induction there is a basis of with respect to which the matrix of (restricted to ) is diagonal, say By we get that is a basis for with respect to which the matrix of is and the result is proved.   It is natural to turn the proof of into an algorithm that starts with a basis for and converts it into a basis such that has a diagonal matrix. One step of the algorithm is the following Unfortunately this process only works if you can guarantee that at least one of the has . Sadly there are examples where but . For instance if then , but is not identically on . One situation where this algorithm does work is when (or equivalently B) is positive definite , meaning that for all we have . In this context this algorithm is known as the Gram-Schmidt algorithm.   A bilinear form on a real vector space is called positive definite if the associated quadratic function satisfies for all with .  "
},
{
  "id": "sec-inner-product-ortho",
  "level": "1",
  "url": "sec-inner-product-ortho.html",
  "type": "Section",
  "number": "5.4",
  "title": "Inner Product Spaces and Orthogonality",
  "body": " Inner Product Spaces and Orthogonality  All the vector spaces in this section will be vector spaces over one of the fields or .   We will use the complex conjugation operation on given by and the absolute value function   Note that for any . In fact for we have if and only if .   An inner product on vector space is a function which is    conjugate-symmetric: for all we have     linear in the first entry: for all and we have     positive definite for all such that we have     The vector space together with its inner product is called an inner product space . If is an inner product space over the field of real numbers we call a Euclidean space .   Suppose is an inner product space and denote the inner product of by . The corresponding quadratic function can be used as a way of measuring lengths of vectors in . We define the norm of a vector to be Note that since the inner product is positive definite we have and moreover for all .   For all and we have                .   Exercise.   If Properties (b) and (c) of an inner product imply that it is a bilinear function. Thus when is a Euclidean space, the inner product is a symmetric bilinear function.   If or , the dot product is the standard inner product on . For this inner product is just the usual length of .   The following is an inner product on , the real vector space of continuous functions : Checking the conditions is straightforward.     Symmetry means for all . (Since we don't need to conjugate.)  This holds since multiplication of real numbers is commutative so that for all and therefore , that is .     Linearity means       Positive definiteness means that whenever .  This is a consequence of the fact that and so for all . Moreover, if there exists so that and in fact by continuity on some interval within . This gives that whenever .     If are elements of an inner product space we say that and are othogonal (to each other) if .   If is an inner product space then a list of vectors is a orthonormal list (ONL) if An orthonormal basis (ONB) is an orthonormal list which is also a basis for .   If or then the standard basis of is a ONB with respect to the standard inner product.   Any finite dimensional inner product space has an ONB. Moreover a ONB can be obtained using the following algorithm:   Start with any basis of   Set .  For i from 1 to set Then is a ONB for .   We prove by induction on that is a ONL that satisfies .   Base case:  since we deduce that . Moreover    Induction step: assume that is a ONL that satisfies . Therefore . The definition of shows that . Since we conclude that .  Conversely the inductive hypothesis gives . Moreover solving for from the equation defining shows that . Since we conclude that .  We may now conclude that .  To show is a ONL we compute by the inductive hypothesis that for . Now for we have Since we have In view of we conclude . Finally, by the same argument as in the base case.  The case shows that is a spanning set for . Since is a basis of we know that . By this implies that is a basis for .   Find a ONB for the Euclidean vector space of polynomials of degree at most two with inner product   We start with the basis . Then we set Further   In the next step we have   The desired is .  Having a ONB presents the advantage that it make it easy to write any vector in as a linear combination of the ONB. We can even give a closed formula for the coefficients of such a linear combination.   If is a ONB for an inner product space and then the unique way to write as a linear combination of the s is    Since is a basis, in particular it spans , so there exist such that Substituting gives , as claimed.   Let be an inner product space and let . Then the orthogonal projection of onto is a vector so that with and orthogonal to .   The orthogonal projection of onto is unique and is given by    If is an inner product space and is a subspace then we define the orthogonal complement to to be the set    The set is a subspace of ; this is a consequence of the linearity of in the first input.   If is a subspace of a finite dimensional inner product space , then    To show that is a direct sum we use . The fact that and have trivial intersection is a consequence of the fact that if then and hence by positive definiteness.  To show that is all of , start by picking an orthonormal basis for . Given set Note that so it suffices to show that . For this in turn it suffices to show that for . We have We have shown where and , so . Thus . Since the opposite containment is true, we conclude .   shows that if is a subspace of a finite dimensional inner product space , then for each there exist unique vectors and (which depend on ) so that . Then the projection of onto is and the projection map from to is the function .   If is an orthonormal basis for then shows that   "
},
{
  "id": "sec-least-squares",
  "level": "1",
  "url": "sec-least-squares.html",
  "type": "Section",
  "number": "5.5",
  "title": "Application: Least Squares (Linear Regression)",
  "body": " Application: Least Squares (Linear Regression)  Often people want to solve linear equations that can't be solved. For instance given a large collection of points they want to find a straight line that goes through the points. Suppose the straight line has equation for some real numbers . Then we want to solve for in the equations If we write for the matrix we can write the system above in matrix form as given below      image   Of course typically it is not possible to solve , meaning that this system has no solution because the points we started with are not actually on any line. So instead we wish to find the nearest thing in terms of minimizing distances. More formally, let's consider the actual -coordinates of the points on some line which correspond to . Say these -values are and set . The vectors we can get in this way are exactly . We are interested in minimizing the distance from to , that is . This is equivalent to minimizing .  The least squares problem is then   It turns out that the unique optimal solution to this problem is We justify this below. To compute the projection from we may employ which requires computing an othonormal basis of .   If is an inner product space, is a subspace, and then the unique point with smallest is .   First note that there is, by , a unique way of writing with , . Now suppose that is any element of . We have with equality only when . The second equality in the above display uses the fact that and .  "
},
{
  "id": "figure-3",
  "level": "2",
  "url": "sec-least-squares.html#figure-3",
  "type": "Figure",
  "number": "5.3",
  "title": "",
  "body": "  image  "
},
{
  "id": "sec-riesz",
  "level": "1",
  "url": "sec-riesz.html",
  "type": "Section",
  "number": "5.6",
  "title": "Riesz Representation Theorem",
  "body": " Riesz Representation Theorem  If is an inner product on and we fix then the map is an element of (since in fact this is true for any bilinear form). More is true though; in fact any function in can be represented this way. This is the content of the following theorem.   If is an inner product on a finite dimensional vector space and then there exists a unique vector such that for all we have Moreover, the map given by is a bijection.   By there is a ONB for such that for all we have Consider what does to the . For some we have . Define . Since is a basis we know that for any there exist such that , and therefore Suppose that vectors are both candidates to be ; in other words for all we have In particular for all . Choosing gives and therefore .  The proof so far has justified that the rule defines a function and that this function is given by Now let's try to define an inverse to this function. Consider a function given by is the unique linear function so that .  Now if , then is the unique linear that maps . But also maps so by uniqueness we have .  If , then   We have thus shown that is the inverse of , so is bijective.   The function in is not linear if , but it is linear if . So in the case of real inner product spaces this function gives a canonical vector space isomorphism .   We know that the evaluation function defined by is a linear map from to . The Riesz Representation Theorem tells us that there is some fixed polynomial such that for all we have It turns out that this polynomial is   "
},
{
  "id": "sec-adjoints",
  "level": "1",
  "url": "sec-adjoints.html",
  "type": "Section",
  "number": "5.7",
  "title": "Adjoints",
  "body": " Adjoints  Suppose are real vector spaces each equipped with an inner product, and that . We define the adjoint of , written as follows. Given a vector we define a linear map in by This is clearly linear (once we check that we can in fact compute since ). Therefore, by the Riesz Representation Theorem, there is a unique vector, that we denote by , with the property that for all we have The next lemma verifies that is linear.   Given between finite dimensional Euclidean spaces the map is linear.   We need to check that for and the vector has the property that for for all  To verify this we compute as follows: The second equality used the definition of and .   Consider the subspace . Let the linear map be the differentiation map . I claim that is given by To prove this we have to show that for all and we have . This is simply integration by parts:    For a matrix with entries in we define to be the conjugate-transpose of . Specifically, if then . Transposeing commutes with conjugation so we could also write .  If the entries of are in then .   Consider the vector spaces and with their standard inner product . The adjoint of the linear map is the linear map . To see this note that for and    We will prove a version of the Rank-Nullity formula that gives, for inner product spaces, an even clearer picture of the situation.   If are inner product spaces and then .   If then the vector is the unique vector in satisfying for all . We'll show that has this property; for all  Thus for all .   If , an inner product space, then we say that two subspaces of are orthogonal if are orthogonal for all , . If and are orthogonal then we write    If are finite dimensional inner product spaces and then Moreover .   First note that by we need prove only one of the two equalities, since the second is simply what the first says about . We start by showing that is orthogonal to , and that therefore in particular their intersection is . Take and . Then   To prove that is all of we're going to give a dimension argument and use that , or in other words that . We'll start by proving that . Consider then the linear map This map has trivial kernel, exactly by the argument above: a vector that was in and had would be in . By we get . Now using again we have . This tells us that is a subspace of having dimension , so the two spaces are equal.   Suppose that is a subspace of an inner product space and we consider the inclusion map What is ? It is the orthogonal projection onto . Since we can represent any vector uniquely as where and . The projection map is the map sending to . To prove that consider and with as before. Thus and . We need to show that : as required.   Let be an injective linear map on a finite dimensional inner product space. Then the map is bijective.   To show the map is bijective it suffices to show it is injective, that is, . Suppose that . Then so . Since is injective we deduce that , so we're done.  Returning to our example of fitting a straight line to our set of points we see that is the closest point in to . Thus to solve for and we should solve .   The solution to the least squares problem in is    Let be the linear map given by .  The final piece of the puzzle is the fact that by we know exactly what is: it's just and over the real numbers . Thus we can compute as follows using that with .   "
},
{
  "id": "sec-simplest-matrix",
  "level": "1",
  "url": "sec-simplest-matrix.html",
  "type": "Section",
  "number": "6.1",
  "title": "Simplest Matrix of a Linear Transformation",
  "body": " Simplest Matrix of a Linear Transformation  Here is a theorem we could have proved a long time ago.   If are finite dimensional and is linear then there are bases for respectively such that the matrix of has the form where all entries are except for  s on the main diagonal, where    We already know the appropriate bases; they are those used in Theorem . There we took a basis for , a basis for and picked preimages such that for . Extend (arbitrarily) to a basis of , and set . With respect to these bases the matrix of has the form above. The fact that gives the first columns and the fact the gives the next .  Though this theorem appears to completely answer the question of the simplest form of the matrix of an linear map (and in a beautiful way) it is actually substantially less valuable than it appears at first sight. The most natural cases of linear maps come from ; linear maps from a space to itself. For these we want to think about only one basis for , that is, we want to have , and use that basis to express both the input vector and the output vector . It turns out the simple form in cannot always be achieved for . This leads us into deeper waters, which we explore in this chapter.  "
},
{
  "id": "sec-eigen-again",
  "level": "1",
  "url": "sec-eigen-again.html",
  "type": "Section",
  "number": "6.2",
  "title": "Eigenvectors and Eigenvalues",
  "body": " Eigenvectors and Eigenvalues  The most general tool we have to help us understand high powers of linear maps is the of eigenvectors.   For a vector space we denote the vector space of linear maps . We call such a map a linear operator on .   The identity map , is an element of .   If is a vector space and then we say that is an eigenvector of if for some scalar . If is an eigenvector and we say that is the eigenvalue associated to .    If is the vector space of infinitely differentiable functions from and mapping to is the differentiation map then any exponential function is an eigenvector of with eigenvalue .  Consider the left shift map on defined by Again we have that exponential sequences are eigenvectors: if then is an eigenvector with eigenvalue . Even gives an eigenvector; and is thus an eigenvector with eigenvalue .  The following simple observation is surprisingly important in finding eigenvectors. It shifts the focus of attention away from the eigenvector onto the eigenvalue.   For a linear map , the following are equivalent:    has an eigenvector with eigenvalue       is not injective.  If the vector space is finite dimensional then the statements above are further equivalent to    is not injective,   is not surjective.   We have This shows the equivalence of 1. and 2. For 2. and 3., see . For the equivalence of 3., 4., and 5. see .   If then the map has an eigenvector of eigenvalue exactly if the matrix has rank less than . We have a quick way of determining (this is a pun) whether the matrix has rank less than . This happens precisely when . Thus is an eigenvalue of exactly if Thus the eigenvalues of are precisely . To find the corresponding eigenvectors we need to find for each of . We have If we choose the basis where and then the matrices of and with respect to this basis are respectively.   Consider the right shift map on defined by This map has no eigenvectors. First consider and a solution to . If then let be minimal such . We have This contradiction establishes that , or in other words that is not a eigenvalue of . On the other hand if we'd be looking for . It is clear however that ; if has then .  To compute eigenvalues it is very helpful to endow the vector space with extra structure.   Given a vector space over a field , the set endowed with the operations of addition , scalar multiplication , and composition (written from now on as multiplication ) is a - algebra meaning that it satisfies:    with its addition and scalar multiplication is a -vector space  Composition is associative: for all   Composition distributes over addition: and for all   Composition respects scalar multiplication: for all , .   Given any and any polynomial we can sensibly define in the following way: for set ( times) and define   A very important fact about polynomials in is the following.   Any monic polynomial of degree factors into linear factors; there exist such that   The following theorem begins to explore the connection between eigenvalues of and polynomials satisfied by .   If is an -dimensional -vector space and then has an eigenvector.   Since the set is linearly dependent. In other words there are scalars with (without loss of generality) such that Consider then the polynomial . All monic polynomials (of degree at least ) over factor into linear factors; there exist such that Thus Now if is any non-zero vector in we have . Let be minimal such that (Note that we might have , but certainly .) Then by the minimality of we have so is an eigenvector of , and is the corresponding eigenvalue.  It is quite possible, as the following example shows, to have an operator on with only one eigenvector (up to scalar multiples).   Consider the map (whose matrix with respect to the usual basis has s on the diagonal immediately above the main diagonal and s everywhere else) The action of on the basis vectors is to map Clearly is an eigenvector with eigenvalue . The only eigenvectors of are multiples of . To see this note that, by an argument precisely analogous to that in , there are no non-zero eigenvalues. If then implies ; in other words .  We are not able, in light of , to prove that every linear map on a finite dimensional -vector space is diagonalizable. The closest we can come in general is a situation somewhat similar to that example.   If is a finite-dimensional -vector space and then has a basis with respect to which the matrix of is upper triangular.   The result is trivially true for . Suppose then that . Let be an eigenvector of with eigenvalue and set . Consider the map This map is well defined since whenever there is some such that . Thus thus . Pick such that is a basis for with respect to which the basis of is upper triangular. Now consider the basis of . With respect to this basis the matrix of with respect to appears in the lower right hand corner. This is because if Since the first column of the matrix only has a non-zero entry in the first row and, by assumption, the matrix of is upper triangular, we have that the matrix of is upper triangular.   If is linear and are eigenvectors with corresponding distinct eigenvalues then the are linearly independent.   If there are non-trivial linear combinations of the that are zero take one, say , with the fewest possible non-zero coefficients. Without loss of generality we may suppose that . Now apply the linear map . We have This linear combination of the has fewer non-zero coefficients than the original (since the coefficient of is now ) so all the coefficients must be . But then all of the original coefficients must have been except . But since and (it is an eigenvector). This contradiction proves that the are linearly independent.  The following corollary says that sometimes this is enough to show that is diagonalizable.   Suppose that is linear and that has dimensional . If has distinct eigenvalues then is diagonalizable.   Let where each is an eigenvector with eigenvalue . By we know is linearly independent, and since it has the correct size it must be a basis.  We have seen a few ways of looking for eigenvalues. Considering worked well for matrices. We can find eigenvalues for also by solving polynomial equations. We will turn now to an alternative avenue.  "
},
{
  "id": "sec-spectral",
  "level": "1",
  "url": "sec-spectral.html",
  "type": "Section",
  "number": "6.3",
  "title": "Self-Adjoint Maps and the Spectral Theorem",
  "body": " Self-Adjoint Maps and the Spectral Theorem   A linear map on a Euclidean space is called self-adjoint if .   If is a symmetric matrix then is self-adjoint, where we condider with its standard inner product. We have    Let , the vector space  This example is not actually an example, since is not finite dimensional, and hence not Euclidean. Ot the other hand the eample is too nice not to include. of infinitely differentiable functions satisfying , with inner product The linear map defined by is self-adjoint as proven on homework.  One of the nicest things about self-adjoint maps is that they are always diagonalizable—there always exists an basis of eigenvectors. Even better, the eigenvectors can be chosen to be orthogonal.   If is a self-adjoint map on a finite dimensional inner product space over , then there is an orthonormal basis for consisting of eigenvectors of .  Next we need a lemma that extends allows us to do induction on the dimension.   If is an inner product space, is self-adjoint, and is a subspace that is -invariant (meaning that for all ), then is also -invariant.   Suppose . To show we need to show that for all . But since and .  We now prove , using induction on .    Proof of . If the result is trivial, so suppose and is an eigenvector of with eigenvalue (guaranteed by ). Set . By we know , so by induction there is an orthonormal basis of consisting of eigenvectors of . Adjoining to this ONB of we get an orthonormal basis for consisting of eigenvectors of .  "
},
{
  "id": "sec-sets-functions",
  "level": "1",
  "url": "sec-sets-functions.html",
  "type": "Section",
  "number": "A.1",
  "title": "Sets, Functions, Constructions",
  "body": " Sets, Functions, Constructions   Sets  Coming soon to an OER near you!    Functions  Coming soon to an OER near you!    Set Constructions   Subsets  Coming soon to an OER near you!    Product Sets  Coming soon to an OER near you!    Quotient Sets  Coming soon to an OER near you!    "
},
{
  "id": "sec-groups-rings-fields",
  "level": "1",
  "url": "sec-groups-rings-fields.html",
  "type": "Section",
  "number": "A.2",
  "title": "Groups, Rings, Fields",
  "body": " Groups, Rings, Fields  hi  "
},
{
  "id": "appendix-2",
  "level": "1",
  "url": "appendix-2.html",
  "type": "Appendix",
  "number": "B",
  "title": "Notation",
  "body": " Notation   "
},
{
  "id": "appendix-list-definitions",
  "level": "1",
  "url": "appendix-list-definitions.html",
  "type": "Appendix",
  "number": "C",
  "title": "List of Definitions",
  "body": " List of Definitions   "
},
{
  "id": "appendix-list-results",
  "level": "1",
  "url": "appendix-list-results.html",
  "type": "Appendix",
  "number": "D",
  "title": "List of Results",
  "body": " List of Results   "
},
{
  "id": "colophon-2",
  "level": "1",
  "url": "colophon-2.html",
  "type": "Colophon",
  "number": "",
  "title": "Colophon",
  "body": " This book was authored in PreTeXt .  "
}
]

var ptx_lunr_idx = lunr(function () {
  this.ref('id')
  this.field('title')
  this.field('body')

  ptx_lunr_docs.forEach(function (doc) {
    this.add(doc)
  }, this)
})
